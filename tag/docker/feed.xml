<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="https://hoex5h.github.io/tag/docker/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://hoex5h.github.io/" rel="alternate" type="text/html" />
  <updated>2025-01-15T11:35:46+00:00</updated>
  <id>https://hoex5h.github.io/tag/docker/feed.xml</id>

  
  
  

  
    <title type="html">hoeeeeeh | </title>
  

  
    <subtitle>HOEH 개발 블로그</subtitle>
  

  

  
    
      
    
  

  
  

  
    <entry>
      <title type="html">[Kuploy Project] Kuploy History</title>
      <link href="https://hoex5h.github.io/Kuploy_Project-_Kuploy_History" rel="alternate" type="text/html" title="[Kuploy Project] Kuploy History" />
      <published>2025-01-14T02:40:00+00:00</published>
      <updated>2025-01-14T02:40:00+00:00</updated>
      <id>https://hoex5h.github.io/%5BKuploy_Project%5D_Kuploy_History</id>
      <content type="html" xml:base="https://hoex5h.github.io/Kuploy_Project-_Kuploy_History">&lt;h1 id=&quot;kuploy-kuploy-history&quot;&gt;[Kuploy] Kuploy History&lt;/h1&gt;

&lt;p&gt;CICD 자동화 프로젝트를 만들면서 수정사항이 정말 아주 많이 생기고 있는데,&lt;/p&gt;

&lt;p&gt;History 를 보기 좋게 작성해 두는 것이 추후 프로젝트를 리팩토링할 때 아주 쓸모 있을 것 같다.&lt;/p&gt;

&lt;p&gt;살짝 늦은 감이 없지 않지만,,&lt;/p&gt;

&lt;h2 id=&quot;kubernetes&quot;&gt;Kubernetes&lt;/h2&gt;

&lt;h3 id=&quot;kompose&quot;&gt;Kompose&lt;/h3&gt;

&lt;p&gt;Docker-compose 파일을 k8s 에 apply 시키기 위한 tool 이다.&lt;/p&gt;

&lt;p&gt;현재 버전은 1.31.0 사용 중이다.&lt;/p&gt;

&lt;h3 id=&quot;kompose-문제점&quot;&gt;Kompose 문제점&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;-controller statefulset&lt;/p&gt;

    &lt;p&gt;statefulset 을 만들기 위해서 flag 를 썼더니 json 마지막 부분에 status: 항목이 생겨있다.&lt;/p&gt;

    &lt;p&gt;status 는 쿠버네티스가 배포/운영 하면서 쓰는 항목이지 apply 하기 전에 쓰는 항목이 아닐텐데 이게 왜 생기지?&lt;/p&gt;

    &lt;p&gt;심지어 status: replica : 0 으로 잡혀있어서, 만들자마자 replica 가 1이 되기 때문에 Live Manifest 와 Desired Manifest 가 달라져서 바로 out of sync 상태가 된다…&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;-namespace={“namespace”}
모든 항목의 네임스페이스를 지정해주려고 하니 namespace.yaml 파일을 만들어버린다.&lt;/p&gt;

    &lt;p&gt;이대로 배포하면 namespace.yaml 이 apply 되면서 배포한 어플리케이션의 일부가 되어버리는데,&lt;/p&gt;

    &lt;p&gt;어플리케이션을 삭제하면 namespace 도 삭제된다..&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;metrics-server&quot;&gt;Metrics Server&lt;/h3&gt;

&lt;p&gt;K8s 에 배포되어있는 리소스들의 메트릭을 관찰할 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;cni-calico&quot;&gt;CNI (Calico)&lt;/h3&gt;

&lt;p&gt;K8s 의 노드들이 통신하는 규약&lt;/p&gt;

&lt;p&gt;여러가지 종류가 있지만 현재 Calico 사용 중&lt;/p&gt;

&lt;h3 id=&quot;storage-class&quot;&gt;Storage Class&lt;/h3&gt;

&lt;p&gt;Persisten Volume Claim 을 만들 때, Storage Class 를 사용하는데&lt;/p&gt;

&lt;p&gt;Storage Class 에는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;provisioner&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;parameters&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;reclaimPolicy&lt;/code&gt; 를 정의할 수 있다.&lt;/p&gt;

&lt;p&gt;Storage Class 에 대한 자세한 설명은 &lt;a href=&quot;추후 작성 예정&quot;&gt;여기&lt;/a&gt;를 참고하면 된다.&lt;/p&gt;

&lt;p&gt;Kuploy는 로컬에 저장할 수 있으면서 동시에 동적 프로비저닝을 할 수 있어야 한다.&lt;/p&gt;

&lt;p&gt;이러한 조건을 만족시켜주는 Storage Class 가 &lt;a href=&quot;https://github.com/rancher/local-path-provisioner&quot;&gt;Rancher/local-path-provisioner&lt;/a&gt; 였다.&lt;/p&gt;

&lt;p&gt;현재는 다른 storage class를 사용할 필요가 없기 때문에 default storage class 이다. (따로 storage class 가 지정되지 않으면 자동으로 default storage class 로 지정)&lt;/p&gt;

&lt;h2 id=&quot;docker&quot;&gt;Docker&lt;/h2&gt;

&lt;h3 id=&quot;docker-hub&quot;&gt;Docker hub&lt;/h3&gt;

&lt;p&gt;유저들의 이미지를 저장할 레지스트리를 on-premise 로 만들 여력이,, 안되어서 도커 허브를 쓰기로 했다&lt;/p&gt;

&lt;p&gt;나중에 저장소가 많이 늘어난다면 사설 레지스트리를 써보거나, Harbor 혹은 Nexus 를 써볼 수도?&lt;/p&gt;

&lt;h2 id=&quot;github-action&quot;&gt;Github Action&lt;/h2&gt;

&lt;h2 id=&quot;argocd&quot;&gt;Argocd&lt;/h2&gt;

&lt;h3 id=&quot;installation&quot;&gt;Installation&lt;/h3&gt;

&lt;h3 id=&quot;argocd-api&quot;&gt;Argocd Api&lt;/h3&gt;

&lt;p&gt;Kuploy 웹에 사용자가 어떤 어플리케이션을 배포중인지 보여주기 위해서 사용했다.&lt;/p&gt;

&lt;h3 id=&quot;argocd-image-updater&quot;&gt;Argocd Image Updater&lt;/h3&gt;

&lt;p&gt;사용자들이 소스코드의 변경 -&amp;gt; image push 하는 과정만 존재할 경우,&lt;/p&gt;

&lt;p&gt;즉 현재 argocd 가 호시탐탐 지켜보고 있는 application 의 status 에 변화를 불러 일으킬만한 변경점이 없다면 sync 되지 않는다.&lt;/p&gt;

&lt;p&gt;우리는 image version 이 바뀌는 것도 sync 를 해야한다.&lt;/p&gt;

&lt;h3 id=&quot;trouble-shooting&quot;&gt;Trouble Shooting&lt;/h3&gt;

&lt;h3 id=&quot;절대-project-name-을-멋대로-짓지-마&quot;&gt;절대 Project Name 을 멋대로 짓지 마.&lt;/h3&gt;

&lt;p&gt;Project Name 을 IDE 에서 프로젝트 생성할 때 짓는 이름 쯤으로 생각하면 무한 오류에 빠진다.&lt;/p&gt;

&lt;p&gt;이거 때문에 교수님 앞에서 눈물의 에러쇼를 했다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;당신이 원하는 프로젝트 네임을 적는 것이 아니라 ArgoCD에 미리 생성한 Project 중에 하나를 고르는 것이다!!!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;기본으로 default 프로젝트가 존재하고, 따로 원하면 만들 수 있다.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content>

      
      
      
      
      

      <author>
          <name>[&quot;hoeeeeeh&quot;]</name>
        
        
      </author>

      

      
        <category term="Kubernetes" />
      
        <category term="Ansible" />
      
        <category term="ArgoCD" />
      
        <category term="Docker" />
      
        <category term="Linux" />
      

      
        <summary type="html">[Kuploy] Kuploy History</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">[Kuploy Project] Kuploy</title>
      <link href="https://hoex5h.github.io/Kuploy_Project-_Kuploy_" rel="alternate" type="text/html" title="[Kuploy Project] Kuploy " />
      <published>2025-01-14T02:40:00+00:00</published>
      <updated>2025-01-14T02:40:00+00:00</updated>
      <id>https://hoex5h.github.io/%5BKuploy_Project%5D_Kuploy_</id>
      <content type="html" xml:base="https://hoex5h.github.io/Kuploy_Project-_Kuploy_">&lt;h1 id=&quot;kuploy-kuploy-는-어떤-프로젝트인가&quot;&gt;[Kuploy] Kuploy 는 어떤 프로젝트인가?&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;Kuploy 는 Konkuk + Deploy 의 합성어로 건국대학교 학생들의 Deploy 를 도와주기 위한 프로젝트이다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;유휴-컴퓨팅-리소스를-활용해서-학생들에게-제공해주자&quot;&gt;유휴 컴퓨팅 리소스를 활용해서 학생들에게 제공해주자!&lt;/h2&gt;

&lt;p&gt;처음 시작은 건국대학교의 유휴 컴퓨팅 리소스들을 활용하여 학생들에게 제공해주자! 라는 취지를 가진 프로젝트였다. 이게 무슨말인가 하면,&lt;/p&gt;

&lt;p&gt;건국대학교에는 수 많은 컴퓨터들이 학생들의 수업, 실습을 위해 존재한다. 하지만 실제로 이 컴퓨터들은 수업 시간 외에는 잘 사용되지 않는다.&lt;/p&gt;

&lt;p&gt;실습용 컴퓨터도 마찬가지인데, 요새는 대부분 노트북을 하나씩 소유하고 있다보니 모여서 프로젝트를 하는 팀플의 경우를 제외하고는 잘 사용되지 않는다. 이런 상황이다보니 많은 컴퓨터들이 사용되지도 않은 채 관리 미흡으로 먼지가 쌓여가고 있었다.&lt;/p&gt;

&lt;p&gt;이렇게 잘 사용되지 않는, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;유휴 컴퓨팅 리소스들을 잘 합쳐서 학생들에게 사용할 수 있게끔 제공&lt;/code&gt;한다면 학교와 학생 모두가 좋은 일이 아닐까? 라는 긍정적인 사고의 결과물이 바로 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Kuploy&lt;/code&gt; 프로젝트이다.&lt;/p&gt;

&lt;h2 id=&quot;학교의-컴퓨터들-그냥-원격으로-쓰면-안돼&quot;&gt;학교의 컴퓨터들 그냥 원격으로 쓰면 안돼?&lt;/h2&gt;

&lt;p&gt;우리의 목적은 단순히 유휴 컴퓨터들 하나하나에 Teamviewer 를 깔아서 학생들에게 GUI 원격으로 사용할 수 있게 하거나, 아니면 그냥 ssh 를 열어서 사용할 수 있게끔 하는 것이 아니다. 만약 이런 것이 목적이였다면 프로젝트 이름은 Kuploy 가 아니라 Kumote(Kuploy + Remote) 가 되지 않았을까?&lt;/p&gt;

&lt;p&gt;우리는 학교의 유휴 컴퓨팅 리소스 뿐만 아니라 개발 프로세스 자동화에도 관심이 많이 있었다.&lt;/p&gt;

&lt;p&gt;학생들은 주로 AWS 를 이용해서 자신들의 프로젝트를 테스트하고 서비싱한다. 이렇게 서비싱하는 과정은 크게 아래의 3가지 절차를 밟는다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;AWS Free tier(신규 회원은 1년간 공짜니까!) ec2 생성&lt;/li&gt;
  &lt;li&gt;Inbound, Outbound 규칙 생성(일단 ssh 부터 뚫어놔야 나도 접근할 수 있으니까)&lt;/li&gt;
  &lt;li&gt;프로젝트 build 파일들을 ec2 에 옮겨서 run 하거나, docker를 활용하여 run&lt;/li&gt;
  &lt;li&gt;변경사항이 생긴다면 github 에 commit push 후, 3번과정을 다시 수행&lt;/li&gt;
&lt;/ol&gt;

&lt;blockquote&gt;
  &lt;p&gt;이러한 과정은 복잡하지는 않지만 새로운 프로젝트를 할 떄마다 매번 해줘야하는 번거로움이 있다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;게다가 Free tier 는 성능이 썩 좋은 편은 아니다. 실제로 프로젝트를 진행하던 도중 ec2 의 메모리 부족으로 제대로 실행시킬 수 없는 경우도 있었다. 메모리 스왑을 통해 임시 방편을 마련해놨었지만 말 그대로 임시방편일 뿐이다.&lt;/p&gt;

&lt;p&gt;이러다보니 매번 할 때마다 귀찮은 반복 작업이 필요하며, 성능에도 제한이 있어서 제대로 활용하기 힘든 AWS 를 대체하고 싶었다.&lt;/p&gt;

&lt;p&gt;Kuploy 에서는 건국대학교 학생 한정으로, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Free tier 보다 높은 성능(물론, 상한선은 있다)을 제공하면서 동시에 배포 과정을 최적화&lt;/code&gt;시킬 수 있다.&lt;/p&gt;

&lt;h1 id=&quot;kuploy-프로젝트의-진행-과정&quot;&gt;Kuploy 프로젝트의 진행 과정&lt;/h1&gt;

&lt;h2 id=&quot;cluster-구축&quot;&gt;Cluster 구축&lt;/h2&gt;

&lt;p&gt;우리는 우선 교수님으로부터 총 4대의 학교 컴퓨터를 대여 받았다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;1대의 Master Node&lt;/li&gt;
  &lt;li&gt;2대의 Worker Node&lt;/li&gt;
  &lt;li&gt;1대의 Ansible Control Node&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;4대의 컴퓨터들은 이렇게 구성해서 클러스터를 구축하기로 결정하였고, 그 전에 4대의 컴퓨터들을 초기화해서 셋팅할 것이 많이 있었다.&lt;/p&gt;

&lt;h3 id=&quot;master-node-worker-node-공통-설정&quot;&gt;Master Node, Worker Node 공통 설정&lt;/h3&gt;

&lt;h3 id=&quot;ubuntu-2204-server-lts-설치&quot;&gt;Ubuntu 22.04 Server LTS 설치&lt;/h3&gt;

&lt;p&gt;일단 &lt;a href=&quot;https://releases.ubuntu.com/jammy/&quot;&gt;Ubuntu 22.04&lt;/a&gt; 에서 설치할 OS 환경을 잘 골라서 Ubuntu 22.04 live server 의 iso 파일을 다운받자.&lt;/p&gt;

&lt;p&gt;이제 부팅 디스크로 USB를 만들어야하는데, 집에 USB가 다 어딘가로 사라지고 없어서 SanDisk 사의 USB 2개를 구매하였다.&lt;/p&gt;

&lt;p&gt;USB를 간단한 방법으로 설치 디스크로 만드는 툴은 크게 2개가 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Rufus&lt;/li&gt;
  &lt;li&gt;Ventoy&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;이 중에 우리는 Ventoy 를 활용했다. 이유는 Rufus 보다 훨씬 쉽고 간단하다!
우선 &lt;a href=&quot;https://www.ventoy.net/en/download.html&quot;&gt;Ventoy&lt;/a&gt; 링크를 통해 본인의 OS에 맞는 Ventoy 툴을 다운받자.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/upload/2025-01-14-[Kuploy_Project]_Kuploy_.md/0.png&quot; alt=&quot;0&quot; /&gt;&lt;em&gt;image.png&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;이후에 Ventoy2Disk 를 실행하고 부팅 디스크로 만들 USB 를 선택한 후, Install 을 누르면 준비 끝이다.&lt;/p&gt;

&lt;p&gt;이후에 iso 파일을 USB에 넣어주기만 하면 자동으로 부팅디스크가 된다.&lt;/p&gt;

&lt;p&gt;이렇게 만든 부팅 디스크를 꽂고 설치하면 된다!
만약 위에서 만든 USB로 부팅되지 않는다면 BIOS 에서 부팅 순서를 바꿔보도록 하자.&lt;/p&gt;

&lt;h3 id=&quot;스왑-메모리-끄기&quot;&gt;스왑 메모리 끄기&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;shell
$ sudo swapoff -a

# 재부팅 후에 스왑 메모리가 자동으로 켜지는 것 방지
$ sudo vim /etc/fstab

# 스왑 파티션(아래 줄) 주석 처리
# /swap.img     none    swap    sw      0       0



&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;쿠버네티스가 메모리를 할당할 때, 스왑메모리가 있으면 문제를 초래할 수 있다.
애초에 쿠버네티스가 리소스 관리를 하기 때문에 필요 하지도 않다. 스왑 메모리가 꺼졌는지 확인해보려면 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;free&lt;/code&gt; 커맨드를 사용해보면 된다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;swapoff -a 는 일시적으로 스왑 메모리를 끄는 것이다. 만약 /etc/fstab 에서 스왑 메모리를 영구적으로 끄지 않는다면 컴퓨터를 재부팅했을 때 자동으로 스왑 메모리가 켜지게 된다. 이 경우에 쿠버네티스가 정상적으로 작동하지 않는다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;스왑 메모리가 켜진 상태로 kubectl get nodes 명령어를 입력해보면 Ready여야 하는 노드가 NotReady 상태로 변경되고, 해당 노드에서 실행중인 Pod 들은 Terminating 상태나 Pending 상태에 무한히 빠진다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;방화벽ufw-포트-allow&quot;&gt;방화벽(ufw) 포트 allow&lt;/h3&gt;

&lt;p&gt;쿠버네티스 apiserver 가 6443 포트를 사용하기 때문에, 6443 포트를 뚫어주어야 api-server 와 통신할 수 있다.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;shell
$ sudo ufw allow {port}


&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;kubectl-kubelet-kubeadm-docker-설치&quot;&gt;kubectl, kubelet, kubeadm, docker 설치&lt;/h3&gt;

&lt;p&gt;최신 버전을 사용하면서 충돌 날 것을 우려하여 1.23.6-00 버전으로 명시하여 설치하였다.&lt;/p&gt;

&lt;p&gt;설치 이전에 epel, net-tools 를 설치한다.&lt;/p&gt;

&lt;details&gt;
&lt;summary&gt;EPEL, net tools 는 뭘까?&lt;/summary&gt;
&lt;code&gt;
&lt;pre&gt;


`EPEL` == Extra Packages for Enterprise Linux
말 그대로 엔터프라이즈 리눅스를 위한 추가 패키지이다. 기본 레포에 없는 오픈 소스들을 사용할 수 있다.


`net-tools` 는 리눅스의 네트워킹 관련 커맨드라인 도구 모음이다.


&amp;gt; A collection of programs that form the base set of the NET-3 networking distribution for the Linux operating system.  
&amp;gt; Includes: arp, hostname, ifconfig, netstat, rarp, route, plipconfig, slattach, mii-tool and iptunnel and ipmaddr.  
&amp;gt; A mirror of the sourcecode is available on https://github.com/ecki/net-tools


&lt;/pre&gt;&lt;/code&gt;
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt; 쿠버네티스 1.24 미만 버전 &lt;/summary&gt;
&lt;code&gt;
&lt;pre&gt;



```
shell
# install Extra Packages for Enterpries Linux System, Docker
sudo apt-get install epel-release -y
sudo apt-get install net-tools -y
sudo apt-get update &amp;amp;&amp;amp; sudo apt-get -y upgrade

# Install Kubernetes Package
# Add Google Cloud Public GPG Key to apt
sudo curl -s &amp;lt;https://packages.cloud.google.com/apt/doc/apt-key.gpg&amp;gt; | sudo apt-key add -

# Create new APT source list
cat &amp;lt;&amp;lt;EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list
deb &amp;lt;https://apt.kubernetes.io/&amp;gt; kubernetes-xenial main
EOF

# Update APT Package and upgrade all package
# flag -y means all yes
sudo apt-get update &amp;amp;&amp;amp; sudo apt-get -y upgrade

# Install Kubernetes - kubectl, kubelet, kubeadm
sudo apt-get install -y kubelet=1.23.6-00 kubeadm=1.23.6-00 kubectl=1.23.6-00

# hold kubelet, kubeadm, kubectl version
sudo apt-mark hold kubelet kubeadm kubectl

echo &quot;sleep 3 sec&quot;
sleep 3

sudo apt-get update &amp;amp;&amp;amp; sudo apt-get upgrade

sudo systemctl daemon-reload
sudo systemctl restart kubelet

echo &quot;Installing Docker&quot;
sudo curl -sSL &amp;lt;https://get.docker.com/&amp;gt; | sh

echo &quot;sleep 1 sec&quot;
sleep 1

echo &quot;sleep 3 sec&quot;
sleep 3


```



&lt;/pre&gt;&lt;/code&gt;
&lt;/details&gt;

&lt;p&gt;쿠버네티스 버전을 1.24 이상 버전을 사용하면서, docker 에서 containerd 로 옮겨가게 되었다. 쿠버네티스와 containerd 의 설치 방법은 &lt;a href=&quot;https://hoex5h.github.io/kubernetes/2024/04/06/kubernetes_containerd.html&quot;&gt;k8s-docker-containerd&lt;/a&gt; 에 적어두었다.&lt;/p&gt;

&lt;h3 id=&quot;containerd-설정&quot;&gt;containerd 설정&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;shell
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml


&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;systemd-를-cgroup-driver-로-설정하기&quot;&gt;systemd 를 cgroup driver 로 설정하기&lt;/h3&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/containerd/config.toml&lt;/code&gt; 에 아래 항목 추가&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;shell
[plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.runc]
  ...
  [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.runc.options]
    SystemdCgroup = true


&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;shell
sudo systemctl restart containerd


&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;containerd 재시작.&lt;/p&gt;

&lt;h3 id=&quot;ipv4-포워딩-bridge-iptable-규칙-수정&quot;&gt;IPv4 포워딩, bridge iptable 규칙 수정&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://kubernetes.io/docs/setup/production-environment/container-runtimes/#containerd&quot;&gt;Container Runtime&lt;/a&gt; 쿠버네티스 공식 문서를 참조해서 진행했다.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;shell
cat &amp;lt;&amp;lt;EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

sudo modprobe overlay
sudo modprobe br_netfilter

# sysctl params required by setup, params persist across reboots
cat &amp;lt;&amp;lt;EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF

# Apply sysctl params without reboot
sudo sysctl --system


&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;아래 커맨드로 br_netfilter 와 overlay 모듈이 load 되어 있는지 확인해보라고 한다.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;shell
lsmod | grep br_netfilter
lsmod | grep overlay


&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;이제 sysctl에 저 3가지 항목의 값이 1로 설정되어 있는지 확인해보자.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;shell
sysctl net.bridge.bridge-nf-call-iptables net.bridge.bridge-nf-call-ip6tables net.ipv4.ip_forward


&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;cgroup&quot;&gt;cgroup&lt;/h3&gt;

&lt;p&gt;리눅스는 cgroup가 프로세스의 리소스를 제한하는 역할을 한다.&lt;/p&gt;

&lt;p&gt;kubelet과 containerd 도 pod의 리소스를 제한하는데 cgroup driver 를 사용해야 하는데, 반드시 kubelet 과 containerd 는 같은 구성의, 같은 cgroup driver 를 사용해야 한다는 것이다.&lt;/p&gt;

&lt;p&gt;cgroup driver 에는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cgroupfs&lt;/code&gt; 와 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;systemd&lt;/code&gt; 가 존재하는데 만약 init 프로세스가 systemd 인 경우에는 kubelet 과 containerd 의 cgroup driver 또한 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;systemd&lt;/code&gt; 를 사용하도록 권장하고 있다. 이유는 systemd 가 cgroup 관리자는 하나라고 인식하기 때문이다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;추가로 cgroup v2 를 사용할 경우에도 systemd 를 사용하라고 한다. cgroup v2 가 무엇인지 문서를 읽어보니 cgroup 의 업그레이드 버젼인듯 하다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;kubelet-의-cgroup-driver-를-systemd-로-수정하기&quot;&gt;kubelet 의 cgroup driver 를 systemd 로 수정하기&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://kubernetes.io/ko/docs/tasks/administer-cluster/kubelet-config-file/&quot;&gt;kubelet-config-file&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;kubeadm 을 통해 init 을 하고 난 후에, kubelet 의 config 파일을 아래처럼 수정해주면 된다. 아직 kubeadm init 후에 적용하면 되기 때문에 지금 당장 config 파일을 만들 필요는 없다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/upload/2025-01-14-[Kuploy_Project]_Kuploy_.md/1.png&quot; alt=&quot;1&quot; /&gt;&lt;em&gt;image.png&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;master-node-설정&quot;&gt;Master Node 설정&lt;/h2&gt;

&lt;h3 id=&quot;kubeadm-init&quot;&gt;kubeadm init&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;shell
$ sudo kubeadm init --apiserver-advertise-address=[마스터 노드 IP] --pod-network-cidr=[CNI 네트워크 라우팅 대역]


&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;마스터노드에서 kubeadm 을 init 하면서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;네트워크 라우팅 대역&lt;/code&gt; 을 입력해야하는데, Calico 를 사용한다면 192.168.0.0/16 를 입력해 주면 된다.&lt;/p&gt;

&lt;p&gt;cidr 에 관해서는 &lt;a href=&quot;https://hoex5h.github.io/network/2024/03/22/cidr.html&quot;&gt;Gateway,사설망,CIDR&lt;/a&gt; 을 참조하자!&lt;/p&gt;

&lt;p&gt;해당 커맨드를 입력하면 아래와 같은 결과값이 반환되는데 이를 어딘가에 적어두거나 기억해두자. 추후 Worker Node 설정에 필요하다.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;shell
# 딱히 건들지 않았다면 api-server 의 포트가 6443 으로 설정되어 있을 것이라서 마스터 노드의 6443 포트를 입력한다.
sudo kubeadm join {마스터 노드 IP}:6443 --token {토큰값~}


&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;이후에 config 에 권한을 부여해줘야 한다.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;shell
sudo mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
systemctl restart kubelet
systemctl restart docker


&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;cnicontainer-network-interface&quot;&gt;CNI(Container Network Interface)&lt;/h3&gt;

&lt;p&gt;CNI에 대한 자세한 내용은 따로 작성해둔 &lt;a href=&quot;https://hoex5h.github.io/kubernetes/2024/03/21/kubernetes_cni.html&quot;&gt;Kubernetes-CNI&lt;/a&gt; 문서를 참조하자.&lt;/p&gt;

&lt;p&gt;우리는 CNI 로 calico 를 선택했다. 사실 이렇게 소규모 프로젝트에서 어떤 CNI를 쓰든 큰 차이가 나지는 않을 것이다. 가장 대중적으로 사용하는 Calico, Flannel, Weave Net 정도의 차이점만 알아보았는데 대규모 트래픽 연산(성능) 측면에서는 L3를 활용하는, 즉 모든 Container 마다 ip를 부여해서 통신하는 Calico의 성능이 제일 좋았고, 간편성만 따졌을 때는 Flannel 이 제일 좋았다. WeaveNet은 Mesh 네트워크 구조라서 성능이 나머지 두 플러그인에 비해 조금 떨어진다.&lt;/p&gt;

&lt;p&gt;대규모 트래픽 연산에 성능이 좋은 Calico와, 소규모 프로젝트에 어울리는 Flannel 중에서 선택을 고민했고 Calico 가 조금 더 대중적인 이유를 고려해서 Calico를 선택했다.&lt;/p&gt;

&lt;h3 id=&quot;calico-설치&quot;&gt;Calico 설치&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;shell
# 2024-03-22 기준!
$ kubectl create -f &amp;lt;https://raw.githubusercontent.com/projectcalico/calico/v3.27.3/manifests/tigera-operator.yaml&amp;gt;
$ kubectl create -f &amp;lt;https://raw.githubusercontent.com/projectcalico/calico/v3.27.3/manifests/custom-resources.yaml&amp;gt;


&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;더 자세한 것은 &lt;a href=&quot;https://docs.tigera.io/calico/latest/getting-started/kubernetes/quickstart&quot;&gt;Calico 공식 홈페이지&lt;/a&gt; 를 참조하자!&lt;/p&gt;

&lt;p&gt;calico 가 예전의 프로젝트에서 분리되어 나온건지 예전과 달라진 적이 있으므로 공식 홈페이지에서 안내해주는대로 설치하는 것을 권장한다.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;shell
$ watch kubectl get pods -n calico-system


&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;이후에 위의 커맨드로 모든 calico pod 들의 Status 가 Running 으로 바뀌는지 확인한다. 대략 5~6분 정도 소요되는 것 같다.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;shell
kubectl taint nodes --all node-role.kubernetes.io/control-plane-
kubectl taint nodes --all node-role.kubernetes.io/master-


&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;마지막으로 control plane 의 taint 를 제거함으로써 pod 스케쥴링을 할 수 있도록 바꿔준다.&lt;/p&gt;

&lt;h3 id=&quot;helm-설치&quot;&gt;Helm 설치&lt;/h3&gt;

&lt;p&gt;Helm 은 쿠버네티스의 Package managing tool 이다. Linux 의 APT 나 YUM, 맥에서는 homebrew 와 비슷하다고 보면 된다.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;shell
curl -fsSL -o get_helm.sh &amp;lt;https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3&amp;gt;
chmod +x get_helm.sh

helm version


&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;worker-node-설정&quot;&gt;Worker Node 설정&lt;/h2&gt;

&lt;p&gt;Master Node 에서 kubeadm init 하면서 얻었던 join 커맨드를 입력해주자.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;shell
# 딱히 건들지 않았다면 api-server 의 포트가 6443 으로 설정되어 있을 것이라서 마스터 노드의 6443 포트를 입력한다.
sudo kubeadm join {마스터 노드 IP}:6443 --token {토큰값~}


&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;join 이 정상적으로 되었다면 master node 에서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubectl get nodes&lt;/code&gt; 를 입력 했을 때, worker node 들의 상태가 NotReady 가 아닌 Ready 상태여야 한다.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>[&quot;hoeeeeeh&quot;]</name>
        
        
      </author>

      

      
        <category term="Kubernetes" />
      
        <category term="Ansible" />
      
        <category term="ArgoCD" />
      
        <category term="Docker" />
      
        <category term="Linux" />
      

      
        <summary type="html">[Kuploy] Kuploy 는 어떤 프로젝트인가?</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">[Kubernetes] 도커를 기반으로 멀티노드 클러스터를 구축해보자</title>
      <link href="https://hoex5h.github.io/Kubernetes-_%EB%8F%84%EC%BB%A4%EB%A5%BC_%EA%B8%B0%EB%B0%98%EC%9C%BC%EB%A1%9C_%EB%A9%80%ED%8B%B0%EB%85%B8%EB%93%9C_%ED%81%B4%EB%9F%AC%EC%8A%A4%ED%84%B0%EB%A5%BC_%EA%B5%AC%EC%B6%95%ED%95%B4%EB%B3%B4%EC%9E%90" rel="alternate" type="text/html" title="[Kubernetes] 도커를 기반으로 멀티노드 클러스터를 구축해보자" />
      <published>2025-01-14T02:23:00+00:00</published>
      <updated>2025-01-14T02:23:00+00:00</updated>
      <id>https://hoex5h.github.io/%5BKubernetes%5D_%EB%8F%84%EC%BB%A4%EB%A5%BC_%EA%B8%B0%EB%B0%98%EC%9C%BC%EB%A1%9C_%EB%A9%80%ED%8B%B0%EB%85%B8%EB%93%9C_%ED%81%B4%EB%9F%AC%EC%8A%A4%ED%84%B0%EB%A5%BC_%EA%B5%AC%EC%B6%95%ED%95%B4%EB%B3%B4%EC%9E%90</id>
      <content type="html" xml:base="https://hoex5h.github.io/Kubernetes-_%EB%8F%84%EC%BB%A4%EB%A5%BC_%EA%B8%B0%EB%B0%98%EC%9C%BC%EB%A1%9C_%EB%A9%80%ED%8B%B0%EB%85%B8%EB%93%9C_%ED%81%B4%EB%9F%AC%EC%8A%A4%ED%84%B0%EB%A5%BC_%EA%B5%AC%EC%B6%95%ED%95%B4%EB%B3%B4%EC%9E%90">&lt;h1 id=&quot;note&quot;&gt;NOTE!&lt;/h1&gt;

&lt;p&gt;먼저, 실습 환경은 M1 pro 맥북 프로임을 알립니다.&lt;/p&gt;

&lt;h2 id=&quot;kind-설치&quot;&gt;kind 설치&lt;/h2&gt;

&lt;p&gt;지난번에는 Docker Desktop 에서 쿠버네티스를 사용해봤는데, 그냥 Docker Desktop 를 이용하면 로컬에서는 싱글 노드로 밖에 클러스터를 구성할 수 없다고 한다.&lt;/p&gt;

&lt;p&gt;로컬에서 여러 노드를 클러스터로 구성하려면, “kind” 를 사용해야 한다.&lt;/p&gt;

&lt;p&gt;m1 맥에서는 간단하게 brew 를 통해서 설치할 수 있다.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;text
brew install kind


&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;cluster-생성&quot;&gt;cluster 생성&lt;/h2&gt;

&lt;p&gt;kind 에서는 간단하게&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;text
kind create cluster
kind delete cluster --name { $NAME }


&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;으로 단일 노드 클러스터를 생성 및 삭제할 수 있다고 한다.&lt;/p&gt;

&lt;p&gt;그러나 우리가 사용할 목적은 multiple node 로 구성된 클러스터이기 때문에, config yaml 파일을 하나 만들어야한다.&lt;/p&gt;

&lt;h3 id=&quot;cluster-config-file&quot;&gt;cluster config file&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;yaml
kind-make-own-cluster-config.yaml

# four node (three workers) cluster config
# &quot;extraPortMappings&quot; maps literally container to the host machine
# On Mac, port 5000 is used by &quot;ControlCE&quot; which related to &quot;AirPlay&quot;
# If you want to use port 5000, turn off the &quot;AirPlay Receive Mode&quot; in mac os Setting
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
name: kluster
nodes:
- role: control-plane
  labels:
    roles: master
  extraPortMappings:
  - containerPort: 5001
    hostPort: 5001
- role: worker
  labels:
    roles: worker
  extraPortMappings:
  - containerPort: 5002
    hostPort: 5002
- role: worker
  labels:
    roles: worker
  extraPortMappings:
  - containerPort: 5003
    hostPort: 5003
- role: worker
  labels:
    roles: worker
  extraPortMappings:
  - containerPort: 5004
    hostPort: 5004



&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;맥에서 5000번 포트는 Airplay 와 연관된 ControlCE 라는 놈이 이미 차지하고 있으므로, 5001번 포트부터 열었다.&lt;/p&gt;

&lt;p&gt;만약 5000번 포트로 열고 싶으면 맥 설정에서 “공유 &amp;gt; 시스템 환경설정 &amp;gt; AirPlay 수신 모드 끄기” 를 하면 된다고 한다. (직접 해보지는 않았으니 반드시 확인해보고 할 것!)&lt;/p&gt;

&lt;p&gt;나는 굳이 AirPlay 기능을 건들이고 싶지 않기 때문에 5001번 포트부터 열었다.&lt;/p&gt;

&lt;p&gt;이후, 터미널에&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;text
kind create cluster --config kind-make-own-cluster-config.yaml


&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;로, 위의 yaml 파일대로 클러스터룰 구성하면 된다.&lt;/p&gt;

&lt;h2 id=&quot;result&quot;&gt;Result&lt;/h2&gt;

&lt;p&gt;아래의 사진처럼 도커에 컨테이너가 생기고, 이 컨테이너들을 클러스터로 묶은 것을 알 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/upload/2025-01-14-[Kubernetes]_도커를_기반으로_멀티노드_클러스터를_구축해보자.md/0.png&quot; alt=&quot;0&quot; /&gt;&lt;em&gt;image.png&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/upload/2025-01-14-[Kubernetes]_도커를_기반으로_멀티노드_클러스터를_구축해보자.md/1.png&quot; alt=&quot;1&quot; /&gt;&lt;em&gt;image.png&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/upload/2025-01-14-[Kubernetes]_도커를_기반으로_멀티노드_클러스터를_구축해보자.md/2.png&quot; alt=&quot;2&quot; /&gt;&lt;em&gt;image.png&lt;/em&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>[&quot;hoeeeeeh&quot;]</name>
        
        
      </author>

      

      
        <category term="Docker" />
      
        <category term="Kubernetes" />
      

      
        <summary type="html">NOTE!</summary>
      

      
      
    </entry>
  
</feed>
