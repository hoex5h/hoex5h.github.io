
var documents = [
    










{
    "id": 0,
    "url": "https://hoex5h.github.io//about/",
    "title": "About",
    "body": "Ghost is professional publishing platform designed for modern journalism. This is a demo site of a basic Ghost install to give you a general sense of what a new Ghost site looks like when set up for the first time.  If you’d like to set up a site like this for yourself, head over to Ghost. org and start a free 14 day trial to give Ghost a try! If you’re a developer: Ghost is a completely open source (MIT) Node. js application built on a JSON API with an Ember. js admin client. It works with MySQL and SQLite, and is publicly available on Github. If you need help with using Ghost, you’ll find a ton of useful articles on our knowledgebase, as well as extensive developer documentation. "
    
},









{
    "id": 1,
    "url": "https://hoex5h.github.io//author/hoeeeeeh/",
    "title": "Author Page",
    "body": "{% for author in site. data. authors %}  {% if author[1]. username == page. author %}  &lt;header class= site-header outer {% if author[1]. cover or page. cover %}  style= background-image: url({{ site. baseurl }}{% if author[1]. cover %}{{ author[1]. cover }}{% elsif page. cover %}{{ page. cover }}{% endif %}) {% else %}no-cover{% endif %} &gt;    &lt;div class= inner &gt;      {% include site-nav. html %}      &lt;div class= site-header-content &gt;        {% if author[1]. picture %}                  {% endif %}        &lt;h1 class= site-title &gt;{{ author[1]. name }}&lt;/h1&gt;        {% if author[1]. bio %}          &lt;h2 class= author-bio &gt;{{ author[1]. bio }}&lt;/h2&gt;        {% endif %}        &lt;div class= author-meta &gt;          {% if author[1]. location %}            &lt;div class= author-location &gt;{{ author[1]. location }} •&lt;/div&gt;          {% endif %}          &lt;div class= author-stats &gt;            {% if paginator. total_posts == 0 %}No posts{% elsif paginator. total_posts == 1 %}1 post{% else %}{{ paginator. total_posts }} posts{% endif %} •          &lt;/div&gt;          {% if author[1]. url %}            {% include website. html %}          {% endif %}          {% if author[1]. twitter %}            {% include twitter. html %}          {% endif %}          {% if author[1]. facebook %}            {% include facebook. html %}          {% endif %}          {% include rss. html %}        &lt;/div&gt;      &lt;/div&gt;    &lt;/div&gt;  &lt;/header&gt;  {% endif %}{% endfor %}                   {% include post-card. html %}      "
    
},





























































































































































































{
    "id": 2,
    "url": "https://hoex5h.github.io//%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC_CS_%EC%A7%80%EC%8B%9D",
    "title": "네트워크 CS 지식",
    "body": "2025/01/15 - 네트워크 CS 지식 정리 전이중화 통신 Full Duplex : 양쪽 장치가 동시에 송수신할 수 있는 방식 회선이 두 개(송신로, 수신로)가 있어서 같은 시간에 데이터를 주고 받을 수 있는 것이 전이중화 통신반 이중화 통신은 회선 하나로 보내기 때문에 양방향으로 보내기 전에 먼저 상대방이 보냈는지 확인하는 기술 CSMA-CD 충돌이 발생하면 잠깐 기다렸다가 다시 보내기 무선랜무선랜은 반이중화 통신을 사용 half duplex (반이중화 통신)은 양쪽 장치가 서로 통신은 할 수 있지만 동시에는 못한다. 즉 한 번에 한 방향만 가능 CSMA/CA  사용하지 않는 채널 발견 IFS 시간만큼 기다림 프레임 보내고, 다음 프레임 보내기 전까지 IFS 만큼 기다림대표적인 예시) 와이파이, 블루투스, 지그비 2. 4Ghz : 장애물에 강함, 속도가 느리다, 호환성이 좋다 5Ghz : 장애물에 약함, 속도가 빠르다, 호환성이 비교적 나쁘다 대규모 트래픽으로 인한 서버 과부하 해결 방법서버 과부하 : 서버가 리소스를 소진하여 들어오는 요청을 처리하지 못할 때. (응답없음) 자원의 한계점 도달 → 자원을 빠르게 증가시켜주기 (오토 스케일링) 클라우드 와치가 모니터링한다. → 무료 모니터링 (netData) 도 있다. → 쓰레스 홀드(임계치)를 정해놓고 이상으로 넘어가면 알림을 준다. 모니터링을 하는 이유: 모니터링을 통해 어떤 서비스에 트래픽이 몰리는지, 어떤 이유로 몰리는지 분석이 가능하다. 또한 페이지를 나누어 트래픽을 나눌 수도 있다. 로드밸런서: 로드밸런서를 통해서 트래픽을 분산 오토 스케일링을 하는 동안 시간이 조금 걸리기 때문에, 로드 밸런스를 앞 단에 두고 트래픽을 분산 시킨다. 로드 밸런서에다가도 오토 스케일링을 걸기도 한다. (트래픽이 많은 기업) 블랙스완 프로토콜: 예측할 수 없는 사고가 일어난 것 엄청나게 많은 대비를 해도 서버가 죽을 수도 있다. 서버가 예기치 못한 사태로 죽고나서 분석은 가능하지만 미리 예측하기는 쉽지 않다. 블랙스완을 대비한 규칙이 있으면 좋다. 구글의 경우에는,  영향을 받은 시스템과 각 시스템의 상대적 위험 수준을 확인→ 체계적으로 데이터를 수집하고 원인에 대한 가설을 수립한 후 이를 테스팅  잠재적으로 영향을 받을 수 있는 내부의 모든 팀에 연락 최대한 빨리 취약점에 영향을 받는 모든 시스템을 업데이트 복원계획을 포함한 우리의 대응 과정을 파트너와 고객 등 외부에 전달서킷 브레이커: 서비스 장애를 감지하고 연쇄적으로 생기는 에러를 방지하는 기법 서비스와 서비스 사이에 서킷브레이커 계층을 두고, 미리 설정해놓은 타임아웃 임계값을 넘어가면 서킷브레이커가 그 이후의 추가 호출에 무조건 에러를 반환하게 한다. 다운된 서비스에 무한정 대기하면서 스레드 혹은 리소스를 잡아먹게 되면서 다른 스레드가 차단되는 현상이 발생할 수 있다. 기다린다는 것은 사용자 입장에서 썩 좋은 경험은 아니다. 기다리는것보다 차라리 오류가 생겼다고 말해주는 것이 더 좋은 경험이다. 따라서 빠르게 알려줄 수 있는 서킷 브레이커가 더 좋을 수 있다. 서킷 브레이커의 상태:  closed (정상) : 네트워크 요청의 실패율이 임계치보다 낮음     어느정도는 실패할 수 있다. 100퍼센트가 아니라 90퍼이상 성공하면 괜찮다같이 정할 수 있다    open (에러) : 에러가 임계치 이상의 상태일 때.      요청을 서비스로 전송하지 않고 에러를 반환. 이를 fail test 라고 한다.     half open (확인 중) : 오픈 상태에서 일정 타임 아웃으로 설정된 시간이 지나면 장애가 해결되었는지 확인하기 위한 상태. 장애가 풀리면 closed 상태로, 실패하면 다시 open 상태로 전환image. png 서킷 브레이커의 장점:  연쇄적인 에러 발생을 막아준다 일부 서비스가 다운되더라도 다른 서비스를 정상적으로 돌아가게 도와준다.  사용자 경험을 높여준다. Netfilix 의 Hystrix 컨텐츠 관리: 어떻게 하면 서버의 부하를 덜 수 있을까  불필요한 컨텐츠 제거     인프런에서 할인 이벤트를 열었는데 장애가 발생.    https://tech. inflab. com/202201-event-postmortem/   강의 본문을 조회하는 쿼리가 select * 로 되어 있어서 사용 여부와 관계 없이 모든 컬럼을 조회    CDN 을 통한 컨텐츠 제공     정적 자원들은 메인 서버가 주는게 아니라, CDN 을 통해서 주면 부하가 분산된다.     컨텐츠 캐싱     요청 자체를 줄일 수 있다.     컨텐츠 압축     텍스트 기반 리소스는 gzip / brotli 등을 통해 압축 (70퍼정도로 압축됨)   압축 푸는 서버의 자원도 필요    컨텐츠의 우아한 저하 (미리 준비된 응답)     시스템의 과도한 부하를 줄이기 위해 제공하는 컨텐츠 및 기능을 일시적으로 줄이는 전략 예를 들어 정적 텍스트 페이지를 제공하거나, 검색을 비활성화하거나 더 적은 수의 검색 결과를 반환하거나, 필수적이지 않은 기능을 비활성화   이미지나 썸네일 등을 빼버리고 텍스트 기반으로 축소 서빙 (중요한 텍스트만 남기고 제거)    video  REST API 란 무엇인가요?→ Restful 한 API  Uniform-Interface     API에서 자원들은 각각의 독립적인 인터페이스를 가지며, 각각의 자원들이 url 자원식별, 표현을 통한 자원조작, Self-descriptive message, HATEOAS 구조를 가지는 것을 말합니다.    독립적인 인터페이스라는 것은 서로 종속적이지 않은 인터페이스를 말합니다. 예를 들어, 웹페이지를 변경했다고 웹 브라우저를 업데이트 하는 일은 없어야 하고, HTTP 명세나 HTML 명세가 변경되어도 웹페이지는 잘 작동해야 합니다.    URL 자원 식별: → identification of resources, 자원은 url 로 식별되어야 합니다. /product/${id} 로 요청을 하면, 여기에 해당하는 product 를 줘야한다. 표현을 통한 자원 조작: Manipulation of resources through representations, URL 과 GET, DELETE 등 HTTP 표준 메서드 등을 통해 자원을 조회, 삭제 등 작업을 설명할 수 있는 정보가 담겨야 한다. getProduct 가 아니라, GET 메소드를 쓰고 /productSelf-descriptive Messages: HTTP Header 에 타입을 명시하고 각 메세지들은 MIME types 에 맞춰 표현되어야 합니다. 예를 들어 . json 을 반환한다면 application/json 으로 명시하기 HATEOAS (Hypermedia As The Engine Of Application State): 강의 내용에서 설명이 좀 부족한 것 같다. 하이퍼링크에 따라 다른 페이지를 보여줘야 하며, 데이터마다 어떤 URL에서 왔는지 명시해주어야 한다. 보통은 href, links, link, url 속성 중 하나에 해당 데이터의 URL 을 담아서 표기해야 합니다.  동적 상호작용 제공 : 클라이언트는 고정된 URL 경로를 하드코딩하지 않고, 서버가 제공하는 하이퍼링크를 따라가며 필요한 리소스나 작업을 탐색 자체 설명적 메시지: 서버가 응답 메시지에 포함된 링크를 통해 클라이언트가 다음 작업을 수행할 수 있는 정보를 제공합니다.  애플리케이션 상태 전환: 링크를 통해 리소스 상태를 바꾸거나, 새로운 리소스를 요청하거나, 작업을 수행할 수 있습니다. HATEOAS 미적용 json{  id : 123,  name :  John Doe ,  email :  johndoe@example. com }HATEOAS 를 적용한 응답 json{  id : 123,  name :  John Doe ,  email :  johndoe@example. com ,  _links : {   self : {    href :  https://api. example. com/users/123   },   update : {    href :  https://api. example. com/users/123 ,    method :  PUT   },   delete : {    href :  https://api. example. com/users/123 ,    method :  DELETE   },   orders : {    href :  https://api. example. com/users/123/orders   } }}이 응답에서는 클라이언트가 self, update, delete, orders와 같은 작업을 수행할 수 있도록 하이퍼링크를 제공합니다. •	클라이언트는 이 링크를 활용해 데이터를 업데이트하거나 삭제하며,•	해당 사용자가 주문한 orders 데이터를 조회할 수도 있습니다. HATEOAS의 장점    API 유연성 증가: 클라이언트는 URL을 하드코딩할 필요 없이 동적으로 링크를 탐색하며 API와 상호작용할 수 있습니다.     버전 관리 간소화: API 구조가 변경되더라도 클라이언트는 링크를 따라가기만 하면 되므로 API 변경의 영향을 덜 받습니다.     문서 의존성 감소: API 응답 자체가 필요한 작업에 대한 정보를 포함하므로, 별도의 문서 없이도 클라이언트가 API를 사용할 수 있습니다.  HATEOAS의 단점    구현 복잡성: 서버는 각 리소스에 적절한 하이퍼링크와 메타데이터를 제공해야 하므로 구현 난이도가 증가합니다.     성능 부담: 추가적인 링크 정보를 포함하기 때문에 응답 크기가 커질 수 있고, 처리 시간이 늘어날 수 있습니다.     지원 부족: 일부 클라이언트 라이브러리나 개발 환경에서 HATEOAS를 직접 활용하기 어려울 수 있습니다.  HATEOAS를 사용해야 할 때 •	API 사용자가 특정 리소스에서 어떤 작업을 수행할 수 있을지 동적으로 안내가 필요한 경우. •	대규모 애플리케이션에서 URL 구조가 자주 변경될 가능성이 높은 경우. •	클라이언트와 서버 간의 긴밀한 결합을 줄이고 독립성을 높이고자 할 때. HATEOAS와 RESTful API의 관계 HATEOAS는 REST의 핵심 원칙 중 하나이지만, 모든 RESTful API가 반드시 HATEOAS를 구현해야 하는 것은 아닙니다. RESTful API 설계에서 HATEOAS의 채택 여부는 시스템의 요구 사항과 복잡성에 따라 달라질 수 있습니다. Stateless: REST API 를 제공하는 서버는 세션을 해당 서버 쪽에 유지하지 않는다는 의미 유저의 상태가 서버에 저장되기 때문에, 세션 방식은 RESTFUL 하지 않다. Cacheable: HTTP 에선 기본값으로 캐싱이 된다. Client - Server 구조: 클라이언트와 서버가 서로 독립적인 구조를 가져야 합니다. 물론 이는 HTTP 를 통해 가능한 구조입니다. Layered System: 계층 구조로 나누어져 있는 아키텍처. REST API 의 URI 규칙:  동작은 HTTP 메소드로만 해야하고, url 에 해당 내용이 들어가면 안된다. 수정은 PUT, 삭제는 DELETE, 추가는 POST, 조회는 GET 확장자는 표시하면 안된다.  동사가 아닌 명사로만 표기 계층적인 내용을 담고 있어야 한다. /집/아파트/전세 처럼 내려가는 구조 대문자가 아닌 소문자만 쓰고, 너무 길 경우에는 언더바가 아닌 대시(-) 를 사용 HTTP 응답 상태코드를 활용. 성공 200, 생성 201, 리다이렉트 301 …그 외, api 업그레이드를 위해 Version 을 표기 (/wp/v2/posts?page=2) 브라우저 렌더링 과정아래의 과정이 동시에 일어난다.    DOM, CSSOM Tree 생성   DOM Tree : 각각의 태그 노드   CSSOM Tree : CSS 파서에 의해 정해진 스타일 규칙 적용   image. png     DOM 트리와 CSSOM 트리가 합쳐져 렌더 객체가 생성된다.  image. png 이때 display: none 이 포함된 노드는 지워지고, font-size 등 상속적인 스타일은 부모노드에만 위치하도록 설계하는 등의 최적화를 거쳐 렌더 레이어가 완성 된다. display: none 은 렌더트리에서 삭제 되지만, visibility: hidden 은 요소를 보이지 않게 할 뿐 여전히 레이아웃에서 공간을 차지한다. 또한 렌더 레이어가 완성될 때, GPU 에서 처리되는 부분이 있으면 이 요소들은 강제적으로 그래픽 레이어로 분리된다. 그래픽 레이어는 리플로우와 리페인트를 유발하지 않는다. GPT맞습니다. **그래픽 레이어**는 리플로우(reflow)와 리페인트(repaint)를 유발하지 않습니다. 대신 그래픽 레이어는 브라우저의 렌더링 성능 최적화와 관련이 있으며, GPU를 활용한 컴포지팅 단계에서 동작합니다. text	# **그래픽 레이어는 리플로우와 리페인트를 유발하지 않는다**		그래픽 레이어는 브라우저의 렌더링 최적화와 관련이 있으며, GPU를 활용한 **컴포지팅(compositing)** 단계에서 동작합니다. 따라서 리플로우(Reflow)나 리페인트(Repaint)를 유발하지 않습니다. 		---		## **리플로우와 리페인트 개념**		1. **리플로우(Reflow)** 	  - DOM의 구조나 레이아웃이 변경될 때 발생.  	  - 예: 요소의 크기, 위치, 폰트 크기 변경 등.  	  - **비용이 크며** 다른 요소들에게도 영향을 미칠 수 있음. 		2. **리페인트(Repaint)** 	  - 레이아웃은 변경되지 않지만 요소의 스타일(예: 색상, 그림자)이 변경될 때 발생.  	  - 리플로우보다 비용이 적지만 여전히 성능에 영향을 줄 수 있음. 		---		## **그래픽 레이어(Graphics Layer)란?**		그래픽 레이어는 브라우저가 특정 요소를 **별도의 GPU 가속 레이어로 분리**하여 관리하는 것을 의미합니다.  	이를 통해 레이아웃 계산이나 스타일 재계산 없이 GPU에서 직접 작업(컴포지팅)을 수행합니다. 		### **그래픽 레이어의 특징**	- **리플로우와 리페인트를 유발하지 않음. **	- GPU에서 병렬로 작업을 처리하여 성능을 향상시킴. 	- 독립적인 레이어로 분리되어 다른 레이어와 병렬로 처리 가능. 		---		## **그래픽 레이어가 생성되는 상황**		1. **CSS 속성**:	  - `transform` (예: `translate`, `scale`, `rotate` 등)	  - `opacity`	  - `will-change` (예: `will-change: transform, opacity;`)	  - `filter`		2. **CSS 애니메이션 또는 트랜지션**:	  - GPU 가속이 필요한 속성(`transform`, `opacity` 등)에 대한 애니메이션. 		3. **하드웨어 가속 사용**:	  - 예: `translate3d` 또는 `translateZ` 사용 시. 		---		## **그래픽 레이어의 장점**		1. **성능 최적화**:	  - 레이아웃 계산과 페인트 작업의 부하를 줄임. 	  - GPU에서 병렬적으로 작업을 처리해 성능을 크게 개선. 		2. **애니메이션 부드러움**:	  - GPU 가속을 활용해 애니메이션을 부드럽게 렌더링. 	  - 리플로우와 리페인트를 유발하지 않음. 		---		## **예시**		### **리플로우/리페인트를 유발하는 경우**	css	. element {	 width: 100px;	 height: 100px;	 background-color: red;	 transition: width 1s;	}			•	width 속성은 레이아웃을 변경하므로 리플로우가 발생. 		그래픽 레이어를 활용한 경우	. element {	 width: 100px;	 height: 100px;	 background-color: red;	 transform: translateX(0);	 transition: transform 1s;	}		•	transform 속성은 레이아웃을 변경하지 않으므로 리플로우나 리페인트를 유발하지 않음. 		•	GPU에서 작업이 처리되어 성능이 더 부드럽고 빠름. 				그래픽 레이어는 GPU 가속을 활용한 컴포지팅 단계에서 처리되므로 리플로우와 리페인트를 유발하지 않습니다. 	성능 최적화가 필요한 애니메이션이나 트랜지션에서는 그래픽 레이어를 적극적으로 활용하는 것이 좋습니다. 	 레이아웃 잡기 (Layout) 렌더레이어를 대상으로 칠하기 (Paint)   레이어 합치기 (Composite Layer) 및 표기   각각의 레이어로부터 비트맵이 생성되고, GPU에 텍스쳐로 업로드 됩니다. 그 다음 텍스쳐들은 서로 합쳐져 하나의 이미지로 렌더링되며 화면으로 출력됩니다.   💡 렌더트리와 DOM 트리는 1:1 대응일까요?아닙니다. DOM 트리 &gt; 렌더 객체 &gt; 렌더 트리가 되는 과정에서 display: none 으로 사라지는 렌더 객체들이 있을 수 있기 때문. 아닙니다. DOM 트리 &gt; 렌더 객체 &gt; 렌더 트리가 되는 과정에서 display: none 으로 사라지는 렌더 객체들이 있을 수 있기 때문. www. naver. com 을 주소창에 입력했을 때 생기는 과정, 그리고 DNS 까지 설명해주세요강의 답변리다이렉트, 캐싱, DNS, IP 라우팅, TCP 연결 구축을 거쳐요청, 응답이 일어나는 TTFB(TIme TO First Byte) 가 시작되고이후 컨텐츠를 다운받게 되고,브라우저 렌더링 과정을 거쳐 네이버라는 화면이 나타나게 됩니다. 리다이렉트: 리다이렉트가 있다면 리다이렉트를 진행하고, 없다면 그대로 해당 요청에 대한 과정이 진행 캐싱: 해당 요청이 캐싱이 가능한지, 가능하지 않은지를 파악합니다. 캐싱이 이미 된 요청이라면 캐싱된 값을 반환하며, 캐싱이 되지 않은 새로운 요청이라면 그 다음 단계로 넘어갑니다. 캐싱은 요청된 값의 결과값을 저장하고 그 값을 다시 요청하면 다시 제공하는 기술입니다. 브라우저 캐시와, 공유 캐시로 나뉘어집니다. 브라우저 캐시: 쿠키, 로컬스토리지 등을 포함한 캐시 (개인, private 캐시) 공유 캐시: 클라이언트와 서버 사이에 있으며, 사용자간에 공유할 수 있는 응답을 저장할 수 있다. 예를 들어, 요청한 서버 앞 단에 프록시 서버가 캐싱을 하고 있는 것. 이를 리버스 프록시를 둬서 내부 서버로 포워드한다고도 말합니다. DNS: Domain Name System 은 계층적인 도메인 구조와 분산된 데이터베이스를 이용한 시스템으로, FQDN 을 인터넷 프로토콜인 IP 로 바꿔주는 시스템입니다. 이는 DNS 관련 요청을 네임서버로 전달하고, 해당 응답값을 클라이언트에게 전달하는 Resolver, 도메인을 IP 로 변환하는 네임서버 등으로 이루어져있습니다. FQDN: FQDN (Fully Qualified Domain Name) 은 호스트와 도메인이 합쳐진 완전한 도메인 이름을 말합니다. www. naver. com 에서 www 등은 호스트 부분 혹은 서브 도메인, naver. com 은 도메인이라고 합니다. DNS 캐싱: 미리 해당 도메인 이름을 요청한 적이 있다면, 로컬 PC 에 자동적으로 저장됩니다. 브라우저 캐싱과, OS 캐싱이 있습니다. gpt```text# **DNS 캐싱**DNS 캐싱은 도메인 이름을 요청할 때 발생하는 네트워크 트래픽을 줄이고, 더 빠른 응답 속도를 제공하기 위해 이전에 조회한 도메인 이름과 IP 주소를 **로컬에 저장**하는 기술입니다.  - 즉, 이미 요청한 도메인 이름이 있다면, DNS 서버에 다시 요청하지 않고 **캐싱된 데이터**를 사용합니다. ---## **DNS 캐싱의 종류**DNS 캐시는 **다양한 계층**에서 발생할 수 있습니다. 주요 캐싱 계층은 다음과 같습니다:### 1. **브라우저 캐싱**- **웹 브라우저**는 자체적으로 DNS 캐시를 관리합니다. - 동일한 도메인 이름을 여러 번 요청하는 경우, 브라우저 캐시에서 IP 주소를 가져와 빠르게 처리합니다. - 브라우저별로 DNS 캐싱 시간(TTL, Time to Live)이 다를 수 있습니다.  - 예: Chrome, Firefox 등에서 TTL 설정에 따라 캐시가 유지됩니다. #### **브라우저 DNS 캐시 확인 및 삭제**- Chrome: `chrome://net-internals/#dns`로 접속하여 캐시를 확인하거나 삭제할 수 있음. ---### 2. **OS 캐싱**- 운영체제(Windows, macOS, Linux)는 **DNS Resolver Cache**라는 이름으로 자체적인 DNS 캐시를 관리합니다. - 애플리케이션(예: 브라우저)에서 DNS 조회 요청이 들어오면, DNS 서버에 요청하기 전에 **운영체제의 캐시**를 먼저 확인합니다. #### **OS DNS 캐시 확인 및 삭제 방법**- **Windows**:  - 확인: 명령 프롬프트에서 `ipconfig /displaydns` - 삭제: 명령 프롬프트에서 `ipconfig /flushdns`- **macOS**:  - 삭제: `sudo dscacheutil -flushcache &amp;&amp; sudo killall -HUP mDNSResponder`- **Linux**:  - 삭제: `sudo systemd-resolve --flush-caches` (시스템에 따라 명령어가 다를 수 있음)---## **DNS 캐싱의 장점**1. **빠른 응답 속도**:  - 캐시된 데이터를 사용하여 DNS 서버와의 통신 시간을 줄임. 2. **네트워크 트래픽 감소**:  - DNS 요청이 줄어들어 네트워크 대역폭을 절약. 3. **서버 부하 감소**:  - DNS 서버로의 요청 수가 줄어들어 서버 부하를 완화. ---## **DNS 캐싱의 단점**1. **오래된 정보**:  - TTL(Time to Live)이 만료되지 않은 경우, IP 주소가 변경되었더라도 **오래된 캐시 데이터를 사용할 수 있음**.  - 이로 인해 사용자가 변경된 서버에 접속하지 못하거나 오류가 발생할 수 있음. 2. **캐시 초기화 필요**:  - DNS 관련 문제를 해결하려면 브라우저, OS, 네트워크 장치의 캐시를 수동으로 초기화해야 할 때가 있음. ---## **TTL(Time to Live)**- DNS 캐시는 **TTL(Time to Live)** 값을 기반으로 유지됩니다. - TTL은 DNS 응답에 포함된 값으로, 특정 도메인-IP 매핑이 캐시에서 얼마나 오래 유지될지를 정의합니다. - TTL이 만료되면, 캐시된 데이터는 삭제되고 새 데이터를 DNS 서버에서 요청하게 됩니다. ---## **정리**DNS 캐싱은 **브라우저 캐싱**과 **OS 캐싱**의 두 가지 주요 계층에서 이루어지며, 인터넷 속도를 향상시키고 서버 부하를 줄이는 데 큰 역할을 합니다.  문제가 발생했을 때는 캐시를 수동으로 초기화하거나 TTL 설정을 확인해 문제를 해결할 수 있습니다. ```IP 라우팅: 해당 IP 를 기반으로 라우팅, ARP 과정을 거쳐 실제 서버를 찾습니다. TCP 연결 구축 (HTTP ~2. 0): 브라우저가 TCP 3Way handshake 및 SSL 연결 등을 통해 연결을 설정합니다. 이후에 요청을 보냄으로써 서버로부터 응답을 받습니다. QUIC 연결 (HTTP 3. 0): 컨텐츠 다운로드: 요청한 컨텐츠를 서버로부터 다운 받습니다. 처음 다운로드를 시작할 때, TTFB (Time To First Byte) 라고 합니다. 브라우저 렌더링: 받은 데이터를 바탕으로 브라우저 엔진이 브라우저 렌더링 과정을 거쳐 화면을 만듭니다. 이더넷 프레임은 무엇이며, 구조가 어떻게 되나요?image. png 이더넷프레임이란 데이터 링크계층의 데이터 단위이며 이더넷 프레임을 기반으로 데이터가 전달 되며 다음과 같은 필드로 구성되어있습니다. **Preamble** : [7바이트]이더넷 프레임이 시작임을 알려준다. **SFD** : [1바이트]Start frame delimiter, 다음 필드부터 주소필드가 시작됨을 알려줍니다. **Address** : [6, 6바이트]목적지 주소, 시작지 주소입니다. **EtherType** : [2바이트]데이터링크계층 위의 IP 프로토콜을 정의합니다. (IPv4, Ipv6) **Payload** : [가변바이트]데이터 또는 페이로드라고 합니다. 가변길이 필드입니다. 해당 필드는 이진데이터(0과1로 이루어진 데이터)로 구성됩니다. IEEE 표준은 최대 페이로드를 1500 바이 트로 지정하고 있습니다. **FCS** : [4바이트]frame check sequence이며 수신측의 에러검출을 위해 삽입하는 필드입니 다. CRC 에러검출 기법에 의해 생성된 비트배열이 여기에 담깁니다. 비트배열을 기반으로 수신된 데이터가 손상되었는지를 확인하고, 에러 확인 시에는 해당 프레임을 폐기하고 송신측에 재전송을 요구하기 위한 필드입니다. CRC 알고리즘에 의해 만들어진 비트 배열이 담기는 필드 CORS란 무엇인가요?Origin: image. png 오리진은 프로토콜 + 호스트네임 + 포트 예를 들어, https://shopping. naver. com/home 에서 https , shopping. naver. com , 8080 SOP (Same Origin Policy): 브라우저 상에서 오로지 같은 오리진끼리만 요청을 허가하는 보안 정책. 브라우저 상에서 다른 오리진끼리는 통신이 불가능하다. 다른 오리진에서 요청을 하는 공격에 방지하기 위함 CORS (Cross Origin Resource Sharing): 다른 오리진과 통신을 해야하는 일이 있을 수 있기 때문에, SOP 보다 조금 느슨한 규칙 포트번호만 달라도 오리진이 다르기 때문에 개발 과정에서 백/프 끼리도 보통 오리진이 다르다 HTTP 헤더를 기반으로 브라우저가 다른 오리진에 대한 리소스(image, css, js, video 등) 로드를 허용할지 말지에 대한 규칙 Preflight Request, Simple Request: CORS는 브라우저가 다른 출처(origin)의 리소스에 접근할 때 안전을 보장하기 위해 사용하는 메커니즘입니다. 요청은 크게 두 가지로 나뉩니다 1. Simple Request (단순 요청): 정의:  브라우저가 사전 확인(preflight)을 생략하고 직접 요청을 서버에 보내는 요청입니다.  CORS 규칙에서 Simple Request는 특정 조건을 만족해야 합니다. Simple Request의 조건: Simple Request가 되기 위해서는 아래 조건을 모두 충족해야 합니다:  HTTP 메서드가 다음 중 하나여야 합니다:     GET   POST   HEAD    헤더(Header):     요청에 사용된 헤더가 CORS에서 허용된 간단한 헤더(Simple Headers)만 포함해야 합니다.    허용된 간단한 헤더:         Accept     Accept-Language     Content-Language     Content-Type (단, 값이 application/x-www-form-urlencoded, multipart/form-data, 또는 text/plain일 경우만 허용)           Content-Type:     요청 본문(Body)의 Content-Type 값이 아래 중 하나여야 합니다:         application/x-www-form-urlencoded     multipart/form-data     text/plain           Credentials:     요청에 Authorization 헤더나 쿠키와 같은 자격 증명(크리덴셜)이 포함되지 않아야 함 (withCredentials가 false).    동작 방식:  브라우저는 요청을 바로 서버에 보냅니다.  서버는 요청의 출처(origin)와 헤더를 보고 허용 여부를 판단합니다.  서버가 허용하면 브라우저는 응답을 처리합니다. 2. Preflight Request (사전 요청): 정의:  요청이 Simple Request 조건을 충족하지 않으면 브라우저는 Preflight Request를 먼저 보냅니다.  Preflight Request는 실제 요청 전에 서버가 해당 요청을 허용할지 여부를 확인하는 안전성 검사입니다. Preflight Request의 동작:  브라우저는 HTTP OPTIONS 메서드로 사전 요청을 보냅니다.  Preflight Request에는 아래와 같은 정보를 포함합니다:     Origin: 요청의 출처.    Access-Control-Request-Method: 실제 요청에 사용될 HTTP 메서드.    Access-Control-Request-Headers: 실제 요청에서 사용될 커스텀 헤더(있을 경우).     서버는 Preflight Request에 대해 응답합니다:     Access-Control-Allow-Origin: 허용된 출처.    Access-Control-Allow-Methods: 허용된 HTTP 메서드.    Access-Control-Allow-Headers: 허용된 커스텀 헤더.    응답 상태 코드가 200이면 브라우저는 실제 요청을 보냅니다.    Preflight가 필요한 조건: Preflight Request는 다음 중 하나 이상에 해당할 때 발생합니다:  요청 메서드가 GET, POST, HEAD가 아닌 경우 (PUT, DELETE, PATCH 등).  요청 헤더가 간단한 헤더 외의 커스텀 헤더를 포함하는 경우.  Content-Type이 application/x-www-form-urlencoded, multipart/form-data, text/plain이 아닌 경우 (예: application/json).  요청에 자격 증명(크리덴셜, 예: Authorization 헤더나 쿠키)이 포함된 경우. 네이글 알고리즘이란 무엇인가요?네이글 알고리즘은 네트워크의 효율성을 높이기 위해 네트워크를 통해 전송되어야 하는 패킷의 수를 줄이고 적은 양의 데이터를 자주 보내면 발생되는 대역폭 낭비를 막아주는 방법입니다. image. png 기존 방법과 다르게 패킷을 보낼 때 고정 크기의 버퍼에 모아두고 버퍼에 어느정도 차면 그 때 보내고, 다시 ACK 가 오면 버퍼에 찬 데이터를 보내는 방법 image. png 네이글 알고리즘의 장점:  전송하는 패킷의 수가 줄어들기 때문에 네트워크의 혼잡도를 줄이고 패킷 손실률을 줄일 수 있다.  작은 패킷들을 많이 전송할 때, 각 패킷에 포함된 헤더의 크기가 커져서 대역폭 소모가 많이 발생할 수 있다. 패킷의 수를 줄임으로써 이러한 오버헤드를 줄일 수 있다. 네이글 알고리즘의 단점:  작은 패킷의 수를 줄이기 위해 데이터를 버퍼링하고, 충분한 양의 데이터가 모일 때 까지 패킷 전송을 지연시킨다.  ACK를 받기 전까지 데이터를 전송하지 않기 때문에 지연시간이 발생할 수 있다. (실시간이 중요하다면 사용하기 힘들다)image. png HTTP 의 멱등성에 대해 설명해주세요HTTP 멱등성(idempotent)이란 하나의 요청이 아닌 여러번 동일한 요청을 보냈을 때 서버가 같은 상태를 가지는 것을 멱등성이라고 합니다. https://docs. tosspayments. com/blog/what-is-idempotency 멱등성을 가지는 메소드: GET (안전한 메소드): 서버에서 정보를 가져오는 것을 여러 번 해도, 동일한 정보를 가져온다. HEAD (안전한 메소드): 헤드는 GET 요청이 생성하는 응답의 헤더를 검색하는데 사용됩니다. 따라서 서버의 상태를 변경하지 않습니다. OPTIONS (안전한 메소드): 대상 리소스의 통신 옵션 ( 서버에서 지원하는 HTTP 메서드 등)을 가져올 때 사용 PUT (안전한 메소드는 아님): 서버에 있는 데이터를 아예 교체하기 때문에 여러 번 보내도 똑같은 데이터로 교체될 것입니다. DELETE (안전한 메소드는 아님): 삭제 작업을 반복해도 시스템 상태가 더 이상 변경되지 않습니다. 그런데 흠. . 예를 들어 DELETE /post/lastest 라고 해보자. 가장 최신 포스트를 지우게 된다고 하면 서버의 상태가 바뀌지 않나? 또한 만약 DELETE /post/abc 인 상황에서 abc 라는 리소스가 여러 개라면? DELETE 는 abc 리소스를 하나만 지우는 상황이라면 어떻게 될까? 이러면 또 멱등성을 보장할 수 없는게 아닌가?  DELETE는 멱등성을 보장하는 메서드이지만, 대상 리소스를 정확히 지정해야 합니다. 예를 들어, DELETE /posts/123와 같이 구체적인 리소스를 명시해야 합니다.  DELETE /posts/last와 같이 마지막 게시글을 삭제하는 요청은 상태 변화가 계속 일어나므로 멱등성을 가지지 않습니다.  이런 경우에는 POST를 사용하는 것이 더 적절합니다. 요약: DELETE는 멱등성을 보장하므로 상태 변화가 없을 때 사용해야 합니다. 상태가 계속 바뀌는 경우에는 POST를 사용하는 것이 HTTP 스펙상 더 맞습니다. POST: 서버에 동일한 요청을 여러 번 보내면, 보낸 만큼 생성 될 것이기 때문에 서버의 상태가 바뀌게 된다. PATCH: 리소스를 부분적으로 업데이트 하게 되는데, 멱등성의 여부는 PATCH 를 실행하는 방법에 따라 달라진다. 예를 들어 특정 부분을 새롭게 교체하는거라면 멱등성을 가지지만, 하나씩 증가하거나 빼기, 배열에 요소 추가 등의 방식은 멱등성을 가지지 않는다. 함수형 프로그래밍이 생각나네 HTTP 멱등성을 고려한 API 설계: POST 메서드의 경우 idempotency Key를 보내서 동일한 요청인지를 식별하게 합니다. 이를 통해 동일한 작업의 재시도를 인식하고 한 번만 처리되도록 합니다. 이는 중복 트랜잭션을 피해야 하는 결제 처리 API에서 자주 사용됩니다. Idempotency Key```textIdempotency Key는 중복 요청을 방지하고 멱등성을 보장하는 데 중요한 역할을 하는 기법입니다. 특히 POST 메서드에서는 요청을 여러 번 보내면 여러 번 처리가 발생하는 경우가 많기 때문에, 이를 방지하기 위해 Idempotency Key를 사용합니다. 1. Idempotency Key의 개념Idempotency Key는 클라이언트가 서버에 보내는 고유한 값으로, 동일한 작업을 여러 번 요청하더라도 서버에서 해당 작업을 한 번만 처리하도록 보장하는 키입니다. 이는 멱등성을 보장하기 위한 기술로, 중복 트랜잭션이나 중복 작업이 발생하지 않도록 합니다. Idempotency Key는 클라이언트가 POST 요청을 보낼 때마다 함께 전송합니다. 이 키는 요청이 중복되었는지 여부를 서버가 인식할 수 있게 해줍니다. 2. POST 메서드와 멱등성일반적으로 POST 메서드는 리소스를 생성하거나 데이터를 변경하는 요청에 사용됩니다. 이런 요청은 여러 번 실행될 경우 여러 번 처리되거나 중복된 리소스가 생성될 수 있습니다. 예를 들어:사용자가 결제를 시도하는 경우, 결제 요청을 여러 번 보내면 여러 번 결제가 처리될 위험이 있습니다. 서버가 POST 요청을 여러 번 받아들여 중복된 리소스를 생성하거나 트랜잭션을 두 번 처리할 수 있습니다. 3. Idempotency Key의 역할Idempotency Key는 이러한 중복 처리를 피하기 위한 방법으로 사용됩니다. 서버는 동일한 Idempotency Key를 가진 요청이 들어오면, 이미 처리한 작업을 다시 처리하지 않고 응답을 재사용합니다. 예시: 결제 처리 API결제 처리 API에서 Idempotency Key가 어떻게 동작하는지 살펴봅시다:클라이언트가 결제 요청을 보냄:클라이언트는 결제 요청과 함께 고유한 Idempotency Key를 생성하고 보냅니다. 예: POST /payment 요청과 함께 {  idempotencyKey :  12345  }를 보냄. 서버는 Idempotency Key를 확인:서버는 요청을 처리하기 전에 이 Idempotency Key가 이전에 처리된 적이 있는지 확인합니다. 만약 이 키가 이미 처리된 적이 있다면, 서버는 새로운 결제를 생성하지 않고, 이전 응답을 그대로 재사용합니다. 만약 이 키가 처리되지 않은 키라면, 서버는 결제를 처리하고 새로운 결제 결과와 함께 Idempotency Key를 저장합니다. 동일한 요청이 여러 번 들어오면:두 번째 요청에서 동일한 Idempotency Key가 포함되면, 서버는 이미 처리된 요청으로 인식하고, 같은 결과를 반환합니다. 이로써 중복 결제나 트랜잭션이 발생하지 않게 됩니다. 4. Idempotency Key가 필요한 이유네트워크 불안정성:클라이언트가 네트워크 문제로 요청이 실패했다고 생각하고 요청을 다시 보낼 수 있습니다. 그러나 실제로 첫 번째 요청은 서버에서 성공적으로 처리되었을 수 있습니다. Idempotency Key는 서버가 이 중복 요청을 인식하고, 두 번째 요청에서 이전의 결과를 반환함으로써 중복 처리를 방지합니다. 중복 트랜잭션 방지:결제나 주문 처리 등과 같은 중요한 작업에서는 동일한 트랜잭션이 두 번 이상 처리되면 큰 문제가 될 수 있습니다. Idempotency Key는 이러한 중복 트랜잭션을 피하기 위해 사용됩니다. 시스템의 안정성 증가:여러 번 요청을 보내더라도 시스템은 항상 동일한 결과를 반환하고, 불필요한 리소스 소모나 상태 변경을 피할 수 있습니다. 5. 실제 예시결제 처리 API 예시:json코드 복사POST /payments{  amount : 100,  currency :  USD ,  idempotencyKey :  12345 }첫 번째 요청: 서버는 idempotencyKey:  12345 를 인식하고 결제를 처리한 후, 결제 정보를 반환합니다. 두 번째 요청 (같은 Idempotency Key): 서버는 이미 idempotencyKey:  12345 에 대한 요청을 처리했음을 인식하고, 결제를 중복 처리하지 않고 이전 응답을 그대로 반환합니다. 6. Idempotency Key 관리 방법서버에서 Key를 저장: 서버는 각 Idempotency Key와 관련된 요청을 처리하고, 그 결과를 저장합니다. 일정 기간 동안 같은 idempotencyKey가 사용되면 서버는 그 결과를 반환하고, 이후에는 해당 키를 삭제하거나 만료시킬 수 있습니다. 유효 기간 설정: idempotencyKey는 보통 일정 기간 동안만 유효하며, 이를 통해 서버의 메모리나 저장소에 과도한 부하를 줄일 수 있습니다. 7. 결론Idempotency Key는 멱등성을 보장하는 중요한 도구입니다. POST 메서드에서 중복 요청으로 인한 트랜잭션 문제를 방지하고, 동일한 작업을 여러 번 요청해도 한 번만 처리되도록 보장합니다. 결제 처리, 주문 생성, 리소스 생성 등 중복된 작업을 방지해야 하는 중요한 작업에서 널리 사용됩니다. ```멱등성이 중요한 이유:  일관성 : 보통 반복되는 요청이 단일 요청과 동일한 효과를 갖도록 보장하는 것은 시스템 전반에 걸쳐 일관된 상태를 유지하는 데 도움이 됩니다.  유지보수성 : 멱등성을 가진 HTTP메서드로 AP로 통신하는 부분에 있어서는 재시도를 처리할 때의 사이드이펙트를 고려한 복잡한 로직을 구현할 필요가 없기 때문에 이 부분을 잘 고려해서 설계한다면 유지보수성이 증가 됩니다. "
    
}, 

{
    "id": 3,
    "url": "https://hoex5h.github.io//Effective_TypeScript_-_2",
    "title": "Effective TypeScript - 2",
    "body": "2025/01/14 -  이펙티브 타입스크립트 책과이펙티브 타입스크립트 스터디 영상을 공부하며 정리한 내용입니다. 타입스크립트 컴파일러 역할 구 버전 브라우저를 위한 트랜스파일 -&gt; Babel 의 역할이었던거 같은데? 코드의 타입 오류 체크두 역할은 서로 독립적이다. 서로의 작업에 영향을 주지 않는다. 타입 오류가 있어도 컴파일이 가능: 컴파일은 타입 체크와 독립적으로 동작하므로 오류 있어도 컴파일은 가능 만약 오류가 있을 때 컴파일 하고 싶지 않으면 tsconfig. json 에 noEmitError 옵션을 true 로 주면 된다. 런타임에는 타입 체크 불가능: 타입스크립트에서 자바스크립트로 컴파일되면서 타입이나 인터페이스 같은 경우 전부 제거된다. 즉, 런타임 시점에서 타입과 인터페이스는 아무런 역할을 하지 못한다. 함수 오버로딩 불가능: 선언문은 타입 별로 여러개 선언은 가능하다. 하지만 구현체는 오직 하나다. 덕타이핑: interface 와 duck-typing Any 타입, 꼭 써야겠어? any 타입 쓰면 타입에 안전성이 없다. 쓰면 number 타입에 string 이 할당될 수도 있어 any 타입 쓰면 함수의 시그니처(약속/의도 정도로 해석?)가 무시된다.  any 타입 쓰면 자동완성 기능, 코드 리팩토링 도구 쓰기 힘들어져 -&gt; 생산성 떨어진다! any 타입 쓰면 코드 리팩토링할 때 버그를 찾기 힘들어 any 타입 쓰면 타입 설계의 의도, 상태 등이 감춰져서 설계를 해석하기 힘들어 any 타입 쓰면 실수가 빈번해져이래도 any 타입, 꼭 사용하셔야겠습니까? 근데 불가피하게 any 타입을 꼭 써야할 때가 있다. 이는 5장에서 다룬다. "
    
}, 

{
    "id": 4,
    "url": "https://hoex5h.github.io//Swift-_Cocoa_Pod_install_Error",
    "title": "[Swift] Cocoa Pod install Error",
    "body": "2025/01/14 - Cocoa Pod install Error에러 내용: textYou may have encountered a bug in the Ruby interpreter or extension libraries. Bug reports are welcome. For details: &lt;https://www. ruby-lang. org/bugreport. html&gt;원인: M1 맥북에서 일어나는 버그라고 하는데,, M1이 출시한지 이렇게나 오래 되었는데 아직도 ㅠ 해결 방법: text$ sudo arch -x86_64 gem install ffi$ arch -x86_64 pod install이렇게 해서 해결할 수 있다고 한다. "
    
}, 

{
    "id": 5,
    "url": "https://hoex5h.github.io//Swift-_@AppStorage",
    "title": "[Swift] @AppStorage",
    "body": "2025/01/14 - @AppStorageSwift 에서도 코틀린처럼 기기 내부에 데이터를 저장할 수 있는 방법이 있다. 코틀린에서는 SharedPreference 를 사용했었는데, Swift 에서는 @AppStorage 를 사용한다. 다만 차이점이 있었는데, text@AppStorage( onboarding ) var isOnboardingViewActive : Bool = true위의 코드대로만 보면 어느 파일에서든 “onboarding” 프로퍼티를 사용하려면 true(혹은 false) 로 초기화를 해야하는 것처럼 보인다.  사실은 계속 초기화가 되는 것이 아니라, 기기 내부 저장소에서 onboarding 프로퍼티를 찾지 못하는 경우에만 true로 초기화 한다는 뜻이고, 이미 존재한다면 해당 초기화 구문은 무시된다고 한다. "
    
}, 

{
    "id": 6,
    "url": "https://hoex5h.github.io//Effective_TypeScript_-_1",
    "title": "Effective TypeScript - 1",
    "body": "2025/01/14 -  이펙티브 타입스크립트 책과이펙티브 타입스크립트 스터디 영상을 공부하며 정리한 내용입니다. TS, JS타입스크립트는 자바스크립트의 상위 집합 이다. 타입스크립트는 타입이 정의된 자바스크립트의 상위 집합이다. 모든 JS 프로그램이 TS 라는 명제는 참, 반대는 성립하지 않는다. TS 타입체커typescript// tslet sample =  this is a sample ;console. log(sample. uppercase());// uppercase 라는 메소드가 string 형식에 없다!// toUpperCase() 를 사용하시겠습니까?이런식으로 sample 이 string 이라는걸 명시해주지 않아도 타입스크립트는 초기값으로부터 추론한다.  코드의 의도가 무엇인지 타입 구문을 통해 타입스크립트에게 알려줄 수 있기 때문에 코드의 동작과 의도가 다른 부분을 찾을 수 있다. 코드의 의도를 표현하려면 명시적으로 선언해라 ex) interface typescriptinterface person { name: string; age: number;}const people: Person[] = [ // Person 예시 { name:  John , age: 5 }, //. . . ];자동 형변환 체크?typescriptconst a = 2 +  3 ; // 자바스크립트 런타임에서 정상, String 타입의 '23'const b = null + 7; // 자바스크립트 런타임에서 정상, Number 타입의 7, 그러나 typescript 는 오류를 발생시킴const c = [] + 12; // 자바스크립트 런타임에서 정상, String 타입의 '12', 그러나 typescript 는 오류를 발생시킴// 근데 이건 왜 String 타입이 되는걸까?alert( Hi ,  world ); // 자바스크립트 런타임에서 정상, 앞의  Hi  만 나옴. 하지만 alert는 arg가 0개~1개 인데 2개를 가져왔다고 ts 오류모든걸 다 잡아주지는 않는다. textconst a = [ hi ];console. log(a[2]); // 컴파일은 정상, 런타임에서 오류문법의 엄격함은 온전히 취향의 차이이다. 타입스크립트 컴파일러의 셋팅에 따라 바뀐다. tsconfig. json 파일에서 여러 옵션으로 셋팅을 줄 수 있다. 타입스크립트를 어떻게 사용할 지 동료들에게 알려줄 수 있다. 예를 들어, noImplicitAny 옵션은 타입 선언이 없으면 암시적으로 any 타입으로 선언하는 옵션인데, false 와 true 로 옵션을 켜고 끌 수 있다. strictNullChecks 는 모든 타입에 null 과 undefined 를 할당할 수 있는지에 대한 옵션이다. null 을 명시적으로 허용하려면, const x : number | null = null; 처럼 사용하는 것이 좋다. 만약 이렇게 되면 Null check 를 위한 assertion(단언문) 이 필요하다. textif (x. abc) { // null 이 아닐때}x!. abc; // x가 Null 이 아니라고 단언한다웬만하면 noImplicit, strictNullChecks 는 설정해두고 시작해라. 안그러면 undefined is not a Object 오류를 계속 만날 것이다. 엄격하게 잡고 싶으면 그냥 strict 옵션을 true 로 줘라. "
    
}, 

{
    "id": 7,
    "url": "https://hoex5h.github.io//%EB%84%A4%EB%B6%80%EC%BA%A0_%EC%B1%8C%EB%A6%B0%EC%A7%80-_%EC%B1%8C%EB%A6%B0%EC%A7%80_%EC%B5%9C%EC%A2%85_%ED%9A%8C%EA%B3%A0",
    "title": "[네부캠 챌린지] 챌린지 최종 회고",
    "body": "2025/01/14 - 베이직부터 시작해서 벌써 챌린지까지 마무리가 되었다. 어렵지 않으면 도전이 아니라는 모토처럼 챌린지는 잠을 쪼개가면서 해야할 정도로 힘들었지만, 마냥 힘든 감정만 있던건 아니고 내 스파게티 코드를 남들에게 보여줘야하는 두려움과 다른 사람의 코드를 보고 배울 수 있고 피드백을 통해 내 코드를 개선할 수 있다는 기대감이 공존하는 지킬앤하이드 같은 4주였다. 이 글을 읽는 사람이 챌린지에 입과하는 사람이라면 챌린지 과정이 어떤 느낌인지 깨닫는데 걸리는 시간을 줄이고. 챌린지를 수료한 동료 캠퍼라면 같이 챌린지 과정을 돌아보는데 도움이 되었으면 한다. 나는 제대로 아는게 없다상당히 과격한 첫 제목 같지만 챌린지 내내 이 생각을 가지고 살았다. 진짜 제대로 아는게 없다. 미션이 주어지면 분명 학교 다니면서 몇 번 들어봤을, 수업도 들어봤던 내용이어도 검색하고 학습하지 않고선 진행할 수가 없었다. 그러다보니 구현을 시작하는게 점점 늦어지고, 조급해지고, 동시에 학습 시간을 줄이고 구현하려고 하면서 스스로 자책도 조금 하는 1,2 주차를 보냈던 것 같다. 구현을 못할수록 그 미션은 매우 못했다고, 구현을 그나마 좀 했다면 그 미션은 어느정도 완성했다고 착각했다. 거의 2주차가 끝나갈때쯤 되어서야 미션을 구현하는거에만 너무 많이 매달리지 말자고 생각했던 것 같다. 나만 어렵고 나만 고민하는게 아니고 모든 동료 캠퍼분들이 스스로의 기반을 토대로 학습과 구현사이에서 저울질 하고 있다 라고 깨닫게 되었다. 1주차 2주차에는 제대로 아는게 없다는 사실에 좌절했다면, 그 뒤로는 인정하고 어떻게 하면 챌린지 기간동안 내가 더 많은 걸 얻어갈 수 있을까를 고민했다. 그래서 다른 동료 캠퍼들의 장점을 보고 배우고, 내 학습과 정리, 구현에 적용해보면서 개선해나가는걸 목표로 삼았다. 이전에 해오던 방식으로 구현에만 몰두해서 새벽까지 구현하던 것과는 확실히 차이가 있었고 이제는 무작정 구현만을 위해서 시간을 보내는건 나한테는 맞지 않는다고 생각한다. 이렇게 시행착오를 겪으면서 어떤 방식이 나한테 맞지 않는가를 어느정도 알 것 같다. 하지만 나에게 딱 맞는, 마치 정답같은 방식은 아직도 확실하지는 않다. 내가 했던 생각은 “그저 지난 주 보다 더 나은 한 주가 되자” 정도 였고지난 주에 불만족스러웠던 점을 다음 주에 개선해보면서, 만족스러운 개선이었다면 잘하고 있다고 믿었다. 잘 하는 사람이 너무 많다챌린지는 정말 잘 하는 동료 캠퍼분들이 많다. 코드가 깔끔하거나, 설계가 명확하거나 학습정리가 완벽하다거나 등등. . 볼 때마다 주눅이 들 것 같은 장점을 가지신 분들이 정말 많다. 사실 네부캠을 비롯해서 거의 모든 사람이 남들과 비교하고 자신을 깎아내리는걸 하지 않기를 바란다. 근데 막상 같이 얘기해보고 코드를 뜯어보고 같이 화면을 바라보고 있으면 그게 쉽지는 않다. 자기 자신을 자책하고 남을 부러워하는걸 안하려고 해도 자꾸 하게 된다. 그래서 자책하고 부러워도 하면서 그 사람의 장점을 좀 흡수해보는 시도를 한다면 충분하다고 생각한다. 학습 정리이번 챌린지를 진행하면서 학습 정리가 중요하다는걸 많이 느꼈다. 1주차 2주차에는 네부캠에서 하라고 시키니까 한다는 느낌이 강했다. 어느 순간 내가 내 블로그에 들어가서 이전에 정리해둔걸 찾으면서,내 학습 정리를 제일 자주 보는 사람은 나인데 왜 이렇게 대충했지 라는 생각이 많이 들었다. 그래서 최소한 자신이 보기에 괜찮을 정도로 정리나 분류가 필요하지 않을까라는 생각을 하게 되었다. 이럴 땐 역시 동료 캠퍼분들만한 참고서가 또 없다. 동료분들의 학습 정리를 보면서 많이 배우고 차용하려고 노력했다. 앞으로는챌린지를 통해 느끼고 배운 점들은 단순히 취업 준비를 넘어서, 앞으로 개발자로서 꾸준히 참고할 지침서 같은 느낌이다. 이 경험을 단순히 ‘열심히 했다’에서 그치지 않고, 나만의 습관으로 만들고 꾸준히 발전시켜 나가고 싶다. 지금까지는 방향보다는 그저 어디론가 나아가는 데만 집중해 왔다. 하지만 이번 챌린지에서는 나만의 속도와 방향을 찾기 위해 노력했다. 앞으로도 개발자로서 내 방식대로 길을 찾고 정리하며, 끊임없이 돌아보는 과정을 이어가고자 한다. 끝으로챌린지 과정은 재밌기도 하고 힘들기도 하고 시간이 빨리 지나가기도 하고 기대되기도 하고 두렵기도 한. . 그런 과정이다. 인생에 이런 기회가 자주 없을거라는 수료생분의 말씀에 적극적으로 공감하면서, 힘들어서 눈물이 나올 것 같지만 얻어가는게 정말 많아서 너무나도 만족스러운 4주가 되었던 것 같다. "
    
}, 

{
    "id": 8,
    "url": "https://hoex5h.github.io//%EB%84%A4%EB%B6%80%EC%BA%A0_%EC%B1%8C%EB%A6%B0%EC%A7%80-_Day_01_~_19_%EC%A0%95%EB%A6%AC_",
    "title": "[네부캠 챌린지] Day 01 ~ 19 정리 ",
    "body": "2025/01/14 - Day01 학습 정리확장성?: 나는 주로 코드를 작성할 때, 나중에 내가 이 코드를 다시 수정해야 하는 일이 생긴다면 최대한 쉽게쉽게 수정할 수 있도록 짜고 싶다는 생각을 한다. 그래서 최대한 바뀔 수 있어보이는 것들은 바뀌어도 문제가 없게끔 작성하려고 노력한다. 하지만 이것 때문에 왔던 길을 다시 되돌아가기도 하고 설계했던 부분을 다시 거슬러 올라가는 일도 빈번하다. 다른 사람들은 코드를 짤 때 어떤 것을 중요하게 여기면서 코드를 짜는지 궁금하다. static: 확장성을 높여보고자 회의실의 이름, 최대 인원수를 변경할 수 있도록 설계했다. 그러면서 static 을 써보려고 생각을 했다. 자바스크립트에서도 ES6 이후로 static 키워드를 사용할 수 있다고 한다. 실제 사용한 코드는 아니고, 예시로 설명하자면 아래 코드와 같다. textclass utility { static multiply(a, b) {  return a * b; } static plus(a, b) {  return a + b; }}console. log(utility. multiply(3, 5));// 15utility 클래스를 하나 만들고 멤버변수 혹은 멤버 메소드에 static 키워드를 붙여주고 사용했다. 클래스의 객체를 따로 만들지 않고 바로 multiply 함수를 사용해도 결과가 잘 나온다. 처음에는 static 메소드도 사용할 일이 있겠다 싶어 class 로 만들고, static 메소드와 변수를 사용하겠다는 설계를 했었는데 구현을 하다보니 static 메소드는 딱히 필요가 없어졌고 오히려 enum 을 사용해도 괜찮았겠다는 생각이 들었다. static 를 사용하기 좋은 경우는 객체들간에 전부 공유되는 변수/메소드가 필요할 때 좋다. enum: 아쉽게도 타입스크립트가 아닌 자바스크립트에는 enum이 구현되어 있지는 않다. 그저 Object 타입의 객체로 만들어주는 것인데, Object. freeze()를 통해 불변객체로 만들어서 활용하는 정도라고 한다. javascriptObject. freeze({ RED:  red , GREEN:  green , BLUE:  blue ,});함수 분리: 코드를 작성하면서 코드가 길어지거나 중첩이 깊어질 수록 가독성이 떨어진다는건 알고 있지만 함수를 여러 개로 분리해서 짜는것도 깔끔하게 작성하기는 쉽지 않다. 함수를 분리할 때 중복되는 코드 나 기능별로 나눌 수 있을 것 같은 함수를 위주로 분해를 많이 했다. commit 메세지:  • feat : 새로운 기능 추가, 기존의 기능을 요구 사항에 맞추어 수정 • fix : 기능에 대한 버그 수정• build : 빌드 관련 수정• chore : 패키지 매니저 수정, 그 외 기타 수정 ex) . gitignore• ci : CI 관련 설정 수정• docs : 문서(주석) 수정• style : 코드 스타일, 포맷팅에 대한 수정• refactor : 기능의 변화가 아닌 코드 리팩터링 ex) 변수 이름 변경• test : 테스트 코드 추가/수정• release : 버전 릴리즈• typo : 오타 수정 커밋 기준은 돌아갔을 때 버그가 없어야 한다. 내가 하는 커밋의 기준은 너무 큰 것 같다. 지금보다는 조금 더 짧은 주기로 커밋 해보자. 입력값이 정수형이라면?: 이번 문제를 해결하면서 필요한 문자열을 적절히 파싱해주는 과정을 거치고 있지만, 정수형으로 주어진다면 파싱을 하지 않아도 될 것이다. 또한 파싱을 일부는 안하고 일부는 하는 방식으로 접근했었는데 정수형으로 주어지면 이렇게 일부를 따로 고려할 필요도 없어진다. Github Pull request: Fork 원본 저장소를 자신의 계정으로 복사 Clone git clone 저장소를 로컬 환경으로 복사 Branch git checkout -b 새로운 브랜치를 만들고 해당 브랜치로 체크아웃 Commit git commit -m 변경 사항을 커밋 메세지와 함께 커밋 Push git push  변경 사항을 원격 저장소에 Push 이후 원본 저장소에 Pull Request 생성 Day02 학습 정리Virtualization &amp; Emulation: 가상화와 에뮬레이션의 차이는 무엇일까? Virtualize 와 Emulate 의 차이는, Virtualize는 선택한 시스템으로 독립적인 컨테이너를 만들어서 사용하는 것이고, Emulate는 기존의 로컬이 선택한 시스템이 동작하는 것처럼 흉내내는 것이다. Virtualize 의 경우 해당 시스템이 동작하는 그대로,선택한 시스템의 운영체제 자체를 네이티브로 돌아가기 때문에 성능이 비교적 좋다. 또한 호스트 머신과 완벽하게 분리되어 돌아가기 때문에 직접적으로 마운트 하지 않은 다른 시스템들과는 서로 영향을 끼치지 않는다. 하지만 CPU 아키텍처가 다르거나 하는 이유로 가상화를 할 수 없는 경우가 있다. Emulate 의 경우에는 다른 시스템을 모방해서 동작하는데 오버로드가 생길 수 있어 성능이 비교적 좋지 않다. 말 그대로 선택한 시스템의 하드웨어가 행동하는 것 처럼, 소프트웨어가 작동하는 것이다. 그러나 CPU 아키텍처가 다른 경우에도 사용할 수 있다는 큰 장점이 있다. 공개키 개인키?: ssh 접속은 비밀번호로도 가능하지만 보안에 취약해 보통 공개키와 개인키를 활용한다. 대칭키와 비대칭키: 암호화에는 대칭키를 이용하거나, 비대칭키를 이용할 수 있다. 대칭키는 암호화하는 키와 복호화하는 키가 같은 경우를 뜻한다. 누군가가 A라는 키로 암호화해서 나에게 건네주었다면 나도 반드시 A라는 키가 있어야한다. 하지만 이 방법은 A라는 키를 나에게 있어야 복호화 할 수 있기 때문에 결국 A라는 키를 전달해 주어야한다. 전달과정에서 보안이 문제가 될 수 있다는 단점이 존재하게 된다. 비대칭키 는 암호화하는 키와 복호화하는 키가 다른 경우이다. 공개키 와 개인키 가 서로 비대칭(서로 다른 키)을 이루어서 암호화하거나 복호화할 수 있다. 공개키는 개인키에 의해서 만들어지는데 개인키에 대응하는 공개키는 오직 1개인 반면, 공개키로는 개인키를 유추할 수 없다. 따라서 공개키는 노출되어도 되지만 개인키는 노출되어서는 안된다. 쉘 스크립트 문법: 자주 사용되는 문법들을 위주로 정리해보려고 한다. grep: &lt;grep [옵션] 패턴 [파일…]&gt; 의 형식으로 사용할 수 있지만 보통은 파이프로 많이 사용한다. 특정 패턴과 일치하는 줄을 찾아 출력하는 아주 강력한 텍스트 검색 도구이다. bash 의 if문: shellif [ 조건문 ]; thenelif [ 조건문 ]; thenelsefi의 형태이다. 셔뱅: #!/bin/bash 는 Shebang(셔뱅)이라고 불리며, 스크립트 파일의 첫 줄에 위치하는 특별한 주석이다. 이 줄은 운영체제에게 해당 스크립트를 실행할 인터프리터(interpreter)를 지정하는 역할을 한다. #!/usr/bin/env python3 처럼 입력하면 python 인터프리터를 찾아 실행하게 되는 것이다. 또한 crontab 같은 스케줄러는 셔뱅을 통해 인터프리터를 파악하게 되므로 반드시 필요하다. $# 파라미터: bash multiply. sh 3 5 와 같이 3과 5를 파라미터로 넘겨줬을 때 코드 내부에서는 $(숫자) 로 첫 번째 파라미터부터 받기 시작한다. $@ 은 모든 파라미터를 담고 있다. for: shellfor i in $@; dodoneDay 03 학습 정리컴파일러 이론: 컴파일러는 고급 프로그래밍 언어로 작성된 소스 코드를 컴퓨터가 실행할 수 있는 저수준 언어(주로 어셈블리어나 기계어)로 변환하는 프로그램이다. 이 프로그램의 단계는 Tokenizer -&gt; Lexer -&gt; Parser 로 나뉘는데, 이에 대해 한 번 알아보자. Tokenizer: 토크나이저는 소스 코드를 의미 있는 최소 단위인 토큰 으로 나누는 역할을 한다. 예를 들어, 아래의 소스 코드가 토크나이저에 의해 나누어진다고 가정해보자. javascriptlet a =  my Tokenizer ;위의 소스코드를 왼쪽부터 읽어보면, l -&gt; le -&gt; let -&gt; let(공백) -&gt; let(공백)a … 의 방식일 것이다. l, le 는 의미가 있지 않은 반면, 자바스크립트에서 변수를 선언할 때 사용하는 let는 의미를 가지게 된다. 따라서 이 경우에는 공백을 기준으로 코드를 쪼갠다면 그 결과물들이 전부 의미 있는 최소 단위인 토큰이 될 것처럼 보인다. 하지만 여기서 무작정 공백으로 나눈다면 “my 와 Tokenizer” 로 분리되게 되는데, 이는 프로그래머가 의도한 것과는 달라보인다. 따라서 이렇게 구분하기 위한 것에는 공백, 따옴표 등등의 기준을 정규식 으로 미리 정의하고 분리한다. 이 과정에서 불필요한 공백(trim) 이나 주석을 제거하기도 한다. [let, a, =, ‘my Tokenizer’] Lexer (어휘 분석): 이렇게 토크나이저에 의해 분리된 토큰은 lexer 에 입력으로 들어오게 된다. lexer 는 토큰들의 의미를 파악해서 토큰의 종류와 값을 나타내는 (ex. 키워드, 식별자, 연산자 및 리터럴) 같은 개별 토큰 객체로 반환한다. 토큰의 종류(type) 결정 (예: 키워드, 식별자, 연산자, 리터럴)토큰의 값(value) 추출 (예: 식별자의 이름, 숫자 리터럴의 값) [let, a, =, ‘my Tokenizer’] -&gt; [keyword(let), identifier(a), operator(equal), literal(‘my Tokenizer’)] Parser: lexer 에 의해 만들어진 토큰 객체들은 parser 로 넘어온다. parser 는 이 토큰 객체들로 문법 규칙에 따라 구문 분석을 수행한다. 구문 분석을 수행하며 파스 트리(Parse Tree) 또는 추상 구문 트리(AST, Abstract Syntax Tree)로 만드는데, ast 그려주는 사이트! 여기에 좋은 사이트가 있었으므로 참고해서 그려보자. image. png let a = 'My Tokenizer' 를 AST 로 표현한 것이 위의 트리인데, let 이라는 키워드는 ‘변수 선언’(Variable Declaration)이므로 루트노드가 ‘변수 선언’ 인 것을 알 수 있다. 이후에는 (할당)표현식으로 나누어지는데, a는 Identifier, =는 Operator, “My Tokenizer” 는 StringLiteral 토큰 객체로 각각 나뉘어진다. 정규 표현식: 위의 컴파일러의 lexer는 주로 정규 표현식을 통해서 토큰의 종류와 값을 파악한다. 또한 정규식은 여러 분야에서 다양하게 쓰인다. 예전에는 정규 표현식 대신 조건문을 사용했었는데, 정규 표현식의 사용이 훨씬 코드를 단축시켜주는 것 같다. 다만 경우에 따라서는 if 문이 가독성 자체는 좋을 수도 있겠다라는 생각도 들어 적절히 잘 사용해야겠다. 정규식에 사용되는 규칙을 전부 다 외우는건 조금 힘들어 보인다. 그 중에서 기본적인 규칙에 대해서만 외우고, 자잘한것은 찾아보면서 진행하는 것이 가장 좋아 보인다. 정규식에 유용한 사이트  정규식 테스트 정규식 생성text문자 클래스 (Character Classes): [abc]처럼 대괄호 안에 문자들을 나열하여 해당 문자들 중 하나와 매치되는 패턴을 만든다수량자 (Quantifiers): *, +, ?, {n}, {n,m} 등을 사용하여 앞의 문자나 문자 클래스가 반복되는 횟수를 지정앵커 (Anchors): ^, $, \\b 등을 사용하여 문자열의 시작, 끝, 단어 경계 등 특정 위치를 지정그룹화 (Grouping): () 괄호를 사용하여 여러 문자를 하나의 그룹으로 묶고, 수량자나 다른 연산을 적용이스케이프 문자 (Escape Characters): \\, \\d, \\w, \\s 등 특수 문자를 사용하여 특정 문자 클래스를 나타낸다주로 공백을 표현할 때는 \\s 를 사용하는데, 만약 공백이 0개 이상 을 표현하고 싶으면 * 을(\s*), 1개 이상 을 표현할 때는 + (\s+)를 사용한다. {n}: 앞의 문자나 문자 클래스가 정확히 n번 반복되어야 함을 나타냅니다. 예시: \d{3} (정확히 3개의 숫자와 매칭){n,}: 앞의 문자나 문자 클래스가 n번 이상 반복되어야 함을 나타냅니다. 예시: \d{2,} (2개 이상의 숫자와 매칭){n,m}: 앞의 문자나 문자 클래스가 n번 이상 m번 이하 반복되어야 함을 나타냅니다. 예시: \d{2,4} (2개 이상 4개 이하의 숫자와 매칭) Day 04 학습 정리Day04 프로세스와 메모리학교에서 ‘컴퓨터 구조’ 수업 시간에 들었던 프로세스, 메모리 관리를 막상 사용해보려고 하니 쉽지가 않았다. 그래서 전반적인 개념에 대해서 다시 한 번 복습해보는게 좋을 것 같다. 프로세스와 스레드: 프로세스와 스레드의 역할은 상당히 유사해보여서 자주 헷갈리곤 한다.       구분   프로세스 (Process)   스레드 (Thread)         대표 키워드   Isolation (독립적)   Concurrency (동시성)       정의   실행 중인 프로그램의 인스턴스   프로세스 내에서 실행되는 작업의 시퀀스       메모리   독립적인 메모리 공간   같은 프로세스 내에서 메모리 공간을 공유       자원 공유   자원을 독립적으로 할당받음   같은 프로세스 내에서 자원(메모리, 파일 등)을 공유       생성 방법   운영 체제의 프로세스 생성 시스템콜(fork 등)   프로세스 내에서 생성       실행 단위   독립적으로 실행   다른 스레드와 병렬로 실행 가능       오류 영향   한 프로세스의 오류가 다른 프로세스에 영향을 미치지 않음   한 스레드의 오류가 같은 프로세스의 다른 스레드에 영향을 줄 수 있음       실행 속도   상대적으로 느림 → 프로세스 간의 통신(IPC) 비용이 높음   상대적으로 빠름 → 스레드 간의 통신 비용이 낮음       스케줄링   독립적인 스케줄링   동일한 프로세스 내에서 스케줄링됨   프로세스에서 메모리 관리: image. png 프로세스가 OS에 의해 할당받는 메모리는 Code(Text)영역, Data영역, Heap영역, Stack영역으로 나뉘게 된다. Code(Text)영역: Code 영역은 실제 프로그램의 코드(명령어)가 저장되는 영역 이다. CPU는 이 영역에서 명령어를 가져가서 실행하게 된다.  생성 → 코드 영역은 프로그램이 시작할 때 할당된다. 소멸 → 종료될 때 사라진다. Data 영역: Data 영역에는 프로그램의 전역 변수와 정적(static) 변수가 저장되는 영역이다.  생성 → 데이터 영역은 프로그램이 시작할 때 할당된다. 소멸 → 종료될 때 사라진다. Heap 영역: Heap 영역은 동적 메모리 할당에 사용되는 영역이다. 대표적으로 malloc 이나, new 키워드 등이 있다. Stack 영역과 반대로, 낮은 메모리 주소부터 할당하기 시작한다.  생성 → 힙 영역은 동적 메모리 할당 요청(malloc, new…)이 있을 때 할당된다. 소멸 → 메모리를 해제(free, delete . . . ) 할 때 사라진다. Stack 영역: Stack 영역에는 지역 변수와, 매개 변수가 저장되는 영역이다. 그 외에도 다양한 정보들이 스택 영역에 쌓이게 되는데, 이렇게 하나의 함수에 대한 정보들이 쌓이는 구간을 스택 프레임이라고 부른다. Stack Frame: javascriptfunction sum(a, b) { const d = a + b; return d;}function main() { const c = sum(1, 2); return c;}간단한 예시를 들어서 스택 프레임에 대해 알아보자. 참고 자료 image. png 우선 main 함수가 호출되었을 때 stack 영역의 상황이다. 맨 아래에는 RET(Return Address) 가 저장이 되는데, 함수가 끝나고나면 돌아갈 위치를 저장하고 있다. main 함수의 경우는 프로그램의 시작점으로 보이지만 실제로는 Start 라는, main 함수를 호출하는 부분이 존재한다고 한다. 그 위에는 RBP 가 쌓이는데, 이전 함수의 RBP를 저장하고나서 현재 스택 프레임의 시작을 가리키도록 RBP의 값을 바꾼다. 현재 스택 프레임의 시작을 가리키도록 바꿔놨기 때문에 RBP를 기준으로 변수 c를 탐색하거나 하는 등을 할 수 있다. 그 다음에는 지역 변수 c가 쌓인다. 이제 sum 함수의 실행 시점까지 넘어가보자 image. png 매개 변수 a,b 가 먼저 쌓이고 RET, RBP, 지역 변수 d가 쌓인 것을 볼 수 있다.  함수를 호출하는 쪽에서 매개 변수를 먼저 스택에 푸시하게 되고, 실제로 호출되면서 RET 가 스택에 푸시된다.  이전 함수의 RBP를 저장하고, RBP는 현재의 스택 프레임 시작 지점을 가리키도록 바꾼다.  지역 변수가 쌓인다. Day 06 학습 정리객체지향 프로그래밍객체지향 프로그래밍은 프로그램을 명령어 또는 함수의 목록으로 보는 전통적인 명령형 프로그래밍의 절차지향적 관점에서 벗어나 여러 개의 독립적 단위, 즉 객체의 집합으로 프로그램을 표현하려는 프로그래밍 패러다임을 말한다. 객체는 다양한 속성들을 가지고, 속성에 대한 값만 다르게 해서 객체들을 생성하고 다른 객체들과 구별할 수 있다. 이처럼 다양한 속성 중에서 프로그램에 필요한 속성만 추려내는 것을 추상화 라고 하며, 우리가 알고 있는 추상클래스도 이렇게 추상화해서 객체들의 공통적인 특징들을 가지고 있는 클래스이다. 클래스: 자바스크립트는 프로토타입 기반의 객체지향 언어이고, 클래스 또한 내부적으로는 프로토타입으로 돌아가기 때문에 문법적 설탕 이라는 표현을 쓰고는 한다. 결국 문법이 조금 다르게 보이는, 프로토타입 패턴을 클래스 패턴처럼 사용할 수 있게 해주는 것에 불과하다는 이야기이다. 물론 클래스는 생성자 함수보다 엄격하며 생성자 함수에서는 제공하지 않는 기능도 제공을 한다고는 한다.  문법적 설탕이란 기존의 코드를 조금 더 깔끔하고 가독성 높게 쓰는 방법을 얘기한다. 생성자 함수: 생성자 함수란 new 연산자와 함께 호출하여 객체(인스턴스)를 생성하는 함수를 말한다. javascriptfunction Character(name) { this. name = name; this. setName = function (newName) {  this. name = newName; };}const firstCharacter = new Character( JjangGu );function 키워드로 만들어진 characters 함수가, new 키워드와 함께 쓰이면 객체를 생성하고, new 키워드 없이 사용되면 그저 함수처럼 작동한다. 자바스크립트의 모든 함수는 객체 이고 call(호출) 할 수 있다. 함수 객체 내부에는 [[Call]], [[Construct]] 와 같은 내부 메소드가 존재한다. new 와 함께 썼을 경우 [[Construct]]메소드가 호출되는 것이다. 그렇다면 모든 함수에 [[Construct]] 메소드가 있는 것일까? 자바스크립트 엔진은 함수를 직접 평가하면서 함수 선언문(function a() {}) , 함수 표현식 (const b = function(){}) , 쿨래스에 한해서 constructor 로 인정하고 나머지의 경우에는 생성자로 인정하지 않는다. 따라서 [[Consturct]] 함수는 constructor 로 인정 받은 함수 객체만이 가지고 있는 것이다. 프로토타입: 상속: 객체지향 프로그래밍에서는 상속 으로 중복되는 코드를 줄일 수 있다. 상속은 어떤 객체의 property 혹은 method 를 다른 객체가 사용할 수 있는 것을 말한다. 자바스크립트에서는 이 상속을 프로토타입 으로 구현한다. javascriptfunction Person(name) { this. name = name;}Person. prototype. type =  mammalia ;Person. prototype. breath = function () { return  휴 살앗당 ;};인간이라는 객체는 각각 다른 이름을 지닐 수 있지만, 모든 인간 객체는 포유류이며, 숨을 쉬어야한다. 혹시 이 규칙에 위배되는 사람이 있으면 제보를 해주면 감사할 것 같다. 그래서 포유류이고 숨을 쉬는 것은 모든 인간 객체가 중복되어 가지고 있을 것이고 Person 객체의 prototype 에 정의하는 것이다. 캡슐화: 캡슐화(capsulation)은 객체의 상태를 나타내는 property 나 method 를 하나로 묶는 것을 말한다. 마치 약 안에 성분이 어떻게 작용하는지는 모르지만, 약의 이름과 효능정도만 알고 먹는다는 느낌이다. 캡슐화를 통해 정보를 은닉하거나, 객체의 상태가 변경되는 것을 막을 수 있다. 다형성: 다형성(polymorphism)은 하나의 객체가 여러 가지 형태를 가질 수 있는 능력을 의미한다. 말이 너무 어려운 것 같은데, 인간이라는 객체는 모두 숨을 쉬어야하지만 숨을 얼마나 참을 수 있는지, 심장과 폐가 얼마나 좋은지에 따라서도 조금씩 다른 형태의 숨쉬기 메소드를 가질 것이다. 이렇게 주로 오버로딩 과 오버라이딩 등으로 하나의 객체가 여러 가지 형태를 가질 수 있도록 구현할 수 있다. 출처: 모던 자바스크립트 Deep Dive Day07 학습 정리자바스크립트 테스트 도구, Jest: 설치: Jest 는 간단하게 npm install --save-dev jest 로 설치해서 사용할 수 있다. 설정이 따로 필요 없다고 하는 것 같았지만 테스트를 하기 위한 js 파일에서 ES6 모듈 import 가 되지 않아서 추가 설정을 거쳤다. babel 이라는 것을 새로 설치해서 config 를 수정해야 #한다고 하는데, babel 이 뭔지는 조금 더 찾아봐야겠다. Babel 이 뭘까?: Babel 은 트랜스파일러(transpiler) 로, 코드를 변환 하는데 목적이 있다. 그렇다면 갑자기 코드를 왜 변환해야하는걸까? 예를 들어 ES6에서 지원하는 화살표 함수는 구형 브라우저(IE 같은. . ) 에서는 지원하지 않는 문법일 수도 있다. 따라서 구형 브라우저가 지원하는 문법으로 변환해야할 때가 있고, 이를 위해서 Babel 을 사용하는 것이다. Jest 는 CommonJS 를 기반으로 동작하기 때문에 ES6 문법인 import, export module 을 이해하지 못한다. 따라서 import, export 부분을 jest 가 이해할 수 있도록 트랜스파일링하는 것이다. shellnpm install -D jest @types/jest @babel/core @babel/preset-envbable 을 사용하려면 @babel/preset-env 를 설치해야 한다고 한다. 이는 함께 사용되어야 하는 Babel 플러그인을 모아 둔 것으로, Babel 프리셋이라고 부른다. Babel이 제공하는 공식 프리셋은 4가지이다.  @babel/preset-env @babel/preset-flow @babel/preset-react @babel/preset-typescript이 중에서, preset-env 는 필요한 플러그인들을 프로젝트 지원 환경에 맞추어 동적으로 결정해준다고 한다. 아래처럼 babel config 를 수정하면, @babel/preset-env 를 사용하겠다는 의미이다. json/* babel. config. json */{  presets : [ @babel/preset-env ]}json/* jest. config. json */{  verbose : true,  collectCoverage : true}#테스트: 테스트를 하는 방법은 상당히 간단했는데, test 를 #사용해서 함수를 만든다. test: javascripttest( two plus two is four , () =&gt; { expect(2 + 2). toBe(4);});결과값을 비교하는 Matchers가 여러 개 있는데, toBe, toEqual, toMatch, toThrow 등이 있다고 한다.  toBe: 참조값이 같은지 확인한다. (내용이 같아도 서로 다른 것을 가리키면 false) toEqual: 객체의 값이 같은지 확인한다.  toMatch: 정규식을 활용해서 문자열 match#- toThrow: 함수가 에러를 발생시키는지 확인한다. 비동기 테스트: 비동기 테스트 jest 공식 문서 비동기 코드는 어떻게 테스트 할 수 있을까 고민이 있었는데 #친절하게도 공식 문서에 잘 정리가 되어 있었다. assertion: expect. assertions(N)) 은 테스트 중에 정확히 N번 expect 호출을 실행할 것을 요구한다. 예시를 들어서 한 번 봐보자. 우선 doAsync 함수는 callback 함수 2개를 받아서, 비동기적으로 각각의 콜백 함수를 1번씩 실행하는 함수이다. 각각의 callback1, 2 에서는 expect 가 한 번씩 실행된다. 의도대로 잘 흘러간다면 doAsync 를 실행했을 때 expect 가 딱 2번 실행될 것이다. assertion 은 콜백함수가 정상적으로 실행되는지를 확인하기 좋다. javascripttest( doAsync calls both callbacks , () =&gt; { expect. assertions(2); function callback1(data) {  expect(data). toBeTruthy(); } function callback2(data) {  expect(data). toBeTruthy(); } doAsync(callback1, callback2);});#Promise:  Return a promise from your test, and Jest will wait for that promise to resolve. If the promise is rejected, the test will fail. For example, let’s say that fetchData returns a promise that is supposed to resolve to the string ‘peanut butter’. We could test it with: Promise 의 경우에는 resolve 되었을 때, test 를 진행하면 된다. reject 의 경우에는 test 가 fail 된다. javascripttest( the data is peanut butter , () =&gt; { return fetchData(). then((data) =&gt; {  expect(data). toBe( peanut butter ); });});#Async/Await: async await 을 사용해서 테스트를 하고 싶으면, 콜백 함수 앞에 async 키워드를 사용하면 된다. javascripttest( the data is peanut butter , async () =&gt; { const data = await fetchData(); expect(data). toBe( peanut butter );});test( the fetch fails with an error , async () =&gt; { expect. assertions(1); try {  await fetchData(); } catch (error) {  expect(error). toMatch( error ); }});async 와 await 을 . resolves 와 . rejects 랑 혼합해서 사용할 수도 있다고 한다. javascripttest( the data is peanut butter , async () =&gt; { await expect(fetchData()). resolves. toBe( peanut butter );});test( the fetch fails with an error , async () =&gt; { await expect(fetchData()). rejects. toMatch( error );});이렇게 작성하는 것과 위에서 작성한 것과 동일하게 작동하는, 문법적 설탕이라고 한다. 만약 promise 가 reject 되는 것으로 예상한다면, . catch 과 assertion 를 사용하라고 한다. javascripttest( the fetch fails with an error , () =&gt; { expect. assertions(1); return fetchData(). catch((error) =&gt; expect(error). toMatch( error ));});#Callback: Promise 를 사용하지 않는 경우, 콜백을 사용해서 테스트 할 수 있다. 예를 들어, data 에 어떤 작업을 하고나서 콜백함수로 data를 넘겨주는 fetchData 라는 함수가 있다고 하자. 만약 아래와 같이 테스트 코드를 작성하면 어떻게 될까? javascript// Don't do this!test( the data is peanut butter , () =&gt; { function callback(error, data) {  if (error) {   throw error;  }  expect(data). toBe( peanut butter ); } fetchData(callback);});아쉽게도 이 테스트 코드는 의도대로 흘러가지는 않는다. Jest 에서는 코드가 끝나는 순간 테스트를 끝내버린다. 다시 말해서 콜백 함수가 실행되기까지 기다려주는게 아니라, fetchData(callback) 의 실행이 끝나고 테스트가 끝날 수 있다는 얘기다. 그러면 어떻게 코드를 작성해야 콜백 함수를 기다릴 수 있을까? test에 done 인자를 넣어서 콜백 함수가 끝날때까지 기다릴 수 있다. javascripttest( the data is peanut butter , (done) =&gt; { function callback(error, data) {  if (error) {   done(error);   return;  }  try {   expect(data). toBe( peanut butter );   done();  } catch (error) {   done(error);  } } fetchData(callback);});만약 모종의 이유로 done() 이 호출되지 않는다면 테스트는 타임아웃 에러로 실패해버린다. 또한 expect 가 fail 하게 되면, error를 던질 뿐이지 done() 이 호출되지는 않는다. 따라서 왜 실패했는지 로그를 보고 싶다면 try-catch 로 감싸서, catch 구문에도 done(error) 같은 것이 있어야 한다. 만약 테스트 함수에 done() 이 전달되고 동시에 Promise 를 반환하는 경우 오류가 발생한다. 이는 테스트에서 메모리 누수를 방지하기 위한 조치라고 한다. done() 과 Promise 를 동시에 쓰는 이상한 짓을 하지 #말자. . resolves / . rejects: . resolves 를 사용하면, promise 가 resolve 될 때까지 대기한다. reject 될 경우에는 테스트에 fail 한다. javascripttest( the data is peanut butter , () =&gt; { return expect(fetchData()). resolves. toBe( peanut butter );});주의해야할 점은, 테스트 함수 내에서 Promise 를 반환하는 경우 return 을 사용하지 않으면 Jest 는 테스트가 끝났다고 판단해버릴 수 있다. 이 경우에는 Promise 가 아직 해결되지 않았음에도 테스트가 끝나기 때문에 의도한 결과가 나오지 않을 수 있다. . reject 는 resolves 와 반대라고 이해하면 된다. fulfilled 될 경우에는 테스트에 fail 한다. Day 08 학습 정리함수형 프로그래밍:  함수형 프로그래밍을 왜 사용하는 것일까?기본적으로 절차지향적, 객체지향적 프로그래밍은 상태 라는 것을 갖는다. 우리가 자주 사용하는 변수들이 이러한 상태의 대표적인 예시이다. javascriptfunction sum(a, b){ return a + b;}class GameCharacter { . . .  #HP = 500; getDamaged(damage){  // 현재 HP 에서 damage 만큼 을 뺀, HP를 반환한다.  }}sum(a, b) 함수의 경우 어느 시점에 (a,b) 를 넣어도 (a, b)가 이전에 넣었던 (a, b) 와 동일하다면 결과 값이 다를 리가 없다. 결과값인 a + b 에 영향을 미칠 상태 같은 것이 없다. 반면 GameCharacter 는 HP 라는 상태를 가지고 있다. HP 상태에 따라 getDamaged(damage) 함수는 다른 결과 값(남은 HP)을 반환할 것이다.  변수를 개발자가 직접 다루게 되는 것을 최소화하는게 함수형 프로그래밍 함수형 프로그래밍은 이런 상태를 최소화 하는데 목적이 있다. 변수(상태)가 많다는 것은 버그나 문제가 일어날 가능성이 높아질 수 밖에 없다는걸 의미한다. 함수가 일급 객체(First-Class Objects)인가?일급 객체는 숫자 등의 일반직언 다른 데이터처럼 다뤄질 수 있다.  변수에 할당되거나, 함수에 인자로 넘겨지거나, 반환 값으로 나오거나, 익명으로 만들어지는 등이 가능하다. 함수형 프로그래밍의 장점과 단점은 무엇일까? 함수형 프로그래밍의 특징 순수성: 함수에서 외부의 상태값을 참조하거나, 외부의 상태를 바꾸는 것은 순수성이 아니다. 동일한 인자를 넣었을 때 항상 동일한 결과를 반환하고, 언제 선언이 되었는지 외부에 전혀 영향이 받지 않도록 해야함 불변성: 함수의 인자로 넘겨진 데이터를 변경하는것은 함수형이 아니고, 새로운 오브젝트를 만들어서 결과값으로 전달해야한다.       side effect 가 없음-&gt; Object freeze    Expression: 조건문같은 여러가지 문장을 사용하는건 함수형 프로그래밍이 아니다. for loop 를 이용해서 배열을 하나씩 돌면 안되고, 고차함수 활용-&gt; 이건 이유가 뭐지? 일급 객체: 함수를 파라미터로 넘기거나, 함수에서 또다른 함수를 리턴하는 고차함수 속성을 가지고 있어야 한다. 팩토리 패턴을 쓰면 클래스를 만들 때궁금증 순수 함수 및 객체 생성 과정을 캡슐화 할 수 있다. 연결리스트를 쓰면서, 연결리스트의 노드에는 변경을 가하지 않고, 연결된 선만 수정해서 반환?  불변성을 유지하기 위해 인스턴스를 복사해서, 인스턴스에 변경을 가하고 반환하면 시간이 오래 걸리지 않는건가??Day 09 학습 정리Promise: 약속이라는 뜻의 키워드를 가지게 된 이유가 뭘까? 당장 결과를 알기는 힘들지만, 미래에 결과를 주기로 약속하는 개념으로 이해하면 쉽다. 보통 비동기 작업을 처리하는데 사용하는 객체 이다. javascriptconst promise = new Promise(() =&gt; { //callback});promise . then((response) =&gt; {  console. log(response);  return response. data; }) . catch((reason) =&gt; {  console. log(reason); }) . then((data) =&gt; {  console. log(data); });콜백 함수로 넘어온 함수를, 비동기로 작업하는동안 promise 객체는 Pending 상태이고, 비동기 작업(콜백 함수)가 완료되면 then 메소드로, 실패하면 catch 메소드로 결과를 받을 수 있다. promise 의 then 함수는 작업이 완료되고 넘어온 결과값을 첫 인자로 전달받게 되는데, 무엇인가를 반환(return) 하려고 하면 또 다시 Promise 로 감싸서(Promise 객체로 감싸서) 내보낸다. 그런데 이 작업, 어디서 많이 본 작업인 것 같다. Promise 의 then 은 flatmap 의 역할을 한다고 볼 수 있어, monadic 하다고 할 수 있고 이러한 이유 때문에 콜백지옥(콜백이 중첩으로 쌓여가는)을 피할 수 있다. day 11-12 학습 정리Array. from: 우선 Array. from 은 생성자가 아님을 유의해야한다. static 메소드이므로 new 키워드와 함께 사용하지 않는다. textArray. from(arrayLike)Array. from(arrayLike, mapFn)Array. from(arrayLike, mapFn, thisArg) Array. from 메소드는 유사 배열 객체 혹은 이터러블 객체를 인수로 전달받아 배열로 변환하여 반환한다. javascriptArray. from( HELLOWORLD ); // ['H', 'E', 'L', 'L', 'O', 'W', 'O', 'R', 'L', 'D'];//문자열은 이터러블 객체이다. Array. from(arrayLike, mapFn) 은 이터러블 객체(arrayLike)를 순회하며 모든 요소에 대해 호출하는 map 함수를 2번째 인자로 넣을 수 있다. 이 mapFn 의 반환값이 대신 배열에 추가가 된다. javascript// arrayLike -&gt; {length : 3}, Length만 있으므로 undefined 3개가 요소를 채우게 된다. [undefined, undefined, undefined]Array. from({ length: 3 });Array. from({ length: 3 }, (element, index) =&gt; { // element : 배열에서 처리 중인 요소 // index : 현재 인덱스 return new Array();});Promise 와 클로저: Promise 함수를 작성하면서 변수를 하나 선언해서 아래와 같이 사용을 했다. textconst saveNumber = 10;  new Promise((resolve, reject) =&gt; {   return resolve(50)  }). then((result) =&gt;   console. log(`${result - saveNumber}`));Promise 의 결과 값에서 saveNumber 만큼을 뺀, 결과 값을 출력하는 promise 이다. 그런데 여기서 Promise 가 제대로 작동하려면 saveNumber 가 존재해야하는(상태가 유지되어야하는) 상황이다. 결국 클로저와 유사하게 코드를 짠 것이였는데 Promise 는 비동기로 작동하기 때문에 saveNumber 의 라이프 사이클이 기존의 클로저와 유사한지 고민이 되었다. 결론은 Promise 에서도 클로저가 잘 동작한다! 구조 분해 할당과 … rest 문법: javascriptconst [a, b] = [s. split( , )[0], s. split( , ). slice(1)];위와 같은 코드를 작성하고 있었다. split 한 결과의 첫 번째 원소만 a에, 나머지는 b에 할당하고 싶었다. 여기서 훨씬 깔끔하게 작성하는 방법이 있는데, rest 문법을 사용하는 것이다. javascriptconst [a, . . . b] = s. split( , );훨씬 간단하다. . ! 스프레드 문법과 rest 문법: 스프레드 문법(…) 은 하나로 뭉쳐 있는 여러 값들의 집합을 펼쳐서 개별적인 값들의 목록으로 만든다. 스프레드 문법은 이터러블에 한해서 사용 가능하고 스프레드 문법의 결과는 값이 아니다. Rest 문법은 스프레드 문법과 반대라고 생각하면 된다. rest 문법은 여러 목록들을 배열로 전달받는 방법이다. 자바스크립트에서 interface, duck typing: 자바스크립트는 공식적으로 interface 를 지원하지는 않는다. 타입스크립트로 넘어가게 되면 지원하는데, 자바스크립트에서 지원되지 않는 이유가 궁금해서 찾아봤더니  Interfaces don’t exist in JavaScript because it is a dynamic language, where types are changed so often that the developer may not have even realised. 라고 한다. 자바스크립트는 동적 언어라서 개발자가 모르는 사이에 타입이 바뀌는 경우가 종종 있으니 인터페이스가 없다는 것이다. 그래서 비슷한 방법이라도 있을까? 찾아보면서 동적 타입 언어에서 활용할 수 있을 법한, duck typing 이라는 개념이 있다는 걸 알게 되었다. duck typing : 날 수 있고, 두 발로 걷고, 소리를 낸다 (-&gt; 메소드들) 라는 메소드를 가지고 있는 무언가를 duck 이라고 가정한다면, 닭도 duck 타입이라고 할 수 있다. 라는 의미이다. 예를 들어, javascriptconst DuckTyping = { WalkingOnTwoFeet : (duckType) =&gt; duckType. WalkingOnTwoFeet(), MakeASound : (duckType) =&gt; duckType. MakeASound(), Flying : (duckType) =&gt; duckType. Flying(),}class RealDuck { WalkingOnTwoFeet(){  console. log( 오리가 두 발로 걷는다.  ); } MakeASound(){  console. log( 꽥꽥 ); } Flying(){  console. log( 오리가 난다 ); }}class Chicken{ WalkingOnTwoFeet(){  console. log( 닭이 두 발로 걷는다.  ); } MakeASound(){  console. log( 꼬끼오 ); } Flying(){  console. log( 닭이 난다.  ); } IamAChicken(){  console. log( 나는 닭입니다.  ) }}const realDuck = new RealDuck();const chicken = new Chicken();DuckTyping. MakeASound(realDuck);DuckTyping. MakeASound(chicken);위의 코드는 닭과 오리를 클래스로 만든 코드이다. 다만 실제로 닭과 오리가 같지는 않으나 하는 행동(메소드)는 비슷하게 보이기도 한다. 이제 DuckTyping 객체에 있는 함수들에 realDuck 객체와 chicken 객체를 넘겨줘서 실행해보면, 둘의 행동에 작은 차이가 있긴 하지만(꽥꽥/꼬끼오 같은) 실제로 두 행동(MakeASound)이 실행은 된다. 닭이 오리와 같은 행동을 수행할 수 있으므로, 닭은 오리 타입이라고 인정하겠다는 것이다. Day 13 학습 정리EventEmitter 와 Promise 는 각각 어떨 때 쓰는 걸까?: 이번에 fs 의 Stream 을 사용해서 파일을 읽어오고 있었는데, 비동기 함수로 동작한다는 걸 직접 코드를 굴려보며 깨달았다. 찾아보니, Stream 은 전부 EventEmitter 의 인스턴스였다. 그렇다보니 on 같은 메소드로 이벤트를 등록하고 관리해야하는 것이였다. 여기서 살짝 이해가 되지 않는 부분이 있었는데, Promise는 어떤 비동기 작업의 성공/실패에 따른 핸들러를, EventEmitter 는 다양한 이벤트에 따른 다양한 핸들러를 사용하는 차이점이 있다고 이해했는데 파일을 읽을 때, 읽기 성공/실패 말고 다른 이벤트들이 많이 있다는건가? 라는 생각을 하게 되었다. 그래서 찾아보니 데이터를 읽을 때 data 이벤트가, 전부 다 읽었을 때 end 이벤트가 발생하는 등, 다양한 이벤트들이 존재하는 것 같았다. Stream and Event All objects that emit events are instances of the EventEmitter class. 라는 구문으로 보아 그냥 이벤트와 관련 있으면 무조건 공식문서를 보고 EventEmitter 의 인스턴스인지 확인해보는 것이 좋겠다. 다만 공식문서를 살펴보니 Promise로 구성된 Stream 도 있는걸 알 수 있었다.  Streams Promises API## Added in: v15. 0. 0The stream/promises API provides an alternative set of asynchronous utility functions for streams that return Promise objects rather than using callbacks. The API is accessible via require('node:stream/promises') or require('node:stream'). promises. EventEmitter 를 조금 더 파헤쳐보자: EventEmitter 가 이벤트를 emit 할 땐, 등록된 순서에 맞게 동기적으로 호출한다. :  Synchronously calls each of the listeners registered for the event named eventName, in the order they were registered, passing the supplied arguments to each. Event 를 리스너 배열의 맨 앞에 추가하고 싶다면: Event 는 원래 리스너 배열의 맨 마지막에 추가가 되고, 위에서 말한 것처럼 동기적으로 호출하는데 만약 이벤트를 리스너 배열의 맨 앞에 추가하고 싶다면 prependListener() 메소드를 사용하자. EventEmitter 는 중복 검사를 하지 않는다. : 만약 똑같은 이벤트에 해당하는 똑같은 핸들러함수를 여러번 생성한다면, 이미 존재하는 리스너(핸들러) 입니다 라고 오류를 뱉는게 아니라 그냥 여러 번의 리스너가 추가 된다. 그러다가 리스너를 지우고 싶어서 removeListener(eventName, listener) 를 활용해서 지우려고 시도한다면, 여러 번 추가한 만큼 여러 번 삭제해야한다. removeListener 는 오직 1개의 리스너 인스턴스를 지운다. 마찬가지로 이벤트가 emit 되면 여러 번 등록했던 리스너는 1번만 호출되는게 아니라 등록한만큼 호출된다. EventEmitter 에서 이벤트를 emit 하고 바로 리스너를 지워도, 리스너(핸들러)는 실행되고나서 지워진다. : javascriptconst EventEmitter = require( node:events );class MyEmitter extends EventEmitter {}const myEmitter = new MyEmitter();const callbackA = () =&gt; { console. log( A ); myEmitter. removeListener( event , callbackB);};const callbackB = () =&gt; { console. log( B );};myEmitter. on( event , callbackA);myEmitter. on( event , callbackB);// callbackA removes listener callbackB but it will still be called. // Internal listener array at time of emit [callbackA, callbackB]myEmitter. emit( event );// Prints://  A//  B// callbackB is now removed. // Internal listener array [callbackA]myEmitter. emit( event );// Prints://  A위의 코드처럼, 콜백A가 실행되면 콜백B는 eventEmitter 에서 삭제된다. 배열의 초반부를 하나씩 순회를 돌고 있는데 누군가 배열의 맨 마지막 원소를 지워버리면 배열의 마지막 원소는 당연히 순회되지 않는 것과 달리 EventEmitter 는 리스너 배열을 복사해서 emit 하기 때문에 콜백B가 호출되기전에 eventEmitter 의 배열에서 삭제되어도, 복사본으로 순회하기 때문에 영향을 미치지 않는다. Promise 가 reject 될 때, eventEmitter 가 이벤트를 발생할 수 있게 만들기: javascriptconst EventEmitter = require( events );class MyEmitter extends EventEmitter { constructor(options) {  super({ captureRejections: true }); }}const emitter = new MyEmitter();// 이벤트 리스너로 프로미스를 반환emitter. on( event , async () =&gt; { throw new Error( Oops! );});emitter. on( error , (err) =&gt; { console. log( Captured: , err. message);});emitter. emit( event );사실 EventEmitter 와 Promise 는 하는 일이 어느정도 겹치지만 완전히 다른 영역에 존재한다고 생각했다. 하지만 공식문서를 읽어보니, Promise 가 error 를 만났을 때, 핸들링 되지 않는 에러라면 문제가 발생할 수 있어, Promise 의 reject 를 마치 EventEmitter 에 on 할 수 있는 느낌이다. 위의 코드처럼 captureRejections 를 true 로 설정하면, Promise 의 Reject 를 추적한다. Git 내부 동작 방식: Git pull? Git fetch?: 그동안 remote repo 에서 코드를 가져올 때, 별 생각 없이 git pull 을 먼저 실행했고 rebase 와 merge 가 다르다는 것만 알고있지 어떻게 다른지는 알지 못했다. 그래서 이번 기회에 한 번 정리를 해보려고 한다. git pull = git fetch + git merge: 내가 자주 사용하던 git pull은 git fetch 후에 git merge 하는 과정을 하나의 명령어로 압축시킨 것이다. 따라서 git pull 을 입력하면 git fetch 와 git merge 두 가지의 명령어를 입력한 것과 동일한 결과를 낳는다. fetch 는 변경점만을 가지고 오는 명령어이다. git merge? git rebase?: A라는 브랜치의 현재까지의 커밋을 B라는 브랜치에 적용시키려면 당연하게도 커밋을 합쳐서 코드를 합치는 과정을 거쳐야 한다. 이 방법에는 git merge 와 git rebase 두 가지가 있다. git merge: image. png 위의 그림을 예시로 들어보자. merge 는 3-way-merge 라는 방식으로 진행되는데, 왜 3-way 냐면, COMMIT_HASH_3 과 COMMIT_HASH_5 만 비교하는게 아니라, 두 커밋의 공통 부모 노드인 COMMIT_HASH_1도 같이 비교하기 때문이다. COMMIT_HASH_3과 5 중에서 어떤 커밋을 남겨야할지 결정하려면 두 커밋의 공통 부모 노드인 COMMIT_HASH_1 이 기준이 되어야하기 때문이다. 만약 기준점이 없다면 두 커밋 중에서 무엇이 수정된 파일이고 무엇이 수정되지 않은 파일인지 알 수 있을까? 다른 커밋을 다 제외하고 COMMIT_HASH_2의 abc. txt 파일만을 열어서 내용을 읽어봤을 때 이 파일이 이전 커밋으로부터 수정되었는지 안되었는지 알 수 없을 것이다. 이전 커밋의 abc. txt 값을 알아야 바뀌었는지 그대로인지 알 수 있으니까! 그래서 기준점 이 있어야 수정되었는지 그대로인지를 알 수 있다. 아무튼 이렇게 공통 부모 노드를 기준으로 COMMIT_HASH_3 과 COMMIT_HASH_5를 비교해보면,  abc. txt 는 COMMIT_HASH_3 에서도 수정되었고, COMMIT_HASH_5 에서도 수정 되었다 =&gt; ** conflict ** a1b2c3. txt 는 COMMIT_HASH_3 에서는 수정되었고, COMMIT_HASH_5 에서는 그대로이다. =&gt; COMMIT_HASH_3 수정사항 선택123. txt 는 COMMIT_HASH_3 에서는 그대로이고, COMMIT_HASH_5 에서는 수정되었다. =&gt; COMMIT_HASH_5 수정사항 선택이렇게 확인할 수 있다. Conflict 가 발생한 abc. txt 파일에 대해서는 사람이 직접 선택해주면 된다. image. png git rebase: 그렇다면 rebase는 어떻게 다를까? 그리고 사람들은 왜 rebase 를 쓰지 않는걸 권장하는 걸까? 다시 아까의 그림으로 돌아가보자. image. png 이번에도 마찬가지로 abc. txt 는 conflict 가 날 것이다. 그래서 직접 사람의 손으로 수정을 해주는 것 까지는 똑같은데, 결과는 어떨까? image. png rebase 의 결과는 merge 와 다소 다르다. merge는 하나로 합쳐진 반면, rebase 는 Sub branch 에 있던 commit 들이 Main 브랜치의 뒤에 달라 붙은 결과가 되었다. 우리는 분명 COMMIT_HASH_1 을 기준(base) 로 파일을 비교했는데 COMMIT_HASH_5(new base) 의 뒤에 Sub branch를 추가한 것 같은 그림이다. rebase 는 심지어 cherry pick 이 가능한데 무슨 의미냐하면 sub branch에서 COMMIT_HASH_2는 숨기고 COMMIT_HASH_3 만 new base에 붙이는 것이 가능하다는 것이다. image. png rebase 의 문제점?: image. png 위의 그림처럼, sub branch 를 3개를 만들고 각각 회색 영역의 커밋이 초기 커밋이었다고 가정해보자. 세 가지의 sub branch 를 열심히 개발한다음에 하나로 합쳐가지고 Main branch 에 Merge 를 하려고 한다. git pull 을 해서 하나로 합치고, git rebase 를 써서 합친 뒤에 마지막으로 Main Branch 에 Merge 를 하니까, 노란색 배경의 커밋은 커밋 Hash 값은 다르지만 내용은 같은 커밋인데 Main Branch 에 2개나 생성된 모습을 볼 수 있다. image. png image. png Hash? SHA-256?: SHA-256 알고리즘: SHA-256 단계별 설명 위의 유튜브 영상에서 설명하는 과정 중에서, 헷갈리거나 설명이 생략되어 있는 부분을 위주로 작성하려고 한다.  “RedBlockBlue” 라는 예시 input 을 256비트로 바꾸기까지의 과정 Preprocess:  “RedBlockBlue” 를 ASCII 코드(각 8bit)로 변환한다. (예시인 “RedBlockBlue” 는 총 96bits (12글자 * 8bit) 총 비트 수를 512의 배수가 되게끔 padding 을 추가한다. 현재는 96bit 이기 때문에 512bit 가 되게끔 아래의 방법으로 padding 을 추가한다.      맨 뒤에 ‘1’ 을 추가한다.    (512에 가장 가까운 배수 - 64)bit 가 되기 위한 나머지 비트들은 전부 0으로 채운다.    64bit 를 남겨두는 이유는, 64비트에는 입력값(“RedBlockBlue”) 의 길이(96bit)를 이진수로 채운다.        image. png        패딩처리한 비트를 512bit 블록으로 나누어야한다. 예를 들어 패딩 처리한 bit 가 1024 bit 라면 2개의 블록으로, 현재 예시는 512bit 이므로 1개의 블록을 그대로 쓰면 된다.  블록마다 한 줄에 32bit 가 되게끔 16개의 줄로 나눈다. (1 개의 block = 512bit 이므로, 512 / 32 = 16줄)위의 예시 이미지에서 이미 32bit 가 되게끔 한 줄로 나누어서 표현했기 때문에, 해당 이미지를 참조. Hash Value: 이제 초기값이라고 부를 H0 ~ H7 까지, 8개의 32bit Hash 값을 정할 차례이다. 이 H0 ~ H7 에 어떠한 작업을 지속적으로 하면서, 마지막에 병합을 하게 된다면 우리가 원하는 256비트의 출력값이 된다. 그렇다면 초기값을 구하는 방법은 뭘까?  These words were obtained by taking the first thirty-two bits of the fractional parts of the square roots of the first eight prime numbers. 처음 소수는 2 이므로, 2의 제곱근은 1. 4142… 이므로 소수점 이하 부분(0. 4142…) 만 가져와서 32비트 값으로 변환한다. Bitwise Addition Modulo 2::  “Bitwise addition modulo 2”는 이진수 덧셈에서 자리올림(carry)을 고려하지 않고 단순히 XOR 연산으로 수행한다. ROTR, SHR: ROTRy은 y칸씩 오른쪽으로 Shift 하되, 더이상 Shift 할 곳이 없으면 앞으로 돌아오는 과정이다. (순환하는 것 처럼) SHRy은 똑같이 y칸씩 오른쪽으로 shift 하되, 맨 처음 y칸을 전부 다 0으로 만들어버린다. (더이상 shift 할 곳이 없으면 마치 절벽에서 밀어버리는 것 처럼 0이 되어버린다. ) Sigma Function: 첫 번째 사진의 σ0, σ1 는 소문자(lowercase) 시그마 라고 불리고, 두 번째 사진의 Σ0, Σ1은 대문자(uppercase) 시그마 라고 불린다. ⋙ 는 ROTR, ≫ 는 SHR 이고, ⊕ 는 Bitwise Addition Modulo 2 이다.  σ0(x)=(x⋙7)⊕(x⋙18)⊕(x≫3) σ&lt;sub&gt;1&lt;/sub&gt;(x)=(x⋙17)⊕(x⋙19)⊕(x≫10) Σ0(x)=(x⋙2)⊕(x⋙13)⊕(x⋙22) Σ&lt;sub&gt;1&lt;/sub&gt;(x)=(x⋙6)⊕(x⋙11)⊕(x⋙25)image. png image. png Ch function: Ch(e,f,g) 라는 함수가 나오는데 설명을 들어보니 Choose 의 ‘Ch’ 인가? 라는 생각이 들었다. Ch(e,f,g) 함수는 ‘e’ 를 기준으로 f 를 선택할 지, g 를 선택할 지 결정하는 함수이다. 간단하게 예를 들어, e = 01010001 f = 10011011 g = 00011111 가 있을 때, e가 0이면 g를, e가 1이면 f를 선택하는 것이다. Ch 함수의 결과값을 r 이라고 할 때, r[i] = e[i] === 0 ? g[i] : f[i] 함수라고 생각하면 될 것 같다. 비트 연산으로만 작성 한다면 아래와 같이 작성할 수 있다. javascriptconst choose = (x, y, z) =&gt; (x &amp; y) ^ (~x &amp; z);// const choose = (x, y, z) =&gt; (x &amp; y) | (~x &amp; z);/* x가 1이면 y 의 값을 가져오고, x가 0이면 z의 값을 가져온다. x가 0이면 앞의 x &amp; y 는 무조건 0이고, x 가 1이면 뒤의 ~x &amp; z 는 무조건 0이다. 따라서 두 연산중에 하나는 무조건 0이고, 남은 연산에서 값을 취해오면 된다. 남은 연산에서 값을 취해올 때는 or, xor 상관없이 가져올 수 있다. 따라서 ^ 연산이 아니라 | 연산이여도 상관 없다. */// ^ 연산자는 XOR 연산자이다. // ~ 연산자는 비트 반전 연산자이다. const x = 0b10101010101010101010101010101010;const y = 0b11001100110011001100110011001100;const z = 0b11110000111100001111000011110000;const result = choose(x, y, z);console. log(`Choose(x, y, z) result: ${result. toString(2). padStart(32,  0 )}`);Maj function: Maj(a,b,c) 함수는 Majority 함수라고 하는데, 이 함수도 상당히 간단하다. 그냥 0이 많은지 1이 많은지, 어떤 숫자가 더 많은지 보면 되는 함수이다. e = 01010001 f = 10011011 g = 00011111 Maj 함수의 결과값을 r 이라고 할 때, r[i] = e[i]+f[i]+g[i] &gt; 1 ? 1 : 0 이라고 생각하면 되겠다. (e[i] + f[i] + g[i] === 1의 갯수 이므로) 비트 연산으로 작성한다면 아래와 같이 작성 가능하다. javascriptconst majority = (x, y, z) =&gt; (x &amp; y) ^ (x &amp; z) ^ (y &amp; z);const x = 0b10101010101010101010101010101010;const y = 0b11001100110011001100110011001100;const z = 0b11110000111100001111000011110000;const result = choose(x, y, z);console. log(`Choose(x, y, z) result: ${result. toString(2). padStart(32,  0 )}`);Pipe: Day02 에서도 한 번 스쳐지나갔던 Pipe 를 함수형 프로그래밍을 공부하며 다시 한 번 보고, 이번에 또 보게 되었다. Pipe의 개념은 마치 통로로 연결하는 느낌과 비슷하다. 이전의 출력값을 다음 함수의 입력값으로 넘겨주는걸 반복하는, 마치 출구와 입구가 파이프로 이어져있는 것 같은 느낌으로 보면 될 것 같다. 자바스크립트에서는 따로 Pipe 커맨드는 없지만 구현을 할 수는 있다. javascriptfunction pipe(. . . funcList) { // Rest 파라미터를 사용해서, 실행할 함수들의 배열을 func 에 저장한다.  return (firstFuncArg) =&gt; {  funcList. reduce((prevFuncReturn, nextFunc) =&gt; {   return nextFunc(prevFuncReturn);  }, firstFuncArg); };}pipe(func1, func2, func3, func4)( firstFunctionArg );const pipeline = pipe(func1, func2, func3, func4);pipeline( firstFunctionArg );먼저 pipe 함수는 funcList 파라미터로 몇 개의 함수들이 넘어올 지, 알 수가 없다. 따라서 Rest 파라미터를 활용해서 funcList 파라미터를 받는다. pipe 함수는 바로 함수를 return 하는데, 이 함수는 내부적으로 funcList 의 reduce 를 돌면서, 함수의 반환값을 다음으로 넘긴다(prevFuncReturn) 다음 함수(nextFunc)는 prevFuncReturn 을 받아서 돌아가는 방식이 된다. pipe의 funcList 에 func1, func2, func3, func4 를 넣어두고, 첫 번째 함수의 인자를 넘겨주면 pipe 가 실행된다. Day 16-17 학습 정리Linux File System: FHS: Linux File System Explained!(이 유튜브 영상을 참고) 초기 리눅스 버전에서는, 다양한 배포판들에서 파일 시스템의 구조가 다 달라서 굉장히 혼란스러웠다고 한다. 이렇게 혼란스러울때마다 늘 등장하는 것은 역시 인터페이스인가보다. Linux 커뮤니티는 FHS(Filesystem Hierarchy Standard, 표준 파일 시스템 계층) 을 만들어서 획일화된 계층 구조를 제공했다. 물론 배포판들마다 조금씩은 다른 경우가 있다고 하지만, 아마 큰 틀에서 벗어나지 않는 정도일 것이다. 그렇다면 FHS 에서 핵심 디렉토리들에 대해서 한 번 살펴보자. 핵심 디렉토리들: image. png 실행 가능한 바이너리 (/bin, /sbin, /usr): 첫 번째로 ‘실행 가능한 바이너리 경로들’ 인, /bin. /sbin, /usr/bin, /usr/local/bin 이다.  /bin 에는 부팅 시, /usr 이 마운트(연결) 되기 전에 액세스해야 하는 핵심 OS 프로그램이 포함되어 있다. 즉 /usr 에 해당하는 부분이 연결되기 전에 /bin 에 있는 핵심 프로그램들이 먼저 준비가 되어있어야 한다는 것이다. mount, ls, cd 등이 /bin 에 위치하고 있다. /usr/bin 은 운영체제가 제공하는 바이너리의 경로가 아니라, 사용자의 프로그램들을 위한 바이너리가 담겨져 있다. 그렇기 때문에 usr 를 ‘user’ 로 착각할 수 있는데, usr =&gt; Unix System Resources 라고 한다.  /usr/local/bin 에는 일반적으로 소스에서 빌드한 후, 관리자가 설치한 실행 파일이 보관된다. 그렇다면 /usr/bin 과 /usr/local/bin 의 차이는 뭘까? bin 오래된 글이긴 하지만, 스택오버플로우의 글에 따르면 /usr/bin 은 기본 패키지 매니저에서 제공되는 소프트웨어를, /usr/local/bin 은 기본 패키지 매니저 외부에서 제공되는 소프트웨어를 저장하는 경로라고 한다. 예를 들어, 인텔 맥북의 경우에는 homebrew 를 통해 설치한 패키지들이 /usr/local/bin 에 저장되었다고 한다. (M1 이상부터는 /opt/homebrew/bin 에 저장된다고 한다. ) 영상에서는 firefox, VLC(동영상 플레이어) 등으로 예시를 들었다. /sbin 은 root 권한이 필요한 sysadmin 유틸리티(iptables, sshd, …)가 포함되어 있다. 이렇게 시스템 바이너리들을 위한 /bin 과 /usr 를 따로 분리함으로써, 시스템 바이너리들을 덮어쓰지 않고 별도로 유지할 수 있다. 추후 접근 시에는 이런 실행파일들의 경로에 대한 우선순위에 맞게 탐색하게 된다. lib:  /lib 에는 /bin, /sbin 바이너리에 필요한, 필수 라이브러리 파일들이 포함되어 있다. /usr/lib 에는 초기 시스템 초기화에 중요하지 않은 /usr 바이너리용 라이브러리가 들어있다. 예시로 들었던 firefox 나, VLC 를 위한 라이브러리들이 굳이 시스템 초기화에 필요하지 않기 때문에 분리해놓은 것이다. etc:  etc 에는 text-based config file 들이 존재하는데, 네트워킹부터 인증 서비스까지 모든 것을 제어한다. 대표적인 예시로 우리가 자주 사용하는 /etc/ssh/sshd_config 파일이나, /etc/crontab, /etc/sudoers, /etc/network/interfaces 등이 etc 에 있다. home: 이전까지는 /usr 가 사용자 디렉토리가 왜 아닌지 몰랐지만 이제는 안다. home 디렉토리가 사실 진짜 사용자의 데이터를 저장하는 디렉토리라는 것을 문서, 미디어, 프로젝트 등등 우리가 실질적으로 자주 쓰는 파일들이 해당한다. root: home 디렉토리가 사용자 디렉토리라면, root 디렉토리는 root 사용자의 디렉토리이다. 관리자 전용으로 사용해야하는, 일반 사용자들은 접근할 수 없도록 설계되어 있다. var:  로그 및 캐시와 같이 빠르게 변화하는 데이터는 /var 에 존재한다. 특히 /var/log 에는 하드웨어 이벤트, 보안 이슈들, 성능 문제 등등이 기록되기 때문에 늘 접근해야하는 곳이라고 한다. run:  /run 에는 시스템 세부정보, 사용자 세션, 로깅 데몬과 같은 일시적인 런타임 정보가 포함되어 있다. 예를 들면 프로세스 ID 를 저장하는 PID 파일들이나, 소켓 파일이나, 리소스 접근을 조정하는 락 파일 등이 저장된다. 이 파일들은 전부 일시적 이라는걸 생각해보면 조금 더 이해가 쉬울 것 같다. proc, sys:  proc 은 전체 OS 상태를 검사하기 위해 통신 채널을 연다. cpuinfo 를 통해 high level 측정항목을 확인하고, 파일 시스템 마운트를 확인하고, lsof / strace / pmap 과 같은 도구를 사용해서 더 자세히 살펴볼 수 있다.  sys 는 low level 커널 및 하드웨어를 노출하여, 가상 파일을 통해 장치, 모듈, 네트워크 스택과 같은 구성 요소를 세부적으로 모니터링하고 구성할 수 있다. 이렇게 proc 과 sys 에서는 메트릭을 수집할 수 있다고 하는데, 그렇다면 메트릭을 수집하는 프로메테우스나 metrics server 같은 것들은 전부 proc 과 sys 에서 수집해가는걸까? shell docker run -d --rm -p 9256:9256 --privileged -v /proc:/host/proc -v `pwd`:/config ncabatoff/process-exporter --procfs /host/proc -config. path /config/filename. yml궁금해서 찾아봤더니, prometheus exporter 프로메테우스의 exporter 는 /proc 에서 정보를 mines 한다. . ! Stream and Buffer: 우리가 영화나 드라마를 볼 때, 길이가 1시간이 넘는 영상을 전부 로딩하는걸 기다렸다가 재생하지는 않는다. 스트리밍 이라는 용어를 쓰면서, 실시간으로 조금씩 영상이 로딩되면서 재생이 된다. 네트워크가 원활하다면 버퍼링 없이 볼 수 있을테고, 네트워크가 잠깐 끊기거나 운이 좋지 않다면 아예 새로고침까지 해서 다시 로딩해야할 수도 있다. 그렇다면 스트림과 버퍼의 의미는 뭘까?참고 영상 Stream:  Stream is a sequence of data that is being moved from one point to another over time Stream은 시간이 흐름에 따라 한 곳에서, 다른 곳으로 이동하는 데이터들의 연속적인 흐름이다. 전체 데이터가 전부 도착하기를 기다리기 보다는, 도착한 데이터 덩어리(chunk)들의 stream을 바로 process 한다. 데이터를 한 번에 메모리에 담아놓고 처리하지 않기 때문에, 메모리 사용량 면에서 효율적이다. 그렇다면 이 sequences of data 가 어떻게 움직이는 걸까? Buffer: 버퍼는 데이터들을 저장하고 있는 임시 공간이다. 혹시 유튜브 영상을 볼 때 버퍼링이 걸려서 로딩이 걸려본 적이 있다면 이해가 조금 더 쉬울 것 같다. 인터넷 속도가 느려져서 버퍼에 영상을 플레이할 만큼의 데이터가 들어있지 않다면, 데이터가 조금 더 도착할때까지 로딩을 하는 것이다. 그렇게 버퍼가 가득차면 그 만큼의 영상을 우리가 볼 수 있게 되고 만약 앞선 버퍼만큼의 영상을 다 보는 동안 다음 버퍼가 채워지지 않았다면 또 대기를 하게 된다. 이번엔 node. js 코드로 버퍼에 대해서 한 번 살펴보자. javascriptconst buffer = new Buffer. from( what the hello world ,  utf-8 ); // String을 받아서 문자열 인코딩에 맞는 Buffer 객체 생성console. log(buffer);console. log(buffer. toJSON());/*두 출력값이 다른 이유는, buffer 에는 16진수로 담겨져있고 toJson 에는 아스키코드에 해당하는 숫자로 담겨있기 때문이다. 77(16) -&gt; 119(10)&lt;Buffer 77 68 61 74 20 74 68 65 20 68 65 6c 6c 6f 20 77 6f 72 6c 64&gt;{ type: 'Buffer', data: [  119, 104, 97, 116, 32, 116, // w, h, a, t, ' ', t  104, 101, 32, 104, 101, 108, // h, e, ' ', h, e, l  108, 111, 32, 119, 111, 114, // l, o, ' ', w, o, r  108, 100 // l, d ]}*/이번에는 buffer 에 write 하면서 어떤 일이 생기는지 살펴보자. javascriptconst buffer = new Buffer. from( what the hello world ,  utf-8 ); // String을 받아서 문자열 인코딩에 맞는 Buffer 객체 생성console. log(buffer. toString());console. log(buffer);console. log(buffer. toJSON());buffer. write( abcdefg );console. log(buffer. toString());buffer. write( abcdefghijklmnopqrstuvwxyz );console. log(buffer. toString());/*what the hello world&lt;Buffer 77 68 61 74 20 74 68 65 20 68 65 6c 6c 6f 20 77 6f 72 6c 64&gt;{ type: 'Buffer', data: [  119, 104, 97, 116, 32, 116,  104, 101, 32, 104, 101, 108,  108, 111, 32, 119, 111, 114,  108, 100 ]}abcdefge hello worldabcdefghijklmnopqrst*/buffer. write(string)은 버퍼에 덮어쓰기를 실행하는데 보다시피 what the hello world -&gt; abcdefge hello world 로 바뀐 것을 볼 수 있다. 이는 앞에서부터 덮어쓰기를 실행한 결과이다. 두 번째로 buffer. write(“abcdefghijklmnopqrstuvwxyz”); 를 실행했더니 abcdefge hello world -&gt; abcdefghijklmnopqrst 로 바뀐 것을 볼 수 있다. 분명히 …xyz 까지 write 를 했는데 …pqrst 까지 밖에 write 되지 않은 것을 볼 수 있다. 이는 buffer. from(“what the hello world”)로 버퍼 객체를 만들면서 최대 크기를 “what the hello world” 크기로 지정한 것이기 때문이다. 초과한 양은 전부 버려진다. -&gt; 버퍼 오버플로우 Buffer 와 Stream 은 그래서 무슨 관계가 있는거야?: 우선! 알아두면 좋은 것은 Stream은 EventEmitter 의 instance 이다. Stream 에서 버퍼가 가득찼는지 등의 이벤트 가 발생하고 이를 Listener 함수로 처리할 수 있다. 아래에서 한 번 더 설명하겠지만 미리 알고 있으면 좋을 것 같다. node. js 의 Stream은 스트리밍 데이터를 처리하기 위한 인터페이스로, 4가지의 타입이 있다.  Writable: streams to which data can be written (for example, fs. createWriteStream()).  Readable: streams from which data can be read (for example, fs. createReadStream()).  Duplex: streams that are both Readable and Writable (for example, net. Socket).  Transform: Duplex streams that can modify or transform the data as it is written and read (for example, zlib. createDeflate()). Stream 은 위에서 언급했던 것처럼 다른 곳으로 이동하는 데이터들의 연속적인 흐름이라는 추상화된 개념 이고, 그 데이터들을 담고 있는 것이 버퍼이다. 하지만 이것만으로는 node. js 에서 어떻게 활용해야할지 감이 잘 안잡혀서, node. js 의 stream 이 어떤 방식으로 동작하는지 조금 이해해보기 위해 공식 문서를 읽어보면서 각 타입이 어떤건지 알아보자. stream buffering  The amount of data potentially buffered depends on the highWaterMark option passed into the stream’s constructor. For normal streams, the highWaterMark option specifies a total number of bytes. For streams operating in object mode, the highWaterMark specifies a total number of objects. For streams operating on (but not decoding) strings, the highWaterMark specifies a total number of UTF-16 code units. 우선 highWaterMark 라는 Stream 생성자의 옵션을 통해서 내부 버퍼의 사이즈를 정해줄 수 있다. 또한 normal, object mode, string 마다 highWaterMark 가 의미하는 바가 조금씩 다르다고 한다. Readable Stream: Readable Stream  Data is buffered in Readable streams when the implementation calls stream. push(chunk). If the consumer of the Stream does not call stream. read(), the data will sit in the internal queue until it is consumed.  Once the total size of the internal read buffer reaches the threshold specified by highWaterMark, the stream will temporarily stop reading data from the underlying resource until the data currently buffered can be consumed (that is, the stream will stop calling the internal readable. _read() method that is used to fill the read buffer). Readable stream 에서는 stream. push(chunk) 를 호출할 때마다 데이터를 읽고 내부 버퍼에 데이터를 쌓는다. 따로 consumer 가 stream. read() 로 데이터를 소비하지 않는 이상, 쭉 누적 된다. 그러다가 만약 내부 버퍼의 사이즈가 highWaterMark 에 도달하게 되면 스트림은 버퍼에 쌓인 데이터들이 소비될때까지 “데이터를 읽고 버퍼에 쌓는 과정”(=&gt;readable. _read())을 멈춘다. event: Readable Stream 에는 close, data,end, error, pause, readable, resume 이벤트가 있다. 바로 위의 Readable Stream 링크를 눌러서 공식문서를 확인해보면 자세히 나와있어서 알아두어야 할 이벤트만 정리해보려고 한다.    data   data 이벤트는 stream 이 데이터를 읽어올 처리할 준비가 되었을 때 발생하는 이벤트이다. 어떤 상태인지 알아봤더니,   스트림에서 새로운 데이터가 읽혀질 때마다 data 이벤트가 발생한다고 한다.     readable      The ‘readable’ event is emitted when there is data available to be read from the stream, up to the configured high water mark (state. highWaterMark). Effectively, it indicates that the stream has new information within the buffer. If data is available within this buffer, stream. read() can be called to retrieve that data. Additionally, the ‘readable’ event may also be emitted when the end of the stream has been reached.     readable 이벤트는 스트림으로부터 데이터를 읽을 수 있을 때 호출된다. 스트림의 버퍼에 읽을 수 있는 데이터가 존재한다는 뜻이되고, stream. read() 로 데이터를 소비할 수 있다. 추가로 스트림의 끝에 도달했을때도 readable 이벤트가 한 번 더 호출된다고 한다.   다시 한 번 이벤트가 발생하는게 조금 의아해서 chatgpt 에게 물어본 결과인데, 참고만 하자.     참고용 Chatgpt    1. 버퍼에 남아있는 데이터 처리 마지막 데이터 조각: 스트림이 끝나기 직전에 버퍼에 일부 데이터가 남아 있을 수 있습니다. 이 남은 데이터를 처리하기 위해 'readable' 이벤트가 한 번 더 발생하는 것입니다.  완전한 처리: 스트림에서 모든 데이터를 정확하게 처리하기 위해서는 버퍼에 남아있는 마지막 데이터 조각까지 처리해야 합니다.  2. 스트림 종료 시점 명확화 스트림 종료 신호: 'readable' 이벤트와 함께 스트림의 종료를 알리는 추가적인 신호가 제공될 수 있습니다. 예를 들어, end 이벤트가 발생하면 스트림이 완전히 종료되었다는 것을 알 수 있습니다.  안정적인 종료: 스트림의 종료 시점을 명확하게 하여 프로그램의 안정성을 높일 수 있습니다.  3. 데이터 처리 로직 완성 마무리 작업: 스트림이 끝나기 전에 마무리해야 할 작업이 있을 수 있습니다. 예를 들어, 파일을 닫거나 연결을 해제하는 작업이 필요할 수 있습니다. 'readable' 이벤트를 통해 이러한 마무리 작업을 수행할 수 있습니다.  4. 스트림의 특성 비동기 처리: 스트림은 비동기적으로 데이터를 처리하기 때문에, 실제로 데이터가 모두 읽혀졌더라도 이벤트 루프에서 처리되는 데 약간의 시간이 걸릴 수 있습니다.  버퍼링: 스트림은 내부적으로 버퍼를 사용하여 데이터를 관리합니다. 버퍼의 크기나 스트림의 설정에 따라 'readable' 이벤트 발생 시점이 달라질 수 있습니다.  즉, 스트림 끝에 도달했음에도 불구하고 'readable' 이벤트가 발생하는 것은 버퍼에 남아있는 데이터를 처리하고, 스트림의 종료를 명확하게 하기 위한 메커니즘입니다.    Writable Stream: Writable Stream  Data is buffered in Writable streams when the writable. write(chunk) method is called repeatedly. While the total size of the internal write buffer is below the threshold set by highWaterMark, calls to writable. write() will return true. Once the size of the internal buffer reaches or exceeds the highWaterMark, false will be returned. Writable Stream 에서는 writable. write(chunk) 를 반복적으로 호출하면서 내부 버퍼에 데이터를 쌓는다. 만약 내부 버퍼의 용량이 아직 highWaterMark 를 넘지 않는다면 true 를, 넘는다면 false 를 반환하는데, 반복적으로 writable. write(chunk) 를 수행하다가 false 가 반환되면 더 이상 버퍼에 쓰는걸 멈추고 fs 같은 파일시스템으로 작성한다든지의 처리를 하면 된다. event: Writable Stream 에는 close, drain, error, finish, pipe, unpipe 이벤트가 있다. 마찬가지로 Writable Stream 링크를 눌러서 공식문서를 확인해보면 자세히 나와있어서 알아두어야 할 이벤트만 정리해보려고 한다.    drain      If a call to stream. write(chunk) returns false, the ‘drain’ event will be emitted when it is appropriate to resume writing data to the stream.     drain 이벤트는 stream. write(chunk) 의 결과값이 false 일때(버퍼가 가득 찼을 때)가 되고난 후, 데이터가 잘 소비되어서 버퍼에 공간이 생겨서 다시 쌓을 준비가 되는 순간이 있을 것이다.  이때 drain 이벤트가 emit 된다.   버퍼가 가득 찼을 때 호출 되는 이벤트가 아니다. 버퍼에 다시 데이터를 저장할 수 있는 순간에 호출이 되는 것이다.     pipe   Readable stream 에서 stream. pipe() 를 호출 했을 때 발생하는 이벤트이다. Readable stream 에서 읽은 데이터를 파이프를 통해 넘겨주기 위해서, 파이프를 연결한 것이다.     unpipe   pipe 와 반대로 Readable stream 이 연결된 writable stream 과의 pipe 를 끊어낼 때 호출되는 이벤트이다.  Duplex, Transform:  Because Duplex and Transform streams are both Readable and Writable, each maintains two separate internal buffers used for reading and writing, allowing each side to operate independently of the other while maintaining an appropriate and efficient flow of data. For example, net. Socket instances are Duplex streams whose Readable side allows consumption of data received from the socket and whose Writable side allows writing data to the socket. Because data may be written to the socket at a faster or slower rate than data is received, each side should operate (and buffer) independently of the other. Duplex 와 Transforrm stream 은 Readable 과 Writable 이 동시에 가능한 stream 이다. 따라서 내부 버퍼도 2개(read, write) 를 가지고 있다. 두 개의 내부 버퍼는 독립적으로 동작하므로 효율성이 좋다고 한다. 예를 들어 Duplex 의 인스턴스인 net. Socket 은 소켓으로부터 읽은 데이터를 소비하는 Readable, 소켓으로 데이터를 write 하는 두 개의 내부 버퍼가 각각 독립적으로 동작한다. 데이터의 쓰기와 읽기가 속도 차이가 날 수 있기 때문에 서로 독립적으로 작동해야 한다고 한다. Stream 의 목적:  A key goal of the stream API, particularly the stream. pipe() method, is to limit the buffering of data to acceptable levels such that sources and destinations of differing speeds will not overwhelm the available memory. Stream API, stream. pipe 의 궁극적인 목표는 데이터의 버퍼링을 어느정도 수준으로 제한해서 데이터를 보내는 쪽과 받는 쪽의 속도 차이가 나도, 메모리 사용량을 초과하지 않게끔 하는 것이다. 데이터를 빠르게 받거나 보내는 경우에도, 데이터를 중간에 너무 많이 쌓아두지 않고 바로바로 처리해서 메모리를 효율적으로 사용한다는 의미로 받아들이면 될 것 같다. stream. pipe() 도 마찬가지의 역할을 하는 것 같다. 그래서 node. js 에서 어떻게 활용해?: stream 이 readable, writable 을 통해서 버퍼에 데이터를 저장한다는 것 까지는 이해를 했다. 보통 stream 을 활용하는 곳이 HTTP 통신이거나 파일일 것 같아서 파일을 읽고 쓰는 방법을 한 번 알아보려고 한다. createReadStream: javascriptimport fs from  fs ;const readStream = fs. createReadStream( inputFile. txt ,  utf8 );readStream. on( data , (chunk) =&gt; { console. log( data , chunk);});readStream. on( end , () =&gt; { console. log( end );});javascriptimport fs from  fs ;const readStream = fs. createReadStream( inputFile. txt ,  utf8 );readStream. on( readable , () =&gt; { let chunk; while (null !== (chunk = readStream. read(15))) {  console. log(chunk); }});readStream. on( end , () =&gt; { console. log( end );});readStream 의 data 이벤트와 readable 이벤트 모두 데이터를 버퍼로부터 읽어올 수 있는데, data 의 경우 chunk 가 그대로 리스너 함수에 넘어온다. 반면 readable 이벤트는 read() 를 통해서 직접 읽어야하는데, 이때 얼만큼 읽을지 정할 수 있다. createWriteStream: javascriptimport fs from  fs ;const writeStream = fs. createWriteStream( outputFile. txt , { highWaterMark: 50 });writeStream. on( drain , () =&gt; console. log( buffer ready ));setInterval(() =&gt; { while (writeStream. write( hello world ));}, 1000);writeStream 에서는 어떤 방식으로 동작하는지 확인해보려고 일부러 버퍼의 크기를 50으로 제한했다. 이후에 1초마다 writeStream 에 “hello world” 라는 문자열을 write 를 시도하는데, 위에서 적은 것 처럼 drain 이벤트는 writeStream. write() 가 false 인 상태에서, 다시 버퍼에 무언가 적는게 가능할 때 호출된다. 이대로 코드를 돌려보면 hello world 를 사이즈가 50이 될 때 까지 write 하고, 주기적으로 버퍼에 공간이 생길때마다 “buffer ready” 라는 출력이 찍힌다. 이후에 또 hello world 가 write 되고 계속 반복된다. 위의 코드는 drain 이벤트 실험을 위해서 작성한 코드이고, 실제로 파일을 작성할 때는 아래처럼 진행하면 된다. javascriptimport fs from  fs ;const writeStream = fs. createWriteStream( outputFile. txt , { highWaterMark: 50 });writeStream. write( hello world );writeStream. end();pipe: javascriptimport fs from  fs ;const readStream = fs. createReadStream( inputFile. txt , { highWaterMark: 5 });const writeStream = fs. createWriteStream( outputFile. txt , { highWaterMark: 5 });writeStream. on( drain , () =&gt; console. log( drain ));readStream. pipe(writeStream);readStream 의 버퍼에서 읽어온 값을 바로 writeStream 의 내부 버퍼로 들어가게 된다. 출력해보면 지속적으로 drain 로그가 찍힌다. Day 18-19 학습 정리Telnet vs SSH: Telnet vs SSH Telnet: 텔넷은 원격 서버에 액세스하기 위해 사용되는 터미널 에뮬레이션 프로그램이다. 터미널로 명령을 이용하여 서버에 수행 할 작업을 지시할 수 있다. 또한 라우터 및 스위치와 같은 다른 네트워크 장치를 관리하고 구성하는데도 사용할 수 있다. 1969년에 개발되었다보니 서버로 전송하는 명령에 암호화 같은 것은 없다. SSH: SSH는 Secure Shell 로, 텔넷보다 보안이 강화되었다. 암호화 및, 공개 키 인증을 통해 데이터를 갈취당하지 않고 안전하게 서버로 보낼 수 있다. 통신 프로토콜: TCP: 연결 지향이라서 연결 과정이 필요하다.  서버(소켓) 생성 서버와 클라이언트 소켓을 연결node. js: net. Server 소켓 서버 클래스 -&gt; 서버 (eventEmitter)net. Socket 소켓 클래스 -&gt; 서로 연결하는 스트림(duplex) javascript/*const server = net. createServer([options][,connectionListener]);server. listen(port[,host][,backlog][,callback]) // 클라이언트 접속 대기server. close([callback])server. getConnections(callback)server. address() : 서버 주소*/// 서버 측const server = net. createServer((socket) =&gt; { // connection 이벤트 리스너 함수});server. on( listening , () =&gt; {});server. on( close , () =&gt; {});// 클라이언트 측const socket = new net. Socket();const option = { host:  localhost , port: 3000,};socket. connect(option, () =&gt; {});net. Server event:  listening : 포트 바인딩, 접속 가능한 상태 이벤트 conneciton : 클라이언트 접속 이벤트 close : 서버 닫을 때 error: 에러net. Socket event:  connect data : stream data 이벤트 end timeout error세션? JWT?: 쿠키: 서버에서 클라이언트에 대한 정보를 기록해두기 위해서, 클라이언트의 브라우저에 저장하는 데이터 클라이언트에서 서버에 요청을 보내고, 응답을 받았을 때 쿠키가 들어있을 수 있다. 이후 서버에 요청을 보낼 때, 브라우저는 계속 자동으로 쿠키를 담아서 요청을 보내게 된다. 쿠키는 도메인에 따라 제한되기 때문에, 쿠키를 받은 서버에만, 해당 쿠키를 보낸다. 쿠키는 아래의 세션ID 를 담고있는 매개체이기도 하다. 쿠키는 사이즈에 제약이 있다. 세션: HTTP 프로토콜은 stateless 이다. stateless 라는 의미는, 이전의 리퀘스트와 현재의 리퀘스트는 독립적으로 이루어진다는 의미이다. 서버에서 요청에 대한 처리를 하고 클라이언트가 응답을 받으면, 새로운 요청을 보낼 때마다 클라이언트는 자신의 정보를 담아서 보내야 한다. 서버에서 세션 DB에 데이터를 저장하고, 해당 세션의 ID를 클라이언트에게 돌려준다. 클라이언트의 브라우저는 쿠키에 세션 ID 를 저장하고 있다가, 앞으로 있을 요청에 계속 세션 ID와 함께 보낸다. 서버는 쿠키에 담긴 세션 ID를 가지고 세션 DB를 탐색하여 사용자의 정보를 찾아낸다. 유저는 세션 ID만 보내게 되면 서버가 알아서 세션 ID를 가지고 사용자의 정보를 알아내기 때문에, 클라이언트 입장에서 매우 편해진다. 세션에서 가장 중요한 것은, 서버에서는 현재 로그인한 유저들의 모든 세션 ID 값을 DB에 저장하고 있어야 한다. 토큰: 세션ID를 담아서 보내기 위해서는 쿠키가 필요하다. 그러나 쿠키는 브라우저에만 있기 때문에, 앱 어플리케이션에서는 쿠키를 이용할 수 없다. 따라서 앱 어플리케이션에서 세션을 구현하기 위해서는 쿠키 대신 토큰을 이용한다. 토큰은 그저 String 인데, 서버는 이 토큰과 일치하는 유저 정보를 세션 DB에서 찾아낸다. JWT(Json Web Tokens): JWT도 이름에서 알 수 있는 것처럼, 토큰 형식이다. 대신 다른점은, 서버에서 세션 DB를 가지고 있을 필요가 없다. 서버는 토큰을 가지고 유저 인증(사용자 정보)을 위해서 이런 저런 일을 할 필요가 없다. 사용자가 로그인을 하게 되면, 세션은 로그인 중인 사용자의 정보를 DB에 저장하는 반면, JWT는 따로 DB에 저장하지 않는다. 대신 어떠한 정보(예를 들면 유저의 ID)를 기반으로 사인 알고리즘을 이용해서 사인을 한다. 이후 사인된 정보를 문자열 형태로 클라이언트에게 돌려준다. JWT는 사이즈에 제약이 없어서 길어도 된다. JWT를 발급받고 나서 다시 서버에 로그인을 하려면, JWT 토큰 혹은 사인된 정보 를 서버에 보내야한다. 보통 JWT토큰을 다시 보내서 서버가 받으면 해당 사인이 유효한지 체크해서 토큰이 유효하다면 사용자 인증을 하게 된다. 즉 JWT에서는 유저를 인증하는데 필요한 정보가 전부 토큰에 담겨있다. 토큰에는 길이의 제한이 없기 때문에. 토큰이 유효하다면, 사용자도 유효하다는 뜻이다. JWT는 암호화된 문자열이 아니다. 암호화가 되어 있으면 이를 읽고 이해할 수가 없기 때문이다. 따라서 누구나 JWT 토큰을 열어서 확인할 수 있다. 따라서 JWT 토큰에는 비밀번호 같은 것이 담기면 안된다. 세션 vs JWT: 세션은 서버에서 유저의 정보를 가지고 있기 때문에 다양한 추가 기능을 구현할 수 있다. 대신 DB에 저장을 해야하는 만큼 메모리가 필요하다. JWT는 토큰 자체에 유저를 인증할 수 있는 정보가 있기 때문에 유저 인증을 위한 DB가 필요하지 않다. JWT 토큰은 만료되기 전까지는 무조건 유효하다. 그렇기 때문에 이 토큰이 현재 유효한지, 아닌지만을 판별할 수 있지 다른 것이 불가능하다. UUID: UUID는 ‘Universally Unique Identifier’의 약자로 128bit 의 고유 식별자이다. 이전에 공부했던 SHA-256이 생각났었는데, 실제로 UUID 의 여러 버전 중에 해싱 알고리즘을 쓰는 버전이 있다고 한다. 신기한게 버전별로 UUID가 생성되는 방식이 다른데, 이를 생각보다 주의해서 생성해야 할 것 같다. UUID v1, UUID v2 버전은 타임스탬프 UUID 라고 부르는데, UUID가 만들어진 시점과 기기 정보를 토대로 UUID가 만들어지는 것 같다. UUID v3, UUID v5 버전은 네임스페이스 UUID로, 해싱 알고리즘을 통해서 암호화된 UUID 가 생성된다. v3는 MDA, v5 는 SHA-1 알고리즘을 기반으로 생성된다고 한다. UUID v4는 랜덤값을 기반으로 생성된다. "
    
}, 

{
    "id": 9,
    "url": "https://hoex5h.github.io//%EB%84%A4%EB%B6%80%EC%BA%A0_%EC%B1%8C%EB%A6%B0%EC%A7%80-_3%EC%A3%BC%EC%B0%A8_%ED%9A%8C%EA%B3%A0",
    "title": "[네부캠 챌린지] 3주차 회고",
    "body": "2025/01/14 - 3주차 회고3주차는 동료들과 함께하는 시간이 더 늘어났다. 기대 반, 걱정 반으로 한 주를 보냈는데 3주차가 다른 날들에 비해 어떤 점이 달랐는지 회고해보려고 한다. 생활 패턴: 사실 2주차까지도 생활 패턴을 잘 지키지 못했다. 1주차를 진행하면서 ‘늦어도 2시에 자야겠다’ 라고 생각을 했었는데 2주차에서는 ‘늦어도 4시엔 자야겠다’ 라는 생각으로 바뀌었고 3주차를 진행하면서는 ‘잠을 자긴 해야겠다’ 로 바뀐 것 같다. 여러모로 잠을 못 자서 힘든 한 주긴 했지만, 컨디션이 아예 망가져서 하루를 헛되게 날렸다거나 하는 날이 생기진 않았고 ‘정신적인 고통이 조금 느껴졌던 한 주’ 정도라고 생각해서 컨디션 관리는 이쯤이면 만족하고 있다. 동료 개선/프로그래밍: 이번 주차에서 동료와 함께 설계하고, 동료와 함께 프로그래밍을 하면서 ‘잘못하면 내가 민폐를 끼칠 수도 있겠다’ 는 생각에 걱정이 조금 있었다.  다행히도 친절한 동료분들을 만나서 생각보다 즐겁고, 자유로이 의견을 공유하는 분위기 속에서 진행을 했다. 그러나 내가 했던 걱정이 잘못된 걱정이라는 생각은 딱히 들지 않았고, 언제든지 실력차가 많이 나는 개발자들을 만나야만 하는 일이 생긴다면 저런 걱정을 하면서 더욱 열심히 하는 수 밖에 없다고 생각한다. 저 걱정에서 자유로울 수 있는 사람이 있을까? 아무튼 동료와 함께 개선을 하면서는 동료가 어떤 관점으로 코드를 짜는지 많이 배웠다. 나는 주로 코드가 굴러가는 것이 먼저라고 생각하면서도 늘 ‘지금 다시 돌아가면 더 깔끔한 코드를 짤 수 있을 것 같은데’ 라는 생각에 사로잡히는 것 같다. 하지만 매번 챌린지 미션을 하면서 시간이 남았던 적이 없었기에 생각은 생각으로만 두고 기존에 돌아가던 코드에서 다시 또 새로운 코드들을 누적해나간다. 그리고 이번에 동료의 관점을 좀 자세히 살펴보면서, 우선시하는 사항들에 대해서 만큼은 설계를 깊게 하고 구현을 하는게 맞다고 느꼈다. 나는 늘 구현을 하면서 의존성 에 대해 생각을 하는데 설계가 깊지 못하다보니 생각했던 것 외의 의존성이 여기저기서 생겨난다. 그리고 구현하다보면 ‘아 코드 이거 다 지우고 다시 짜면 의존성을 조금 더 없앨 수 있겠는데. . ’ 라는 생각에 빠져버리는 것이다. 하지만 동료가 의존성을 고려할 때는, 의존성만큼은 확실하게 잡고 가는 설계를 했다. 구현이 나랑 얼마나 다른지와는 별개로 의존성만큼은 제대로 고려했다. 많은 생각이 드는 개선이었고 다음주에 설계를 할 때는 의존성에 대해 깊이 고민을 해보고 출발하려 한다. 피어 프로그래밍을 하면서는 챌린지 전반을 바라보는 시선에 대해서 많이 배웠다. 챌린지를 하면서 열심히 안하는 사람은 없다고 생각하지만 이번주에는 ‘챌린지를 마냥 열심히만 하는게 아니라 내 성장에 도움이 되는 방향으로 해야하는게 맞다’ 라는 생각이 들었다. ‘네부캠에서 시키는것만 잘하자, 시키는거 제출만 하자. ’ 이런 생각은 무슨 의미가 있을까 결국 내가 언젠가 다시 보려고 하는게 학습 정리고 회고인걸 동료를 통해 더욱 깊게 배웠다. 그 외에도 이것저것 얘기를 많이 나누면서 코딩만이 아니라 다른 생각도 많이 해보는 기회였던 것 같다. 다음주에는 어떻게 할까: 설계 관점: 내가 설계를 하고 구현을 하면서 가장 많이 하는 고민은 의존성 인 것 같다. 구현 전반에 걸쳐 하는 고민인 만큼, 설계에서만큼은 단단해야 한다. 설계를 하면서 의존성에 대한 고민은 최대한 깊게 해보고 구현을 시작해보자. 학습 정리 및 회고: 동료들의 개인 회고나 학습 정리를 ‘깔끔하게 정리하는 스킬’은 초짜인 내가 단기간에 습득하긴 어려운 스킬이다. 다음주에는 ‘내가’ 보기에 나쁘지 않은 형태로 학습 정리와 회고를 하는 것에 집중해보는 한 주가 되길 바라면서 임해볼 생각이다. "
    
}, 

{
    "id": 10,
    "url": "https://hoex5h.github.io//%EB%84%A4%EB%B6%80%EC%BA%A0_%EC%B1%8C%EB%A6%B0%EC%A7%80-_2%EC%A3%BC%EC%B0%A8_%ED%9A%8C%EA%B3%A0",
    "title": "[네부캠 챌린지] 2주차 회고",
    "body": "2025/01/14 - 2주 차 회고챌린지 과정을 진행한 지 벌써 2주가 흘렀다. 나는 1주 차보다 더 나아졌을까? 생활 패턴: 일단 1주 차에 비해서 생활방식이 크게 개선된 것 같지는 않다. 1주 차 때도 미션을 완전하게 완성한 적이 거의 없어서 매번 새벽까지 하다가 잤는데, 2주 차 때도 역시 미션을 놓지 못해서 새벽까지 진행했던 것 같다. 1주 차를 끝내면서 2주 차에는 컨디션 관리까지 해내리라 마음먹었었는데 2주 차까지 해본 결과, 3주 차 4주 차에서도 컨디션 관리는 포기하는 게 나을 것 같다. 학습: 1주 차 때는 학습하는 시간이 적었다. 그때는 오로지 구현을 해야 한다 라는 생각만 했었고 실제로 문제를 읽은 뒤에 학습보다는, 구현하면서 모르는 것이 나오면 그제야 잠깐 찾고 다시 구현하는 방식으로 미션을 진행했었다. 그러다 보니 구현하면서 헤매는 일도 많았고 오히려 진행이 더딘 경우도 많았다. 1주 차를 회고하며 느낀 점은 어떤 게 올바른 진행 방식인지는 몰라도, 1주 차 때 했던 방식이 잘못되었다는 것은 확실하다는 것이었다. 2주차때는 미션을 보고 바로 구현에 뛰어들기보다는 배경지식을 먼저 학습하려고 시도했다. 그렇지만 여전히 목표는 ‘구현’ 이었다보니 학습 후 바로 구현으로 뛰어들었고, 시간이 남으면 학습정리를 하자! 라는 생각이었다. 사실 이 방법이 완전히 틀린 방식인가? 를 생각했을때는 잘 모르겠다. 구현과 학습정리 중에서 어떤 것이 우선시 되어야 하는가? 는 여전히 잘 모르겠고 학습정리는 다음에 시간이 또 생길 수 있지 않을까? 라는 안일한 생각도 좀 많이 했던 것 같다. 그래서 3주차에는 학습 후, 구현과 학습정리를 왔다갔다 반복하면서 진행해보려고 한다. 2주차와 3주차의 방식 중, 어떤 것이 더 나은지 확인해보고 4주차를 진행할 것이다. 여전히 어렵다: 1주차는 그래도 초반에는 어느정도 알고 있던 지식들을 사용하는 미션이 나왔었다. 2주차는 정말 내가 알던 지식에서 많이 벗어난 것들도 많았다. 덕분에 학습 시간도 매우 길어졌지만 개발자가 왜 평생 배우는 직업인지 깨닫게 되었다. 프로그래밍은 진짜 다양한 방식과 방법이 있구나라고 생각하게 만드는 한 주였다. 결론: 아무말이나 하는 감자에서 말하는 방법을 배운 감자쯤으로 진화했다. 다음주는 말을 이해하고 내뱉을 줄 아는 감자로 진화해야겠다. "
    
}, 

{
    "id": 11,
    "url": "https://hoex5h.github.io//%EB%84%A4%EB%B6%80%EC%BA%A0_%EC%B1%8C%EB%A6%B0%EC%A7%80-_1%EC%A3%BC%EC%B0%A8_%ED%9A%8C%EA%B3%A0",
    "title": "[네부캠 챌린지] 1주차 회고",
    "body": "2025/01/14 - 1주차 회고네이버 부스트 캠프 1주차가 벌써 지나버렸다. 아쉬움이 많이 남는 1주차였지만 얻어가는 점도 많이 있어서 회고에 한 번 솔직하게 적어보려고 한다. 챌린지: 우선 나는 베이직 과정을 거쳐서 챌린지에 오게 되었는데, 베이직과는 차원이 다른 어려운 미션들이 나왔다. 첫 날에는 할만하다고 느꼈는데 이틀차부터는 무엇 하나 제대로 완성하는게 쉽지 않았다. 학교든 코딩테스트이든 구현이 먼저였던터라 챌린지도 구현에 상당히 많은 시간을 투자했고, 그 결과 구현이 설계를 따라가야하는데 나는 오히려 설계가 구현을 따라가는 느낌이었다. 1주차의 대부분에서 이렇게 시간을 소모했는데 이번 챌린지를 하지 않았다면 앞으로도 이런 과정으로 진행하지 않았을까? 싶다. 구현 먼저? 설계 먼저? CS 학습 먼저?: 학교에서 설계에 대해 자세히 배우거나 직접 해본 수업은 많지 않았던 것 같다. 그 중에서 제일 열심히 설계한 수업은 역시 졸업프로젝트인데 다이어그램이나 흐름도, 아키텍처 등을 자주 그리면서 진행을 했다. 그래서 설계를 좀 할 수 있지 않을까? 라고 생각했지만 열심히 그렸던 것들도 시간이 조금 지나니까 기억도 잘 안나고, 이번 챌린지 과정에서 설계를 다시 하려니 골치가 아팠다. 설계를 하면서 얼마나 깔끔하고 자세히 해야 하는가? 얼마나 설계를 해놔야 구현으로 넘아갈 수 있는가? 같은 여러가지 고민을 하게 되었고 그러다보니 이렇게 고민할 시간에 구현을 먼저 하자! 라는 이상한 결론에 도달했던 것 같다. 구현 먼저. . ?: 1주차는 거의 대부분 방향과 속력을 고려 하지 않은 구현을 먼저 진행했다. 그러다보니 겪는 여러가지 실수나 실패들이 생각보다 많이 있었다. 나는 여기서도 잘못된 선택을 내렸는데 이러한 실수나 실패도 내가 구현한 과정이고 남들이 겪었을지도 모르는, 혹은 궁금해 할 수 있는 공유할거리였는데 오로지 내가 구현에 성공한 과정만을 자세히 설명해야한다는 생각에 사로잡혀서 이런 과정도 제대로 설명하지 못했다. 결국 이번 1주차에서 나는 수 많은 실패를 반복하면서 아주 조금씩 구현을 했던 것인데, 구현한 시간만 따지면 하루에 12시간은 되는 것 같다. 그러고나서 되돌아보며 이번주에는 무엇을 배웠는가? 를 생각해봤더니. . 뭘 배웠는지조차 모르는 상태였다. 왜 이렇게 어려워?: 이번 주 금요일은 상당히 의미있는 미션이였다. 매일매일 새벽까지 코드를 짜면서 망가져버린 몸을 한 번 재정비하는 느낌도 있었고 한 주를 되돌아보는 느낌도 있었다. 그리고 있었던 여러가지 미션들과 세션들은 내가 이번 챌린지에서 무엇을 얻어가야 하는지를 조금이나마 깨우치게 해줬다. 챌린지의 미션은 정말 하루만에 끝낼 수 있을까? 싶을 정도의 난이도를 자랑한다. 그래서 미션을 처음 보면 드는 생각이 구현을 다 할 수 있을까 라는 생각이다. 그리고 드는 생각은, 시간이 부족할 지 모르니 빨리 구현을 시작해야겠다 였던 것이다. 그리고 이게 잘못되었다는걸 금요일에 깨달았다. 구현을 다 하는게 아니다. 이 미션을 통해서 배워야할 지식들을 먼저 습득하고 이를 토대로 설계한다. 그리고 설계를 기반으로 구현해보면서, 직접 부딪혀보면서 내 설계가 맞았는지, 학습했던 지식이 맞았는지 혹시 다른 방법이 있는지 등을 확인하는 것이다. 구현은 물론 중요하지만 100퍼센트 해야할 일이 아니다. (당연히 100퍼센트 하면 좋다. ) 결국 미션에 필요한 지식을 학습하고, 내가 가야할 방향을 잘 설계해서, 내가 할 수 있는 속력의 구현을 잘 생각해보고 진행해야 하는 것이다. 그리고 내가 버틸 수 있는 시간대가 몇 시 인지도 잘 구분해서 잘 땐 좀 자야한다. . 결론오늘 세션에는 특별한 사람들이 왔었다. 내년에는 내가 그 자리에 있고 싶다. 그러기 위해선 1주차에서 했던 잘못된 생각들을 반복해서는 안된다. 다른 사람들에게서 좋은 점은 배워야겠지만 그 사람들의 속도에 너무 신경쓰지말자. 좋은 것을 가져와서 내 속도를 높이는데만 신경써서 성장하자. "
    
}, 

{
    "id": 12,
    "url": "https://hoex5h.github.io//%EB%84%A4%EB%B6%80%EC%BA%A0_%EB%A9%A4%EB%B2%84%EC%8B%AD_%EC%8A%A4%ED%94%84%EB%A6%B0%ED%8A%B8-_1%EC%A3%BC%EC%B0%A8_~_7%EC%A3%BC%EC%B0%A8_%EC%A0%95%EB%A6%AC",
    "title": "[네부캠 멤버십 스프린트] 1주차 ~ 7주차 정리",
    "body": "2025/01/14 - 스프린트 1주차 학습정리(DOM, SPA, Flex, css…) HTML5 Layout Tag: Html5 레이아웃 태그에는 6가지 정도가 있다. header : 페이지 상단의 머릿말을 정의 nav : 네비게이션(메뉴), 사이트 내의 다른 페이지로 이동 aside : 카테고리, 현재 페이지 외의 컨텐츠 main : 가장 중요한 핵심 컨텐츠를 감싸는 태그, 한 페이지에 하나 사용, 강조의 느낌 section : 페이지의 주요 컨텐츠를 구분하는 태그 article : 독립적인 하나의 컨텐츠, 예시) 이름처럼 블로그 글이나 뉴스 기사 footer : 페이지 최하단의 부가 정보, 예시) 저작권, 연락처, 사이트맵 등등 DOM(Document Object Model): DOM 의 개념: DOM MDN DOM 은 HTML, XML 문서의 프로그래밍 interface 이다. 각 브라우저마다 이 인터페이스를 구현해서 저마다의 DOM 을 가지고 있다. 웹 페이지는 일종의 문서(document) 이다. DOM 은 이 문서를 트리 구조로 만들어서(구조화) 제공하며 이렇게 구조화된 정보를 바탕으로프로그래밍 언어가 동적으로 문서의 구조, 스타일, 내용 등을 변경할 수 있게 돕는다.  DOM 은 nodes 와 objects 로 문서를 표현한다. DOM tree 에서 모든 요소는 전부 `node` 로 표현된다. HTML 태그, 텍스트, 속성(attribute) 등은 전부 하나의 노드이다. =&gt; `node` 는 문서를 구조적으로 표현하는 단위이다. `object` 는 노드에 대한 실제 프로그래밍 인터페이스이다. 객체는 노드를 조작할 수 있는 인터페이스를 제공함으로써 프로그래밍 언어가 문서와 문서의 요소에 접근할 수 있도록 한다. =&gt; `object` 는 노드에 대한 접근을 제공한다. 만약 DOM 이 없다면 프로그래밍 언어(js 등)은 문서(웹 페이지 or XML 페이지)및 페이지의 요소들과 관련된 모델이나 개념들에 대한 정보를 얻을 수 없다. (문서에 접근할 수 없다) 초기에는 자바스크립트와 DOM 이 밀접하게 연결되어 있었으나 이제는 프로그래밍 언어와 독립적으로 디자인되었다. 따라서 어떠한 언어에서도 DOM 을 구현할 수 있다. DOM 의 핵심 인터페이스:  document. getElementById(id) document. getElementsByTagName(name) document. createElement(name) parentNode. appendChild(node) element. innerHTML element. style. left element. setAttribute element. getAttribute element. addEventListener window. content window. onload window. dump window. scrollToDOM 의 렌더링 과정: Virtual DOM 위의 영상을 참고해서 작성했습니다. image. png  HTML 을 파싱해서 DOM node 트리를 생성 스타일 정보를 추가하여 Render tree 생성 각 노드들의 배치할 위치를 결정(Layout) 각 요소에 색상을 입히기(Painting)DOM 조작: DOM을 조작할 때는 위에서 말한 렌더링 과정 전체를 다시 수행한다. 이때문에 아주 작은 변경이 있더라도 전제를 렌더링하기 때문에 상당히 비효율적이다. 렌더링 과정은 상당히 값비싼 과정이기 때문에, 매번 이 과정을 수행하는것은 변경 사항에 비해 너무 무거울 수 있다. 정적인 페이지는 DOM 에 대한 조작이 별로 없기 때문에 이런 DOM 조작 방식이 괜찮을 수 있다. 하지만 하나의 페이지로 필요한 데이터들을 바꿔가며 화면에 띄워주는 SPA(Single Page Application) 에는 이런 DOM 방식은 너무나도 비효율적일 것이다. SPA(Single Page Applcation): image. png 렌더링을 Client , Server 중 어디서 하는지에 따라 우리는 CSR(Client Side Rendering), SSR(Server Side Rendering) 이라고 부른다. SSR 에서는 서버가 페이지의 모든 부분을 렌더링해서 html를 클라이언트에게 넘겨주기 때문에, 클라이언트는 받은 html 을 reload 해서 렌더링한다. 반면 CSR 에서는 클라이언트가 직접 필요로 하는 부분의 데이터만 ajax 방식으로 서버에 요청해서, 받은 데이터 부분만 다시 렌더링하는 방식이다. Flexbox: Flexbox 의 기본 개념 Flexbox 의 개념:  flexbox 는 아이템간 공간 배분과 강력한 정렬 기능을 제공하기 위한 1차원 레이아웃 모델 1차원이라 칭하는 이유는, 레이아웃을 다룰 때 한 번에 하나의 차원만 다룬다는 뜻이다. 이는 행만 다루거나, 열만 다룬다는 의미다. 이후에 공부해야 할 Grid 레이아웃은 2차원 모델이다. 주축과 교차축: flexbox 에는 flex-direction 속성을 사용하여 주축 을 설정하고, 주축에 수직인 축이 교차축 으로 설정된다. flex 되는 아이템들은 주축을 기준으로 배치 되고, 교차축을 기준으로 정렬 된다. 주축:  row (가로, 아이템이 왼쪽에서 오른쪽으로 배치) row-reverse (가로, 아이템이 오른쪽에서 왼쪽으로 배치) column (세로, 아이템이 위에서 아래로 배치) column-reverse (세로, 아이템이 아래에서 위로 배치)교차축: 교차축은 주축에 수직이다.       따라서 주축이 가로(row   row-reverse) 라면 교차축은 세로이고,       주축이 세로(column   column-reverse) 라면 교차축은 가로이다.    Flex 컨테이너:  문서의 영역 중에서 flexbox가 놓여있는 영역을 flex 컨테이너라고 부릅니다. flex 컨테이너를 생성하려면 영역 내의 컨테이너 요소의 display 값을 flex 혹은 inline-flex로 지정합니다. 이 값이 지정된 컨테이너의 일차 자식(direct children) 요소가 flex 항목이 됩니다 여기서 일차 자식이란 부모의 바로 밑 자식을 의미한다. 할머니에게 있어서 손자는 일차 자식이 아닌 이차 자식이다. flex-wrap:  flex-box 는 1차원 모델이지만 flex 항목이 여러 행/열에 나열되도록 할 수 있습니다 flex-box 에 있는 flex 항목은 가로 혹은 세로로만 배치되는 1차원 모델이다. 다만 다음 가로줄 혹은 다음 세로줄에 배치되도록 해주는 속성이flex-wrap 이다. css. box { display: flex; flex-direction: row; flex-wrap: wrap;}image. png 아이템이 하나의 행에 들어가지 않을 정도로 크다면, 다음 행에 배치되는 모습을 볼 수 있다. css. box { width: 500px; height: 550px; display: flex; overflow: scroll; flex-direction: column; flex-wrap: wrap;}div { width: 50px; height: 150px;}image. png 아이템이 하나의 열에 들어가지 않을 정도로 크다면, 다음 열에 배치되는 모습을 볼 수 있다. 참고: PUG 문법 궁금증: html&lt;div id= header  role= banner &gt;&lt;/div&gt;&lt;div id= container  role= main &gt;&lt;/div&gt;&lt;div id= footer  role= contentinfo &gt;&lt;/div&gt;네이버 메인 화면의 layout 영역인데, 직접 &lt;header&gt; 를 쓰지않고 div 로 만들었다. 이건 무슨 차이가 있을까? =&gt; HTML5 이전의 접근성 개선을 위함 자바스크립트가 빠른 이유 : 바이트코드 + JIT 컹파일러넷스케이프의 스파이더몽키는 자바스크립트를 바로 인터프리터로 해석하기 때문에 속도가 느리다. 빨라져서 브라우저 바깥으로 탈출할 수 있게 되었다. nginx – 비동기스프링/아파치 -&gt; 100명을 위해 멀티프로세스 혹은 멀티스레드로 처리 -&gt; 메모리를 엄청 잡아 먹는다 nginx: 이벤트 중심 접근 방식, 하나 스레드로 여러 요청 처리apache: 프로세스 기반 접근 방식, 매 요청마다 스레드 생성 및 할당 nodejs 가 I/O 가 빨라지는건 아니다. 커널이 처리하는거라 빨라지는건 아니고 작업의 효율성이 증가하는 것 뿐이다. 노드 JS 공식문서 읽어보기 반드시 쉘 스크립트echo “hello”; echo “hell”echo “hello” &amp;&amp; echo “hell” -&gt; 앞에 리턴값이 0(성공)이면 뒤에 실행|| 는 0이 아닐때 (실패) 뒤에 실행 echo $?-&gt; 0 top -&gt; htop -&gt; bpytop 동기/블로킹과 비동기/논블로킹 공부 이그니션(인터프리터) -&gt; 바이트코드 생성터보팬(컴파일러) -&gt; 최적화된 기계어 코드 생성 최적화? optimizer 때문에 내가 원하는대로 작성한대로 코드가 돌아가지 않을 수 있음. JIT 컴파일러 -&gt; 자바에서 hotspot vm, 크롬 v8 에서 사용중언제 나왔을까? 70년대에 나온 기술, oop의 원조는 smalltalk (GUI 프로그래밍을 하려고 제록스 연구소에 만들었다)스몰톡도 인터프리터 언어인데, 성능을 높이려고 자주 사용하는 결과를 해싱해 놓으면 빠를 것 같아서 인터프리터 한 결과를 재사용 왜 바이트코드와 기계어 코드로 나눠지는 이유는 뭘까? 즉시 실행할때, 는 한 줄 읽고 해석하는 인터프리터가 더 빠르다 자주 사용되는, 재사용되는 코드가 있을 때 터보팬(컴파일) 사용 LibUV 가 node js 의 핵심epoll, kqueue, etc -&gt; 운영체제에서 i/o 비동기 처리를 위해서 커널에서 제공해주는 기능 윈도우는 iocp , 리눅스는 epoll, bsd 계열(mac) kqueue이런것을 I/o 멀티 플렉싱을 제공해준다고 한다. 스레드의 아이오를 담당하는 객체, 소켓을 연다 리슨 소켓말고 실제로 처리하는 소켓을 만든다. -&gt; 결국 쓰레드가 생기는거라 비효율적여러 아이오를 하나의 쓰레드에서 처리할 수 있게 하는게 I/O 멀티플렉싱 https://oliveyoung. tech/blog/2023-10-02/c10-problem/ busy waiting -&gt; 이벤트가 발생할때까지 무한루프 돌면서 계속 체크 (epoll 이 나오기 전, Select 방식) -&gt; 비효율적비동기를 위해서 cpu 자원을 사용 epoll 는 리눅스에서만 써서 윤영체제 비 표준이다. 이벤트루프가 운영체제의 도움을 받는데, 운영체제마다 다르게 동작하게 된다. (윈도우는 iocp , 리눅스는 epoll, bsd 계열(mac) kqueue ) 디버깅 툴 잘 써야한다. 디버거 써야한다 개발10 디버깅90 브레이크포인트 걸고, 컨디션 잡고 … 백엔드 개발자는 개발하는 입장에서, 죽지 않는 서버가 가장 중요하다. 그리고 빨리 살리는것, 예외상황 처리 잘 하는 것-&gt; 탄탄한 cs 지식이 필요운영체제, 데이터베이스, 네트워크 worker_threads를 통해 thread를 만들 때는 Thread마다 별도의 메모리 공간이 부여되는데 thread pool에서 꺼내올 때도 Worker thread마다 별도의 메모리 공간이 있나요??보통의 워커 쓰레드는 대부분 POSIX 쓰레드를 쓸건데, 별도의 메모리(커널의 지원을 통해) 를 가질 수 밖에 없다. 자바 쓰레드는 네이티브 쓰레드일까? 유저레벨 쓰레드일까?자바는 커널 레벨 쓰레드이다. 자바에서 쓰레드를 하나 만들어내면 운영체제에서 쓰레드를 만들어서 매핑한다. m:n 은 다 거짓말이다. webflux 의 쓰레드는 컨텍스트 스위칭이 일어나서 성능이 별로 안좋았음. 커널 쓰레드의 오버헤드를 개선하고 싶은데? 유저 레벨 쓰레드를 만들자 -&gt; 자바에 버츄얼 쓰레드가 생김문법은 똑같은데, 버츄얼 유저레벨 쓰레드가 생성되어서 성능이 대폭 향상된다. mysql 은 X. 1. 10 부터 써라 Project Setting: Global: global 옵션은 devDependencies: devDependencies 는 개발할 때만 필요하고 실제 런타임에서는 필요 없는 dependency 를 의미한다. (개발용) TS, ESLint, Prettier: 코드 품질과 괸련된 것들을 확인하는 도구를 린터(Linter) 라고 부르는데, 자바스크립트에서 사용하는 대표적인 린터가 ESLint 이다. 문법 오류를 감지하거나, 코드 품질 향상, 일관성 유지 등을 위해 적용하는데 나만의 컨벤션을 정립하기 전까지는 airbnb 스타일을 따라가보려고 한다. shellnpm install -D eslint typescript## airbnb 종속 패키지 설치하기## 이렇게 종속 패키지를 한 번에 설치하면 react 관련 eslint 패키지도 설치가 되는데,## react 를 안쓴다면 아래의 명령어로 설치하자. npx install-peerdeps --dev eslint-config-airbnb## React 관련 패키지 없음npm install -D eslint @typescript-eslint/parser @typescript-eslint/eslint-plugin eslint-config-airbnb-base eslint-plugin-import eslint-plugin-node이후에 . eslintrc. json 파일로 셋팅을 하자. json{  parser :  @typescript-eslint/parser ,  extends : [ airbnb-base ,  plugin:@typescript-eslint/recommended ,  plugin:node/recommended ],  plugins : [ @typescript-eslint ,  node ],  parserOptions : {   ecmaVersion : 6,   sourceType :  module  },  rules : {   node/no-unsupported-features/es-syntax : [ error , {  ignores : [ modules ] }],   node/no-missing-import :  off ,   node/no-unpublished-import :  off ,   import/no-unresolved :  off ,   import/extensions : [    error ,    ignorePackages ,   {     js :  never ,     jsx :  never ,     ts :  never ,     tsx :  never    }  ] },  settings : {   import/resolver : {    node : {     extensions : [ . js ,  . jsx ,  . ts ,  . tsx ]   }  } }}shellnpm install -D @types/expressPrettier: Prettier 는 코드 형식을 자동으로 정리해주는 Formatter 의 역할을 한다. prettier 셋팅에 맞게 자동으로 정리해주기 때문에 코드의 가독성과 일관성을 챙길 수 있다. textts-node: shellnpm install -D ts-node스프린트 2주차 학습정리(MySQL, Flux)MYSQL:  참고자료     MYSQL 아키텍처   MYSQL 서버는 크게 MYSQL 엔진 과 스토리지 엔진 으로 나뉘게 된다. 스토리지 엔진은 과거에는 여러가지 있었으나 Mysql 8. 0 버전 이후로는 InnoDB 가 기본값이 되었다고 한다. 사용자가 mysql 서버에 연결 요청을 보내면, 연결 핸들러가 이를 처리하고 세션이 생성된다. 이후에 sql 쿼리를 날리면, 크게 아래의 5단계 과정을 거쳐 정보를 가져온다. MySQL 엔진:  쿼리 파서     쿼리를 토큰으로 잘게 쪼개어 트리의 형태로 파싱하는데, 쿼리에 문법적 오류가 있는지 검사를 한다.     전처리기     전처리기는 예약어를 제외한 토큰을 검사해서 데이터베이스에 실제로 데이터가 존재하는지, 객체에 사용자가 접근할 수 있는지(권한 등) 검증    옵티마이저     넘겨받은 파스 트리를 실행 계획으로 바꾸는 역할   쿼리 재작성, 테이블 스캔 순서 결정, 사용할 인덱스 선택 등 최적의 실행 계획을 작성한다.    실행 계획은 아래와 같이 바뀌는데 id 가 같은 실행 계획이 있다면 JOIN 된다는 걸 의미한다.    최적화 하는 방법은 비용 기반 최적화와 규칙 기반 최적화가 있는데, Mysql 에 존재하는 다양한 통계 정보를 활용해서 비용을 최대한 줄이는 방향의 최적화이고 규칙 기반 최적화는 동일한 sql 이면 동일한 실행 계획을 만드는 방향의 최적화이다. (순수함수가 떠오르게 하네)   대부분의 RDBMS 는 비용기반 최적화이다.    idselect_typetabletypekeyrowsextra1SIMPLEarticleindexPRIMARY5Using where1SIMPLEmemberseq_refPRIMARY1EMPTY    실행 엔진     이렇게 전달된 실행 계획을 토대로 스토리지 엔진과 통신을 해서 데이터를 읽어온다.     스토리지 엔진(InnoDB)     실행 엔진의 요청을 실제 디스크에서 처리하면서 데이터 읽기/쓰기 를 수행한다.    스토리지 엔진은 MySQL 엔진과 다르게 여러 개를 동시에 사용할 수 있다.    그렇다면 InnoDB 는 어떤 구조를 가질까? 스토리지 엔진: 버퍼 풀: 버퍼 풀은 메인 메모리의 한 영역으로 테이블과 인덱스 데이터를 캐싱 해두는 공간이다. 말 그대로 디스크보다 읽기 쓰기가 압도적으로 빠른 메인 메모리에 데이터를 캐싱해두고 디스크 읽기/쓰기 전에 버퍼 풀을 먼저 확인하는 것이다. 버퍼 풀의 데이터의 저장 단위는 여러 행을 저장할 수 있는 Page 단위인데, 그 이유는 디스크의 데이터 저장 단위가 페이지이기 때문이다. 버퍼 풀에서 페이지들은 링크드 리스트로 관리가 되는데, 이때 페이지들은 LRU(Least Recently Used) 에서 살짝 바뀐 알고리즘읉 통해 관리가 된다. 내가 알던 LRU 와 살짝 다른데, 어떤 부분이 다른지 한 번 살펴보자. LRU 알고리즘과 버퍼 풀: 17. 5. 1 Buffer Pool (해당 mysql 공식 문서를 참조해서 작성했다) 버퍼 풀에서 페이지들은 얼마나 자주 사용되었느냐에 따라 New Subset, Old Subset. 로 나뉘고 New Subset 과 Old Subset 이 맞닿아있는 포인트를 Middle Point 라고 한다. 원래 LRU 는 단순히 가장 오랫동안 사용하지 않은 페이지(tail 에 있는 페이지)를 제거하고, 가장 최근에 사용한 페이지를 head 에 넣는 알고리즘이다. image. png 하지만 InnoDB 의 LRU 는 살짝 다른데, 위의 그림에서 보면 버퍼 풀의 $\frac{5}{8}$ 을 New Sublist, $\frac{3}{8}$ 을 Olb Sublist 로 할당하고 New Sublist 의 tail 과 Old Sublist 의 Head 가 맞닿은 곳이 바로 Middle Point 이다. 이제 사용자가 특정 페이지를 읽게 되면 Old Sublist 의 Head 로 해당 페이지가 들어가고 Old Sublist 의 tail 에 해당하는 페이지는 삭제된다. 이 시점에서 왜 New Sublist 의 Head 에 들어가는게 아니라, MidPoint 로 들어가는지 의문일 수 있다.  InnoDB 는 “처음 버퍼 풀에 등록된 페이지가 가장 최근의(중요한) 페이지라고 생각하기보다는, 첫 등록에는 아직까지 그닥 중요한 페이지가 아니다. ” 라고 간주한다 페이지는 유저의 요청에 버퍼 풀에 올라올 수도 있고 아니면 InnoDB가 예측(read-ahead)해서 미리 페이지를 preload 하는 경우에도 버퍼 풀에 올라올 수 있다. (사용자가 다음엔 이걸 읽겠지? 하고 미리 로드) 사용자의 요청에 의해 Old Sublist 에 있는 페이지를 읽게되면 해당 페이지를 young 하게 만든다. 즉, 해당 페이지를 참조해서 유저에게 데이터를 주고, 바로New Sublist 의 head 로 옮겨버린다. 그러나 InnoDB 의 read-ahead 에 의해 버퍼 풀에 올라온 페이지는 말 그대로 예측이기 때문에 유저가 참조하지 않을 수도 있다. 따라서 당연하게도 read-ahead 로 올라온 페이지는 유저에 의해 참조될 수도, 아니면 그대로 계속 순위기 밀려 제거될 수도 있다. 새로운 페이지가 등장하면 Old Sublist 들의 페이지는 전부 노화(사용 순위가 밀림)한다. 만약 Old Sublist 에 존재하는 페이지가 참조되면서 Sublist 의 Head 로 올라가게 되면, 이 페이지를 제외한 나머지 페이지들도 모두 노화한다. 그렇게 노화하다가 Old Sublist 의 tail 에 도달한 페이지는 제거된다. MYSQL 셋팅과 문제 해결: 기존에 사용하던 ubuntu 24. 04 가상머신을 그대로 활용했다. M1 맥 위에 설치한 가상머신이라서 aarch64 아키텍처이다. MySQL 8. 0. XX 버전 설치하기: 우선 APT 저장소에서 다운받을 수 있는 버전을 확인하고 가장 최신 버전으로 다운 받았다. shellapt-cache policy mysql-server# mysql-server:#  Installed: 8. 0. 39-0ubuntu0. 24. 04. 2#  Candidate: 8. 0. 39-0ubuntu0. 24. 04. 2#  Version table:# *** 8. 0. 39-0ubuntu0. 24. 04. 2 500#     500 &lt;http://ports. ubuntu. com/ubuntu-ports&gt; noble-updates/main arm64 Packages#     100 /var/lib/dpkg/status#   8. 0. 39-0ubuntu0. 24. 04. 1 500#     500 &lt;http://ports. ubuntu. com/ubuntu-ports&gt; noble-security/main arm64 Packages#   8. 0. 36-2ubuntu3 500#     500 &lt;http://ports. ubuntu. com/ubuntu-ports&gt; noble/main arm64 Packagesapt install mysql-server=8. 0. 39-0ubuntu0. 24. 04. 2linux-installation post installation 위의 두 mysql 문서를 참조했다.  For other installation methods, you must initialize the data directory manually. These include installation from generic binary and source distributions on Unix and Unix-like systems,and installation from a ZIP Archive package on Windows. 만약 바이너리 파일이나 소스 파일 / 윈도우는 ZIP 으로 다운받았으면 Data Directory 설정을 따로 해주어야 한다고 한다. MySQL 셋팅:  mysqldmysqld 는 MySQL Server 로, 싱글 멀티스레드 프로그램 이다. 추가적인 프로세스를 만들지 않는 멀티스레드 프로그램이라는 의미이다. 아마 다른 프로세스의 작명도 그러하듯이, mysqld 는 mysql daemon 이라는 의미일것이다. daemon 의 의미는 백그라운드에서 실행되는 프로세스를 의미한다. mysqld(MySQL Server)는 데이터베이스와 테이블을 관리하는 MySQL data directory 에 접근하는 것을 담당한다. MySQL data directory 는 로그나 status file 같은 정보들이 저장되는 default location 이기도 하다. mysqld(MySQL Server)는 클라이언트 프로그램으로부터의 네트워크 커넥션을 listen 하고 있다가 클라이언트를 대신해서 데이터베이스에 접근한다. mysqld 는 운영체제에 따라 이름이 mysql 인 경우도 있다.  mysqld_safe mysqld_safe is the recommended way to start a mysqld server on Unix. mysqld_safe adds some safety features such as restarting the server when an error occurs and logging runtime information to an error log. A description of error logging is given later in this section. mysqld_safe 를 통해서 mysqld server 를 실행할 수 있지만 ubuntu 24. 04 버전에서는 systemd 에 의해 mysqld 가 관리되므로 굳이 사용할 필요가 없다. 어떤 역할을 해주는지만 알고 넘어가면 될 것 같다.  ‘systemd 로 mysqld managing’Managing MySQL Server with systemd 사실 apt 로 설치하면서 굳이 건들지 않아도 잘 실행되긴 한다. 그래도 추가적인 설정에 대해 궁금하면 읽어봐야 할 것 같다. ubuntu 24. 04 기준으로 systemctl 은 mysqld 실행 파일에 대한 서비스 이름을 ‘mysql’ 로 지정한다고 한다. 따라서 mysqld 가 잘 실행되었는지는 systemctl status mysql 으로 확인할 수 있고 실행시키려면 systemctl start mysql 로 할 수 있다.  초기 MySQL 계정 셋팅Securing the Initial MySQL Account 초기 root 계정은 비밀번호를 가지고 있을 수도 있고, 아닐 수도 있다. 나의 경우에는 비밀번호가 없었는데 비밀번호가 있다면 위의 문서를 참고하면 될 것 같다. 비밀번호가 없을 경우에는 root 계정으로 들어가서 비밀번호를 설정하라고 한다. shell# mysql 로그인 전이므로 로그인해준다. $&gt; mysql -u root --skip-password# mysql 로그인 후, mysql 프롬프트 내부mysql&gt; ALTER USER 'root'@'localhost' IDENTIFIED BY 'password-you-want';# 이후 로그인 하려면$&gt; mysql -u root -pEnter password: (enter root password here)가상머신에 mysql 을 설치하고, 외부에서 이 mysql 에 접근하는 방법이 tcp/ip 를 기준으로 2가지인데 하나는 직접 host:port 로 접근하는 것이고 두 번째로는 ssh 를 통해서 해당 host 에 접속해서 mysql 에 접근하는 것이다. ssh 로 접근하게 되면 ssh 연결 자체가 암호화되기 때문에 안전하다는 장점이 있으나 데이터 전송 성능이 떨어진다. 직접 tcp 로 접근하게 되면 보안은 좀 떨어지지만 속도가 빠르다. 또한 직접 tcp로 접근하게 되면 mysql 에서 IP 를 허용해주어야 한다. 우선 hoeh 라는 새로운 계정을 만들어서 권한과 허용되는 IP대역을 부여해보자. shell# root 계정으로 로그인mysql -u root -pmysql&gt; use mysql# hoeh@'모든 ip 대역' =&gt; 'hoeh'@'%'mysql&gt; CREATE USER 'hoeh'@'%' identified by 'password-you-want';# Query OK, 0 rows affected(0. 00sec)# 모든 권한 부여mysql&gt; GRANT ALL PRIVILEGES ON *. * to 'hoeh'@'%';# 권한 새로 고침mysql&gt; flush privileges타입 스크립트, MySQL2 Import 문제 해결: image. png import mysql from 'mysql2/promise 를 입력하니까 위 사진과 같은 오류가 발생했다. 분명 npm 으로 mysql2 도 설치를 했고 node_module 파일과 package. json 에도 mysql2 가 있는 것을 확인했는데도 해당 오류가 사라지지 않았다. 심지어 일단 실행을 시켜보니까 정상적으로 동작하기는 했다. 그래서 일단 어떤 오류인지 찾아보았다. 아키텍처: [10분 테코톡] 우디의 Flux Architecture 프론트엔드에서 MVC보다 더 많이 쓰이는 패턴은 ? MVC 의 단점: MVC 패턴을 사용하게 되면 프로젝트의 규모가 커짐에 따라 복잡성이 너무 커지게 되었는데,여러 컨트롤러들이 여러 개의 뷰를 참조하는 등의 거미줄 같이 얽히게 되기 때문이다. 게다가 프론트엔드에 MVC 패턴을 그대로 가져다 쓰기는 살짝 애매했는데보통 MVC 에서 View 는 그냥 만들어지는 결과물에 불과하다. 컨트롤러가 입력을 받고, 모델에서 데이터를 가져와서 만드는 그런 결과에 불과하다. 그런데 프론트엔드에서의 view 는 단순한 결과물이라고 하기엔 조금 다르다. 프론트엔드의 view 는 수 많은 이벤트들이 발생을 하는, 뷰가 마치 컨트롤러처림 역할을 수행해야 하는 일이 발생한다. -&gt; 서버에서 html 을 전부 렌더링해서 주는 상황이라면 조금 다를 수 있을 것 같기도하고. . -&gt; 이 경우에서는 서버에서 MVC 를 쓰기에 아주 적절해보이긴 한다. 게다가 프론트엔드에는 뷰가 매우 다양할 수 있고 모델도 마찬가지로 매우 많을 수 있다. 따라서 뷰와 모델간 복잡도가 매우 올라간다. 이를 해결하기 위해 MVC 패턴의 컨트롤러를 돌고오게 되면 컨트롤러에는 수 많은 비즈니스 로직이 들어가게 되고 컨트롤러의 복잡성이 너무 커지게 된다. 프론트엔드에서의 View 는 계층적인 구조를 가지는 것이 필요하다. 그도 그럴 것이 DOM 조작을 통해 View 를 계속 수정해나갈텐데 DOM 자체가 트리, 즉 계층 구조를 가지고 있다. 사용자의 요청이나 서버의 요청 혹은 SetTimeout 같은 타이머 등의 요청으로 View 는 재랜더링 되는 경우가 굉장히 많고 이렇게 View 를 제어하기 위해서는 계층 구조(DOM 구조) 를 활용하면서 재랜더링 하는 View 를 최소화해야한다. -&gt; 서버 사이드의 문제점?  렌더링, 즉 DOM 을 조작하는 것은 프론트엔드에 있어서 가장 비싼 비용, 가장 느린 작업이다. image. png image. png 과연 이러한 문제들을 MVC 가 잘 해결 할 수 있을까? 이 문제를 해결하기 위해 MVC 를 억지로 끼워넣을 필요는 없다. 그래서 프론트엔드에서 실제로 자주 사용되는 기술들은  데이터 바인딩 MVVM Flux정도라고 한다. 데이터 바인딩: 데이터 바인딩은 상태 값과 그 값을 제어하는 요소와의 결합을 의미한다. 대표적으로 svelte 프레임워크에서 사용하는듯 한데, 양방향 바인딩을 이용해서 동시에 값이 변경되게 처리되도록 할 수도 있다. Model 의 값이 변경되면 View 를 재랜더링 할 수도 있고 (단방향) 여기에 더해서 양방향 바인딩을 하면 View 요소의 변경을 감지해서 Model 을 업데이트 할 수도 있다. svelte 에서는 내부적으로 eventListener 를 생성해서 양방향 바인딩(two-way data binding)을 한다고 한다. MVVM: 위에서 말한 bind 를 만들어주는 역할을 하는것이 MVVM 에서는 VM 이다. VM은 뷰에서 쓰는 모델을 가지고 있다가, 뷰에서 변경사항이 생기면 뷰모델이 바뀌고, 뷰모델이 바뀌면 뷰를 재랜더링하는 방식이다. image. png Flux:  단방향 데이터 흐름을 통해 보다 예측가능하게 상태를 관리할 수 있는 클라이언트 사이드 웹 어플리케이션 아키텍쳐 Flux 는 기본적으로 한 방향으로 흐름을 진행하려고 한다. 계층적인 구조를 가지는 뷰가, 모델과 서로 지속적으로 호출을 하면서 의존성과 복잡성이 올라가게 되는데 이 흐름을 쉽게 만들기 위해서 한 방향으로 만들려는 시도이다. image. png Action: 맨 왼쪽의 Action 은 사용자가 view 와 상호작용하면서 일어날 수도 있고, 타이머함수에 의해서 생길 수도 있고 서버에서 데이터를 받을 수도 있다. Dispatcher: Action 은 그저 액션에 대한 정보를 담고 있는(ex. 액션 이름, 내용 등) JSON 같은 객체일 것이다. 따라서 정보를 가지고, 상태 저장소(store) 에 적절히 잘 전달해줄 객체인 Dispatcher 가 필요하다. Store: Store 는 어플리케이션의 상태가 저장되어 있는 전역 상태 저장소로 Dispatcher 에 의해 넘어온 액션을 통해서’만’ 값이 수정된다. Store 는 스스로 상태를 변경하는 로직을 가지고 있기 때문에 Action 이나, Dispatcher 에 의해 변경 로직이 실행되는 것이 아니라 스스로 가지고 있는 변경 로직을 실행하는 것이다. Store 는 어떠한 상태라도 저장할 수 있는 전역 상태 저장소로, 상태가 변경되면 View 에게 통지하게 된다. (event Emit) View: View 는 관심 있는(구독 하고 있는) ‘어플리케이션의 상태’ 가 변경될 때마다 재렌더링 되는 컴포넌트다. View 또한 재랜더링 하는 로직을 스스로 가지고 있고 store 가 변경됨에 따라 하위 컴포넌트에게 변경된 상태를 전달하기도 하므로, MVC 패턴 관점으로는 controller 의 역할도 겸한다고 볼 수 있다. 그래서 Controller-View 라고 불리기도 한다. View 에서 발생한 Action 은 다시 Dispatcher 에게 전달된다. 프론트엔드에는 규모가 커지면서 복잡한 View 들도 많이 등장했는데 이러한 문제점을 해결하고자 MVVM 패턴이 등장하게 되었다. Flux 아키텍쳐: 왜 Flux 를 선택했나: 클라이언트 사이드 패턴을 공부해보면서, MVC 패턴을 그대로 사용하기 보다는 flux 패턴을 사용해보고 싶어서 학습 위주로 진행을 했습니다. 우선 MVC 패턴을 그대로 가져다 쓰기는 힘들다는 이야기를 듣고 조금 더 찾아보았습니다. MVC 패턴은 검색해보면 나오는 이미지가 다 다를 정도로 마치 코에 걸면 코걸이, 귀에 걸면 귀걸이인 것 처럼 MVC 패턴을 쓰기 위해 코드를 욱여넣는 것이 올바른 방법이 맞나? 라는 생각이 들었습니다. 그래서 MVVM, flux, 데이터 바인딩 등의 다양한 아키텍처를 찾아보고 어떤 방식이 좋을지를 고민해보는게 이번 주 학습의 첫 시작이였습니다. 데이터 흐름을 단방향으로 가져가는 것이 일반적이다. 라는 이야기를 듣기도 했고, 이번 주 스터디그룹에서 이야기가 나왔던 flux 패턴이 가장 흥미로워보였습니다. 그런데 제가 React 나 Redux 등 프론트엔드에 정말 무지해서 학습하는데 시간이 오래 걸릴 것 같아 이걸 학습하고 적용시켜보는게 맞는지 고민이 되었지만, 네부캠이 아니면 이런 기회가 없다고 생각해서 도전해보게 되었습니다. 여러가지 flux 자료들을 찾으면서 가장 많이 활용한 자료는 facebook-flux 이었는데 flux 자체가 페이스북에서 먼저 제안한 방식이기 때문에 해당 레포지토리의 소스를 한 번 읽어봐야겠다 라는 생각으로 읽어보았습니다. 다만 제가 참고한 facebook-flux 의 아이디어가 실제로 나온지는 9년이 넘은 것 같고 위의 레포는 아카이브에 들어가있는, 지금은 업데이트가 되지 않는 레포입니다. 아마 더 최신의 버전이 react 나 redux 등에 있지 않을까 싶습니다. 그래서 앞으로 설명드릴 내용은 위의 facebook-flux 레포를 기준이라는 것을 미리 말씀드리고 싶습니다. 또한 프론트 문외한이 이해한 내용이라 틀린 내용이 있을 수 있습니다 참고 자료  facebook-flux-source facebook-flux-docsFlux: image. png flux 패턴의 핵심은 위의 그림에서와 같이 단방향의 데이터 흐름 입니다. 위의 그림에서 가장 눈에 띄는 요소는 Dispatcher , Store, View 입니다. 데이터의 흐름을 살펴보면 화살표가 시계방향으로만 이어져있을 뿐, 흐름이 역행하거나 단계를 건너뛰고 진행되는 일이 없습니다. 이러한 단순한 흐름 덕분에 코드의 흐름 자체를 이해하기가 쉬워지고 가독성을 얻을 수 있습니다. 데이터 흐름: 그렇다면 어떻게 이런 단방향의 흐름을 설계했을까요? flux 에서는 모든 Action(사용자의 인터랙션, 타이머 함수의 실행, 서버로부터 데이터 받기 등)은 Dispatcher 라는 마치 중앙집중허브에 전달됩니다. Dispatcher 는 이런 Action 들을 모든 Store 에게 전달을 해주고, Store 는 Action 을 기준으로 자신의 상태값을 갱신합니다. 그리고 갱신이 완료되면 ‘change’ 라는 이벤트를 emit 함으로써 View 가 새로운 상태값을 가지도록 만듭니다. 여기서 단방향의 흐름을 위해 Store 는 callback 함수를 Dispatcher 에 등록을 하게 되는데, callback 함수에는 Store 자신의 메소드를 적절히 잘 활용해서 상태값을 바꾸는 등의 역할을 수행합니다. 또한 Store 는 eventEmitter 를 활용해서 상태값이 갱신되면 ‘change’ 라는 이벤트를 emit 합니다. 이 emit 을 통해 View 는 상태값이 바뀌었음을 알게 되고 Store 에게 새로운 상태값을 받아와 재렌더링을 합니다. Dispatcher: 페이스북은 여기서 Dispatcher 의 역할이 pub-sub 구조와는 조금 다르다고 이야기 합니다.  Dispatcher는 등록된 callback에 데이터를 중계할 때 사용된다. 일반적인 pub-sub 시스템과는 다음 두 항목이 다르다: - 콜백은 이벤트를 개별적으로 구독하지 않는다. 모든 데이터 변동은 등록된 모든 콜백에 전달된다. - 콜백이 실행될 때 콜백의 전체나 일부를 중단할 수 있다. 첫 번째로 Store 의 콜백은 이벤트를 개별적으로 구독하지 않습니다. 즉 Dispatcher 에 들어오는 Action은 특정 Store 에게만 전달되는 것이 아니라 모든 Store 에게 전달됩니다. Store 는 Action 의 type 을 기준으로 자신이 필요로하는 Action 인지를 구별하여 작업을 진행합니다. 이러한 과정은 오버헤드를 불러올 것이라고 생각되는데, 그럼에도 이러한 방식을 선택한 이유를 생각해보자면 흐름과 로직을 단순화 할 수 있다는 것이라고 생각합니다. 두 번쨰로는 콜백이 실행될 때 다른 콜백들을 중단(지연)시킬 수 있다는 것입니다. 이는 콜백 간의 순서를 보장할 수 있다는 이야기인데 facebook 에서 들었던 예시로 살펴보겠습니다. javascriptconst flightDispatcher = new Dispatcher();// 어떤 국가를 선택했는지 계속 추적한다const CountryStore = {country: null};// 어느 도시를 선택했는지 계속 추적한다const CityStore = {city: null};// 선택된 도시의 기본 항공료를 계속 추적한다const FlightPriceStore = {price: null};위의 예시는 항공편 예약 서비스로, 나라를 선택하고 해당 나라의 도시를 선택하고, 그 도시의 기본 항공료를 추적하는 시스템이라고 보면 될 것 같습니다. 사용자가 새로운 나라와 새로운 도시를 선택했다면 ‘새로운 나라 선택 callback’ 이후에 ‘새로운 도시 선택 callback’ 이 진행되어야 하는, 일련의 순서가 존재하게 됩니다. 따라서 flux 에는 waitFor 이라는 메소드로 이를 구현했습니다. 다만 이번 TodoApp 에 있어서 waitFor 이 필요하지는 않다고 생각해서 실제로 미션을 위한 flux 를 설계할때는 waitFor 을 고려하지 않았습니다. Store: Store 는 도메인별로 상태를 관리합니다. Store 는 자신의 callback 을 dispatcher 에 등록합니다. callback 은 action 을 파라미터로 받아서, switch 문을 활용하여 action 의 type 을 기준으로 여러 작업을 수행합니다. facebook 에서는 이 switch 문을 포함하고 있는 함수를 reduce 라고 이름 지은 것 같습니다. 따라서 저도 Store 라는 클래스에 reduce 라는 추상 메소드를 선언하고 CardStore, ColumnStore 등이 이 reduce 함수를 구현하도록 진행했습니다. javascriptexport default abstract class Store&lt;TState&gt; extends EventEmitter { private readonly dispatcher = dispatcher; protected state: TState; protected constructor(state: TState) {  super();  this. state = state;  // this. setState(state); } . . .  abstract reduce(action: Action): void;}Store 는 아래와 같은 특징이 있습니다.  Cache data (데이터를 캐싱해둔다) Expose public getters to access data (never have public setters) (데이터에 접근하기 위한 Getter 를 노출시키지만, Setter 는 절대로 있어선 안된다) Respond to specific actions from the dispatcher (디스패처로부터 넘어온 특정 액션(관심있는 액션)에 응답한다) Always emit a change when their data changes (데이터의 변화가 있을 때 항상 ‘change’ 이벤트를 emit 한다) Only emit changes during a dispatch (디스패치 중에만 ‘change’ 이벤트를 emit 해야한다. )이중에서 마지막 Only emit changes during a dispatch 가 살짝 의아할 수 있습니다. dispatch 중이라는 것은, 디스패쳐로부터 데이터가 넘어왔을 때를 의미합니다. 그때 적절한 함수를 통해서 데이터를 변경하고 ‘change’ 이벤트를 emit 할텐데 이 과정 말고 다른 추가적으로 ‘change’ 이벤트를 emit 하는 일이 없어야한다는 의미입니다. 이는 데이터 흐름의 일관성과 예측 가능성을 유지하고 비동기 작업이나 외부 이벤트로 인한 예기치 않은 상태 변경을 방지하기 위함입니다. View: 마지막으로 가장 고민이 되는 View 인데 facebook flux 에서는 React 를 활용하여 View 를 만들고 재렌더링 하는 방법을 사용하고 있었습니다. 저는 react 를 잘 모르기도 하고 이번 미션에서 사용할 것도 아니였기 때문에 내부 동작을 비슷하게 가져와서 구현을 할 생각입니다. facebook-flux 에서도 Store 가 eventEmitter 를 활용하여 이벤트를 emit 하는것으로 보여서 비슷한 방식으로 View 에게 이벤트를 전달할 생각입니다. 별거 없는 코드지만 mini-flux 에 flux 동작을 조금 따라해보는 코드를 올렸습니다. 스프린트 3주차 학습정리 (Merge, Squash Merge, 타입스크립트 유틸리티 타입)Merge 전략: 이번에 프로젝트를 진행하면서, upstream 레포지토리를 fork 해와서 새로운 Feature 브랜치를 생성해서 작업을 했다. merge 가 되는 순서를 간략히 설명하면, Feature(기능 개발 브랜치) -&gt; downstream 개인 브랜치 -&gt; upstream 개인 브랜치 순서로 병합이 된다. 여기서 downstream 개인 브랜치 -&gt; upstream 개인 브랜치 과정의 병합은 내가 하는 것이 아니라, 관리자 혹은 Github Action 봇이 하게 된다. 처음에는 아무 생각 없이 PR 을 올려두고 병합이 되기 전까지 계속 Feature 브랜치에서 작업을 했다. 그러고 나서 PR이 병합될 때문제없이 잘 병합되길래 앞으로도 문제가 생기지 않을 줄 알았다. 그러다가 여느 날과 마친가지로 PR 을 올려두고 Feature 브랜치에서 계속 작업을 하다가 병합이 된 후에, 추가로 작업한 내용을 upstream 개인 브랜치로 올리려고 했더니충돌이 발생했다. 분명 이전과 별 다를 것 없이 머지되었다고 생각했는데 왜 이번에는 충돌이 나는걸까? 에 대해 궁금증이 생겼고, 같은 팀 동료분께서 해답을 찾아 주셨다. 깃의 merge 에 대해서는 git merge 와 git rebase 에 대해서 이전에 작성을 했었는데, 여기서 작성하지 않았던 merge 가 하나 있다. 깃의 Merge 에는 일반 Merge, rebase, 그리고 squash merge 가 있다. 위의 학습 정리에서는 Merge 와 rebase 의 차이점을 중심으로 살펴봤었는데, 이번에는 일반 Merge 와 Squash merge 에 대해 살펴보려고 한다. Merge 와 Squash Merge: Merge 는 기본적으로 Commit 이 몇 개가 쌓여있든 간에 그대로 병합시킨다. 다시 말해서 누적되어 있는 commit 이 그대로 들어와서 병합된다. 그렇기 때문에 커밋의 숫자가 많은 경우에는 커밋을 하나씩 읽고 흐름을 이해하는데 다소 힘들 수 있다. 반면에 Squash Merge 는 몇 개의 Commit 이 쌓여있든 간에 새로운 하나의 커밋으로 만들어서 병합된다. 여기서 새로운 하나의 커밋으로 만든다는 것이 중요하다. 1,2,3 Commit 에 해당하는 PR 을 올려두고, 계속 작업을 진행하면서 4,5,6 Commit 을 만들었다고 하자. 여기서 일반 머지를 하게 되면 1,2,3 커밋이 그대로 들어가면서 Commit 4 는 3을 기준으로 만들어진 커밋이기 때문에 커밋에 충돌이 생기지 않는다. 그러나 스쿼시 머지를 하게 되면 1,2,3 커밋이 하나로 합쳐져서 새로운 커밋(예를 들어 Commit 10)을 만들게 된다. 하지만 Git 은 Commit 1,2,3 이 하나로 합쳐진게 Commit 10이란걸 알지 못한다. 이렇게 되면 Git 은 Commit-4 와 Commit-10을 보고는 코드를 비교해보고충돌이 일어난다면 이때는 conflict 를 발생시킨다. (물론 Commit 10 과 Commit 4 간의 코드 충돌이 없다면 스쿼시 머지임에도 충돌이 나지 않을 것이다. ) Typescript 유틸리티 타입: 이번에 타입스크립트로 코드를 작성해보면서 필요한 interface 들을 미리 정의해놓고 사용을 했다. Partial: 이렇게 선언한 Card 라는 타입은 title, content, id 라는 속성을 무조건 가져야한다. 이제 카드에 대한 수정 기능을 추가하면서 Card 타입의 일부분(title, content, id) 중에서 최소 1개, 최대 3개 전부 다 골라야 하는 상황이 생겼다. 수정을 한다고 하면 title 만 바꾸고 싶을 수도 있고, 전부 다 바꿀 수도 있기 때문이다. 이럴 때는 Partial 을 사용하면 된다. Partial&lt;Card&gt; 라는 뜻은 속성을 가질수도 있고 가지지 않을 수도 있다(옵셔널)는 뜻이다. typescriptinterface Card { title: string; content: string; id: string;}const partialCard_1 : Partial&lt;Card&gt; = { title : 'title'} // OKconst partialCard_2 : Partial&lt;Card&gt; = { title : 'title', id : 'id'} // OKconst partialCard_3 : Partial&lt;Card&gt; = { } // OKRequired: 만약 속성들 ‘모두’가 필수적인 속성이 되기를 원한다면 Required 를 사용하면 된다. 예를 들어 typescriptinterface Card { title: string; content: string; id?: string;}// id 는 옵셔널이다!const partialCard: Partial&lt;Card&gt; = { title:  this-is-title , content: 'this-is-content' }; // OK// id 가 없어서 안된다. const requiredPerson: Required&lt;Person&gt; = { title:  this-is-title , content:  this-is-content  };Pick: 그렇다면 이번에는 Card 의 속성을 콕 찝어서 선언하고 싶다면 어떻게 할까? 이럴 때는 Pick 을 사용하면 된다. Pick&lt;Card, 'title' | 'content'&gt; 은 Card 타입 중에서 title 과 content 를 콕 찝은 타입이다. typescriptinterface Card { title: string; content: string; id: string;}// content 필요const pickCard_1: Pick&lt;Card, 'title' | 'content'&gt; = { title:  this-is-title  };// id 는 있을 수 없다. const pickCard_2: Pick&lt;Card, 'title' | 'content'&gt; = { title:  this-is-title , id:  my-id  };스프린트 4주차 학습정리(Virtual DOM 과 재렌더링)재렌더링 방식: 바닐라 JS 로 flux 패턴과 Component 를 잘게 쪼개서 부모 컴포넌트에서 자식 컴포넌트로 props 를 전달하는 과정을 흉내내보면서 재렌더링 에 대한 고민이 생겼다. DOM 이 렌더링 되는 과정은 여기서 간단하게 볼 수 있다. DOM 렌더링 image. png 위와 같이 포스트잇 1,2,3 이 있던 상황에서 맨 위에 포스트잇 4가 들어오게 된다면, 어떻게 해서 재렌더링을 최소화 시킬 수 있을까? 여기서 가장 좋은 방법은 역시 포스트잇 4 만 추가하고 포스트잇 1,2,3 에 미치는 영향은 최소화 시키는 방법일 것이다. 우선 상황을 보면 포스트잇 4 가 맨 위에 추가되면서 포스트잇 1,2,3 은 화면 상 위치 가 바뀌었다. 물론 1,2,3 간의 순서가 바뀌지는 않았지만 한 칸 씩 내려앉게 되었다. 이런 경우에는 DOM 에서 layout 을 잡는 작업인 reflow 와 화면에 그리는 작업인 reflow 과정이 포스트잇 4 에 일어난다. 그리고 이 reflow, repaint 작업은 DOM 조작 중에서 가장 비싼 작업들이다. 그렇다면 포스트잇 1,2,3 은 어떻게 될까?아쉽지만 포스트잇 1,2,3 간에는 순서가 바뀐 것이 없음에도 reflow 가 발생한다. 포스트잇 1,2,3 의 위치가 바뀌었기 때문이다. 어떻게 해도 포스트잇 1,2,3 의 위치가 바뀌었기 때문에 브라우저에서 reflow 가 일어나야만 한다. 이렇게 하나의 카드만 새로 추가 되는 경우에는insertBefore 메소드를 사용해서 그냥 끼워넣어도 성능에 큰 차이가 없을 수’도’ 있다. 다만 아래 처럼 여러 개의 카드가 동시에 추가 되는 경우를 한 번 생각해보자. image. png 이 경우에 가장 쉽게 이 로직을 작성해보면 javascript// 새로운 카드들이 들어있는 배열newCards. forEach((newCard)=&gt;{ . . .  column. insertBefore(newCard, . . . ); . . . })이렇게 newCards 배열을 순회하면서 포스트잇 4를 추가하고, 그 뒤에 포스트잇 5까지 추가하는 로직이면 상당히 간단하게 구현할 수 있을 것 같다. 하지만 이런 방식은 정말 비효율적인 방식이다. 위에서 고민했던 것을 다시 생각해보면 DOM 에 비효율적인 reflow repaint 가 일어나는 것을 알 수 있다. image. png insertBefore 메소드는 실제로 DOM 의 재렌더링을 유발한다. 정확히 말하면 DOM 에 새로운 요소가 업데이트(insertBefore 가 요소를 업데이트) 될 때마다 재렌더링이 발생한다. 즉 한 번의 insertBefore 마다 한 번의 DOM 재렌더링이 발생한다. 실제로 카드를 2개를 동시에 추가하는 것 처럼 보일 수 있으나 이미지처럼 추가되는 카드의 수 만큼 insertBefore 가 발생하고, 그 만큼 reflow repaint 가 발생할 수 있다. 이렇게 비효율적으로 여러 번 발생하는 reflow repaint 를 최소화시키는 것이 이번 고민의 핵심이었다. Virtual DOM: 그렇다면 어떻게 reflow repaint 를 최소화할 수 있을까? 이 고민은 완벽하게 해결하려면 상당히 복잡할 거라고 생각한다. 실제로 카드 하나만 추가할 때는 insertBefore 한 번 하는 것이 더 나은 경우일 수 도 있는 것처럼 경우에 따라서 최선의 해결책이 달라질 수 있기 때문이다. 그렇다면 React 는 어떤 방식으로 재렌더링을 효율적으로 하고 있을까? 리액트는 Virtual DOM 과 Diff 알고리즘을 핵심으로 사용하고 있다. Vanilla Javascript 로 가상돔 만들기를 참고하면 더욱 더 이해가 빠를 것 같다. 브라우저가 HTML 을 파싱해서 만든 결과인 DOM 트리는 DOM 요소가 업데이트 될 때마다 (위에서 insertBefore 등으로 요소를 업데이트하는 등) 재렌더링 작업을 거치게 되고이때 reflow repaint 가 발생한다. 따라서 우리는 DOM 요소가 업데이트가 되어도 바로 재렌더링을 하는게 아니라 업데이트 되는 요소들을 모아서 재렌더링 해야한다. 그러기 위해서는 브라우저가 만드는 진짜 DOM 을 가지고는 할 수 없다. 따라서 우리는 DOM 과 매우 유사한 Virtual DOM 을 만들어서, Virtual DOM 에 변경사항을 전부 업데이트하고해당 부분만 Virtual DOM 으로 patch 할 것이다. 우리가 만든 카드 웹에서는 포스트잇 1,2,3 말고 더 많은 요소들이 DOM 에 존재할 것이다. 그렇지만 우리가 바꾸고 싶은 부분인 포스트잇 컬럼 부분만 보면 충분하기 때문에 Real DOM 전체를 본따서 만드는 것이 아니라포스트잇 컬럼 부분의 DOM 만 본따서 Virtual DOM 을 만들면 된다. 이후에 Diffing(비교) 알고리즘으로 변경된 포스트잇들을 업데이트하고 Real DOM 에서 Virtual DOM 에 해당하는 부분을 patch 하면 된다. 위의 블로그에서는 Real DOM 을 메모리에만 올려서 활용하는 방식으로 Virtual DOM 을 생성한다고 보면 될 것 같다. 그리고 비슷하게 메모리에만 올려서 활용할 수 있는 DocumentFragment 를 활용해도 좋을 것 같다. DocumentFragment 도 DOM 의 가벼운 버전으로, DocumentFragment 에 요소들이 업데이트 되어도 재렌더링이 되지 않는다. Diff 알고리즘:  원래 있던 카드가 없다 -&gt; 삭제 원래 없던 카드가 있다 -&gt; 추가 원래 있던 카드의 내용이 바뀌었다 -&gt; 변경여기서 3번의 경우는 살짝 복잡할 수 있다. 내용이 바뀌었는가? 를 비교하는 로직이 필요하기 때문이다. 만약 카드 객체가 단순히 String 타입이라면? 문자열만 비교해보고 바뀌었는지 아닌지를 구분할 수 있다. 그러나 카드 객체는 보통 여러 자식을 가지고 있는 태그일 것이므로, 모든 자식 태그들을 비교해서 같은지 같지 않은지 확인해야한다. 스프린트 5주차 학습정리(환경변수)환경변수: 리눅스를 다뤄볼때면 늘 나오는 키워드인 export, source, . 까지 완벽하게 이해하고 사용하지는 못했던 것 같다. 이번 기회에 확실하게 짚고 넘어가보자 우선 환경변수 라는 말은 무엇일까?  환경 변수(Environment Variables) 는 시스템의 전역 설정 으로 프로그램이나 명령어가 실행될 때 참고하는 설정 정보를 담고 있는 변수이다. 그런데 조금 아래에서 보겠지만 전역 설정이라는 것 치고는 전역이 아닌 것 같은 일이 발생한다. export: Export 키워드는 현재 셸에서 환경 변수에 등록하는 키워드이다. 아래의 경우 NAME 이라는 변수에는 hoeh 값이 들어있다. shell# nameScript. sh. . . export NAME=hoehecho $NAME. . . $ . /nameScript. sh# hoeh$ echo $NAME# hoeh 가 출력될까?우선 nameScript 라는 스크립트 파일에 export NAME=hoeh 라고 작성한 뒤에 echo 명렁어로 NAME 을 출력하는 스크립트를 작성해보자. 이후에 nameScript 를 실행해보면 hoeh 라는 문자열이 잘 출력된다. 이는 NAME 이라는 환경변수에 hoeh 라는 값이 잘 들어가있음을 알려준다. 그런데 스크립트 실행 후에 따로 echo $NAME 을 실행해보면 hoeh 가 출력될까? 사실 hoeh 가 아니라 아무것도 출력되지 않는다. 분명 nameScript 를 실행했을때는 echo $NAME 에서 hoeh 가 정상적으로 출력되었는데 왜 echo $NAME 을 따로 실행하니까 출력되지 않는걸까? 이유는 자식 프로세스와 export 의 범위를 알아야한다.  쉘 스크립트를 실행하면 현재 프로세스가 아닌 자식 프로세스를 fork 해서, 자식 프로세스에서 nameScript 스크립트가 실행된다.  export 는 실행된 프로세스와 그 자식 프로세스에게만 전파된다. 이 두 가지의 이유를 조합해보면, nameScript. sh 를 실행할 때 자식 프로세스(B) 를 만들어서 해당 스크립트의 내용이 실행되기 때문에,B 에서 export 를 실행한 것이 되므로 B 와 B 의 자식들은 NAME 이라는 환경 변수를 사용할 수 있다. 그러나 B 의 부모 프로세스, 즉 우리가 입력하고 있는 프로세스에서는 NAME 이라는 환경 변수가 없다. 그렇다면 자식 프로세스의 환경 변수를 부모 프로세스로 역전파(?) 시키는 방법이 필요할 것 같은데 그런 방법은 따로 없다. source: 그렇다면 자식 프로세스에서 일어난 환경 변수 설정을 부모 프로세스가 알게하는 방법이 있을까? 프로세스는 스레드와 다르게 ‘독립성’ 이 강조된다. 자식 프로세스를 만들 때, 환경 변수를 복사해서 만들기 때문에 자식 프로세스에 영향이 있을 뿐, 부모 프로세스에게 영향을 끼치면 안된다. 따라서 우리는 부모 프로세스가 환경변수 설정을 적용되지 못하게 하는 원인을 제거해버릴 수 있다. source 명령어는 스크립트 파일을 현재 셸 에서 실행한다. 즉 자식 프로세스를 만들지 않고 부모 프로세스에서 바로 환경 변수를 설정하기 때문에 현재 셸에 환경변수 설정이 적용된다. 스프린트 7주차 학습정리(DB 인덱싱, 로컬 모듈)"
    
}, 

{
    "id": 13,
    "url": "https://hoex5h.github.io//Swift-_Undefined_symbol-_",
    "title": "[Swift] Undefined symbol: ",
    "body": "2025/01/14 - Undefined symbol: OBJC_CLASS$_ [ ]네이버 맵 API를 적용시키려고 하는데 Xcode가 계속 Undefined symbol: OBJC_CLASS$_ [ ] 에러 메세지만 뿜어댔다. 분명 git-lfs 로 설치도 했고, podfile 에 NMapsMap 도 잘 넣어놨는데 해당 에러 메세지가 계속 발생했다. 열심히 찾아본 결과 git-lfs 를 이용하여 정상적으로 설치했다면 프레임워크의 용량이 약 240MB 가 된다고 하는데, 내 폴더의 용량은 3. 3MB 였다.  이유는 git-lfs 를 설치하고 초기화를 해주지 않아서였다. . textbrew install git-lfscd [project_path]git-lfs installpod cache clean NMapsMap여기서 git-lfs install 커맨드를 통해 git-lfs 를 초기화 시켜주어야 정상적으로 설치를 할 수 있다고 한다. 앞서, 이것저것 설치해보면서 깔렸을 NMapsMap 이랑 Pods 폴더를 pod cache clean 으로 없애주자. "
    
}, 

{
    "id": 14,
    "url": "https://hoex5h.github.io//Svelte-_Deploy_Sveltekit",
    "title": "[Svelte] Deploy Sveltekit",
    "body": "2025/01/14 - [Svelte] Deploy Sveltekit이번에 졸업 프로젝트 하면서 간단한 프론트가 필요해서, 예전에 써본 기억이 있던 Svelte 로 뚝딱 만들었다. 아주 짤막하게 만들면 되는 주제에 Sveltekit 이 재밌어 보여서 한 번 써봤다. 그냥저냥 재밌기만 한 줄 알았는데, 이게 웬걸. . ? 배포할 때 build 폴더가 나오질 않는 것이었다… 앞으로 자주 쓸 일은 없겠지만 혹여나 이번 같이 짤막하게 프론트를 만들 일이 생길 수도 있기에 배포하는 방법 정도는 적어두려고 한다. Sveltekit 공식문서 Sveltekit 은 단순하게 npm run build 만 하면 build 폴더가 뿅 하고 생기지 않는다. . 어째서??  Before you can deploy your SvelteKit app, you need to adapt it for your deployment target. SvelteKit 을 배포하려면, adapt 해야 한단다. . 이게 무슨 소리인가하니,  Adapters are small plugins that take the built app as input and generate output for deployment. 어댑터를 하나 달아주어야 하는데, 이 어댑터가 바로 deployment 를 위한 output 을 만들어 준다고 한다. Cloudflare, netlify, … 등등 다양한 어탭터가 있는데, 나는 따로 서버에 직접 배포할 것이므로 Node 서버 adapter 를 선택했다. Sveltekit Node Server 막상 알고나면, 해야할 것은 별 거 없다. npm i -D @sveltejs/adapter-node 커맨드를 통해서, adapter plugin 다운 받아 주시고, 프로젝트 루트 path 에 있는 svelte. config. js 에 들어간다. textimport adapter from '@sveltejs/adapter-node';export default {	kit: {		adapter: adapter()	}};이후 위의 코드 처럼, 다운 받은 plugin 으로 부터 adapter 를 import 하고, kit 에 달아준다. 그 뒤로는 더욱 별 거 없다. npm run build 하고, npm ci --omit dev 하고, node build 하면 된다! 아래는 아주 간단하게 작성한 Dockerfile 이다. shellFROM node:16. 14. 0 AS buildARG PORT=3000WORKDIR /appCOPY . . /RUN npm installRUN npm i -D @sveltejs/adapter-nodeRUN npm run buildRUN npm ci --omit devEXPOSE ${PORT}CMD node build"
    
}, 

{
    "id": 15,
    "url": "https://hoex5h.github.io//Python-_Requirements",
    "title": "[Python] Requirements",
    "body": "2025/01/14 - Requirementsdockerfile 을 작성하다가 text[receipt-backend 7/7] RUN pip install -r requirements. txt:#0 0. 376 Processing /private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/croot-t_zs64wy/anyio_1644482593257/work/dist#0 0. 378 ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/croot-t_zs64wy/anyio_1644482593257/work/dist'#0 0. 378와 같은 에러 났다. 확인해보니 requirements 가 버전이 아니라, textpkg1 @ file:///tmp/tmp44ir_jikpkg2 @ file:///tmp/tmp5pijtzbq이런식으로 작성이 되어 있었다. textpip list --format=freeze &gt; requirements. txt처럼 version 이 나오는 포맷을 지정해주어야한다. "
    
}, 

{
    "id": 16,
    "url": "https://hoex5h.github.io//Linux-_Ubuntu_unexpected_operator",
    "title": "[Linux] Ubuntu unexpected operator",
    "body": "2025/01/14 - [Linux] Ubuntu unexpected operatorhelm 스크립트를 받아와서 설치하는 Ansible playbook yaml 파일을 작성중에, 자꾸 unexpected operator 에러가 발생했다. text- name: 설치 스크립트 다운로드 및 Helm 3 설치get_url:url: [https://raw. githubusercontent. com/helm/helm/main/scripts/get-helm-3](https://raw. githubusercontent. com/helm/helm/main/scripts/get-helm-3)dest: /tmp/get_helm. shmode: '0700' # 실행 권한을 부여합니다. - name: Helm 3 설치 shell: sh /tmp/get_helm. sh왜 이런고 하니, ubuntu 에서는 sh 링크가 bash 가 아니라 dash 에 걸려있다. sh get_helm. sh 이런식으로 “sh” 를 앞에 적어줬더니 dash 는 get_helm. sh 를 이해를 못한것이었다 해결책으로는 그냥 sh 커맨드를 떼버리거나,get_helm. sh 아니면 앞에 bash 라고 명시를 해주는 방법이 있다. bash get_helm. sh 아니면, 앞으로 sh 는 bash 로 링크되도록 하는 방법도 있다. shellsudo unlink /bin/shsudo ln -s /bin/bash /bin/shsh ysoftman. sh"
    
}, 

{
    "id": 17,
    "url": "https://hoex5h.github.io//Linux-_CS_%EC%A7%80%EC%8B%9D",
    "title": "[Linux] CS 지식",
    "body": "2025/01/14 - [Linux] CS 지식systemd: systemd 는 system + d 로 구분할 수 있는데, 보통 하나의 단어 뒤에 d 만 붙어 있는 경우 d는 daemon 을 뜻한다.  Daemon(데몬) : 리눅스 시스템이 처음 가동될 때 실행되는 백그라운드 프로세스. 사용자의 요청을 기다리다가 요청이 들어오면 적절한 대응을 하는 리스너의 역할을 한다. systemd 외에도 syslogd, ftpd, mysqld, httpd 등의 d 도 다 `daemon`의 d 이다. systemd 는 이러한 daemon 들을 OS 부팅 시에 init 해주는 역할을 한다. 또한, 일반적으로 1번 PID 를 갖는 프로세스로서 프로세스 트리의 가장 상위 프로세스이자 모든 프로세스의 직/간접적 부모이다. k8s daemonset: 쿠버네티스에서 daemonset 에 의해 생성된 pod 는 클러스터의 모든 노드에 실행된다. 마치 리눅스의 daemon 이 백그라운드에서 대기하며 항상 실행되고 있는것과 마찬가지로 daemonset 은 모든 노드에 대기하며 심지어 새로운 노드가 추가되었을때도 pod 를 생성한다. 데몬셋은 리눅스의 syslogd 처럼 로그를 수집하거나 노드들을 모니터링하는 등의 작업을 위해서 사용한다.  Taint 와 Toleration 을 적절히 사용하거나, . spec. template. spec. nodeSelector 를 사용하면 특정 노드 집합에만 pod 를 띄울 수 있다. chmod: 리눅스에서 “특정 파일/폴더”의 “특정 유저/그룹”권한을 수정할 수 있는 명령어이다. 권한은 크게 읽기(r), 쓰기(w), 실행(x) 3가지로 나뉘고, 그룹은 사용자(u), 그룹(g), 다른 사람들(o) 로 나뉘며 세 그룹을 동시에 a(==ugo) 로 모두 선택할 수 있다. rwx 권한을 8진수로 표현해서 입력할 수도 있는데, rwx 순으로 4(2 ** 2), 2(2 ** 1), 1(2 ** 0) 중에서 각 그룹마다 권한을 주고 싶은 클래스를 더해서 입력하면 된다.  $ chmod 755 sample. txt 이 경우에는 u(4+2+1), g(4+1), o(4+1) 의 권한을 sample. txt 파일에 부여한다. 출처: systemd k8s daemonset chmod "
    
}, 

{
    "id": 18,
    "url": "https://hoex5h.github.io//LiBoo_Project-_Green_Developer_%ED%9B%84%EA%B8%B0",
    "title": "[LiBoo Project] Green Developer 후기",
    "body": "2025/01/14 - [Green Developers] Liboo 프로젝트 후기 작성 Liboo 프로젝트프로젝트 링크: 프로젝트 소개에 앞서 깃허브, 노션, 그리고 서비스 페이지 링크입니다! Github   ｜   Notion   ｜   [Liboo. kr](http://liboo. kr/) 프로젝트 소개: 라이부 LiBoo 컨퍼런스를 더 가까이, LiBoo 에서 라이브로 🚀 : Notion   ｜   Figma   ｜   Wiki   ｜   BackLog  프로젝트 개요:  토스, 배달의 민족, 카카오 등 다양한 컨퍼런스들이 있지만 실제로 신청해서 당첨되는 것은 쉬운 일이 아닙니다. 저희는 이러한 컨퍼런스 문화가 더욱 활발해지고, 더 많은 사람들이 기회를 얻을 수 있기를 바라는 마음으로 LiBoo 프로젝트를 기획하게 되었습니다. 작게는 팀원 간 소규모 기술 공유부터, 크게는 네이버 부스트캠프 발표나 기업의 컨퍼런스과도 함께할 수 있는 서비스를 만들고자 합니다! 📺 핵심 기능: 실시간 컨퍼런스 스트리밍: 실시간 컨퍼런스를 시청하거나 호스트가 되어 컨퍼런스를 직접 스트리밍 할 수 있습니다. 실시간 채팅: 컨퍼런스를 보며 채팅으로 소통할 수 있습니다. 질문이 있다면 질문 채팅으로 전송이 가능합니다. 메인 대시보드: 동영상 미리보기, 카테고리 등을 통해 원하는 컨퍼런스를 탐색해 볼 수 있습니다. 컨퍼런스 다시보기: 실시간으로 방송되었던 컨퍼런스를 다시 돌려볼 수 있습니다.  🛠️ 기술 스택: child_database 🏃 주차별 진행 상황:       주차   내용 요약   발표자료         1주차   기획 공유 및 팀 목표 설정   보러가기       2주차   백로그 작성, 퍼블리싱, 동영상 스트리밍 아키텍처 설계   보러가기       3주차   CI/CD, 메인 api 서버 개발, 플레이어 커스텀, 웹소켓 학습   보러가기       4주차   채팅 구현, 서비스 시연   보러가기       5주차   채팅 고도화, 플레이어 고도화, 다시보기 api 설계   보러가기    🔥 기술적 도전: 꾸준한 문서화를 통해 지식을 공유하고, 깊이 있는 기술적 도전을 이어나가고자 합니다.  [학습] 동영상 스트리밍 처리 프로토콜을 알아보자 [학습] 웹 소켓의 실시간 양방향 통신 [과정/근거] 패키지 매니저 및 모노레포 정하기 [트러블슈팅] 외부의 사용자가 Object Storage에 접근하지 못하는 권한 제어 [트러블슈팅] Docker yarn-berry workspace 를 찾지 못하는 오류더 많은 기술정리 보러 가기 TEAM 정권지르기 👊:       김준서   김영길   고민지   김지수   홍창현                            BE   BE   FE   FE   FE       @i3kae   @hoeeeeeh   @gominzip   @jsk3342   @spearStr   Ncloud 서비스: 사용중인 Ncloud 서비스로는  Object Storage Server 2대(production, development) Cloud DB (MySQL, Redis) public ip, storage, VPC를 사용하고 있습니다! 실시간 컨퍼런스 플랫폼이다보니 HLS 세그먼트와 index. m3u8 파일을 Object Storage 에 업로드하고 있고,개발에 조금 더 용이하도록 개발용과 배포용 서버 2대를 사용하고 있습니다. 또한 Cloud DB MySQL, Redis 를 사용중인데 추후에는 서버 한 대를 추가해서 직접 설치해 사용할 예정입니다! Ncloud 서비스 적용: image image 현재 Ncloud 의 production 서버에는 NGINX 를 도커 컨테이너로 띄워서 리버스 프록싱 하고 있습니다. 또한 RTMP 서버, API 서버, Chat 서버도 마찬가지로 production 서버에 각각을 컨테이너로 띄워둔 상태입니다. MySQL 과 Redis 는 현재 NCP 가 제공하는 서비스인 Cloud DB for MySQL, Redis 를 사용중인데 추후에는 Private Subnet 을 활용해서 개별 인스턴스에 직접 설치해 운영할 계획입니다. RTMP 서버에서 rtmp 스트림을 hls 세그먼트와 index. m3u8 로 변환 후에 Object Storage 에 업로드 하면, 시청자는 해당 주소의 Object Storage 에 접근해서 영상을 시청할 수 있습니다. Ncloud 후기: Ncloud는 기본적으로 서비스 설명이 한글로 자세히 제공되어 있어, 각 서비스의 기능과 활용 방안을 쉽게 이해할 수 있었습니다. 서비스 품질 측면에서도 매우 만족스러웠습니다. Object Storage를 활용해 클라이언트가 HLS 세그먼트와 index. m3u8 파일을 요청하는 작업이 끊김 없이 원활하게 수행되었습니다. 특히, 기존에 사용하던 AWS EC2에서 Ncloud로의 마이그레이션 과정도 매우 수월했습니다. EC2 API와 호환되도록 설계되어 별도의 학습 없이 바로 적용할 수 있었기 때문입니다. 현재 저희 서비스는 컨퍼런스를 요약해주는 AI를 도입하려는 목표를 가지고 있습니다. 이 과정에서 Hyper Clova X를 활용해 다양한 AI 서비스를 검토하고 있는데, 저희가 활용하고 싶은 유용한 기능들이 정말 많았습니다. 단순한 요약 AI뿐 아니라 컨퍼런스 실시간 자막 생성, 컨퍼런스 내용을 텍스트로 변환하여 복사/붙여넣기 기능을 제공하는 등 추가적인 서비스 확장을 고려하고 있습니다. 이렇게 좋은 기회를 통해서 다양한 NCP 서비스와 AI 기능들을 직접 체험해보며 많은 것을 배울 수 있었고, 실제로 적용해볼 수 있어서 너무 즐거웠습니다. "
    
}, 

{
    "id": 19,
    "url": "https://hoex5h.github.io//Kuploy_Project-_Kuploy_History",
    "title": "[Kuploy Project] Kuploy History",
    "body": "2025/01/14 - [Kuploy] Kuploy HistoryCICD 자동화 프로젝트를 만들면서 수정사항이 정말 아주 많이 생기고 있는데, History 를 보기 좋게 작성해 두는 것이 추후 프로젝트를 리팩토링할 때 아주 쓸모 있을 것 같다. 살짝 늦은 감이 없지 않지만,, Kubernetes: Kompose: Docker-compose 파일을 k8s 에 apply 시키기 위한 tool 이다. 현재 버전은 1. 31. 0 사용 중이다. Kompose 문제점:    -controller statefulset   statefulset 을 만들기 위해서 flag 를 썼더니 json 마지막 부분에 status: 항목이 생겨있다.   status 는 쿠버네티스가 배포/운영 하면서 쓰는 항목이지 apply 하기 전에 쓰는 항목이 아닐텐데 이게 왜 생기지?   심지어 status: replica : 0 으로 잡혀있어서, 만들자마자 replica 가 1이 되기 때문에 Live Manifest 와 Desired Manifest 가 달라져서 바로 out of sync 상태가 된다…     -namespace={“namespace”}모든 항목의 네임스페이스를 지정해주려고 하니 namespace. yaml 파일을 만들어버린다.   이대로 배포하면 namespace. yaml 이 apply 되면서 배포한 어플리케이션의 일부가 되어버리는데,   어플리케이션을 삭제하면 namespace 도 삭제된다. .  Metrics Server: K8s 에 배포되어있는 리소스들의 메트릭을 관찰할 수 있다. CNI (Calico): K8s 의 노드들이 통신하는 규약 여러가지 종류가 있지만 현재 Calico 사용 중 Storage Class: Persisten Volume Claim 을 만들 때, Storage Class 를 사용하는데 Storage Class 에는 provisioner, parameters, reclaimPolicy 를 정의할 수 있다. Storage Class 에 대한 자세한 설명은 여기를 참고하면 된다. Kuploy는 로컬에 저장할 수 있으면서 동시에 동적 프로비저닝을 할 수 있어야 한다. 이러한 조건을 만족시켜주는 Storage Class 가 Rancher/local-path-provisioner 였다. 현재는 다른 storage class를 사용할 필요가 없기 때문에 default storage class 이다. (따로 storage class 가 지정되지 않으면 자동으로 default storage class 로 지정) Docker: Docker hub: 유저들의 이미지를 저장할 레지스트리를 on-premise 로 만들 여력이,, 안되어서 도커 허브를 쓰기로 했다 나중에 저장소가 많이 늘어난다면 사설 레지스트리를 써보거나, Harbor 혹은 Nexus 를 써볼 수도? Github Action: Argocd: Installation: Argocd Api: Kuploy 웹에 사용자가 어떤 어플리케이션을 배포중인지 보여주기 위해서 사용했다. Argocd Image Updater: 사용자들이 소스코드의 변경 -&gt; image push 하는 과정만 존재할 경우, 즉 현재 argocd 가 호시탐탐 지켜보고 있는 application 의 status 에 변화를 불러 일으킬만한 변경점이 없다면 sync 되지 않는다. 우리는 image version 이 바뀌는 것도 sync 를 해야한다. Trouble Shooting: 절대 Project Name 을 멋대로 짓지 마. : Project Name 을 IDE 에서 프로젝트 생성할 때 짓는 이름 쯤으로 생각하면 무한 오류에 빠진다. 이거 때문에 교수님 앞에서 눈물의 에러쇼를 했다.  당신이 원하는 프로젝트 네임을 적는 것이 아니라 ArgoCD에 미리 생성한 Project 중에 하나를 고르는 것이다!!! 기본으로 default 프로젝트가 존재하고, 따로 원하면 만들 수 있다. "
    
}, 

{
    "id": 20,
    "url": "https://hoex5h.github.io//Kuploy_Project-_Kuploy_",
    "title": "[Kuploy Project] Kuploy ",
    "body": "2025/01/14 - [Kuploy] Kuploy 는 어떤 프로젝트인가? Kuploy 는 Konkuk + Deploy 의 합성어로 건국대학교 학생들의 Deploy 를 도와주기 위한 프로젝트이다. 유휴 컴퓨팅 리소스를 활용해서 학생들에게 제공해주자!: 처음 시작은 건국대학교의 유휴 컴퓨팅 리소스들을 활용하여 학생들에게 제공해주자! 라는 취지를 가진 프로젝트였다. 이게 무슨말인가 하면, 건국대학교에는 수 많은 컴퓨터들이 학생들의 수업, 실습을 위해 존재한다. 하지만 실제로 이 컴퓨터들은 수업 시간 외에는 잘 사용되지 않는다. 실습용 컴퓨터도 마찬가지인데, 요새는 대부분 노트북을 하나씩 소유하고 있다보니 모여서 프로젝트를 하는 팀플의 경우를 제외하고는 잘 사용되지 않는다. 이런 상황이다보니 많은 컴퓨터들이 사용되지도 않은 채 관리 미흡으로 먼지가 쌓여가고 있었다. 이렇게 잘 사용되지 않는, 유휴 컴퓨팅 리소스들을 잘 합쳐서 학생들에게 사용할 수 있게끔 제공한다면 학교와 학생 모두가 좋은 일이 아닐까? 라는 긍정적인 사고의 결과물이 바로 Kuploy 프로젝트이다. 학교의 컴퓨터들 그냥 원격으로 쓰면 안돼?: 우리의 목적은 단순히 유휴 컴퓨터들 하나하나에 Teamviewer 를 깔아서 학생들에게 GUI 원격으로 사용할 수 있게 하거나, 아니면 그냥 ssh 를 열어서 사용할 수 있게끔 하는 것이 아니다. 만약 이런 것이 목적이였다면 프로젝트 이름은 Kuploy 가 아니라 Kumote(Kuploy + Remote) 가 되지 않았을까? 우리는 학교의 유휴 컴퓨팅 리소스 뿐만 아니라 개발 프로세스 자동화에도 관심이 많이 있었다. 학생들은 주로 AWS 를 이용해서 자신들의 프로젝트를 테스트하고 서비싱한다. 이렇게 서비싱하는 과정은 크게 아래의 3가지 절차를 밟는다.  AWS Free tier(신규 회원은 1년간 공짜니까!) ec2 생성 Inbound, Outbound 규칙 생성(일단 ssh 부터 뚫어놔야 나도 접근할 수 있으니까) 프로젝트 build 파일들을 ec2 에 옮겨서 run 하거나, docker를 활용하여 run 변경사항이 생긴다면 github 에 commit push 후, 3번과정을 다시 수행 이러한 과정은 복잡하지는 않지만 새로운 프로젝트를 할 떄마다 매번 해줘야하는 번거로움이 있다. 게다가 Free tier 는 성능이 썩 좋은 편은 아니다. 실제로 프로젝트를 진행하던 도중 ec2 의 메모리 부족으로 제대로 실행시킬 수 없는 경우도 있었다. 메모리 스왑을 통해 임시 방편을 마련해놨었지만 말 그대로 임시방편일 뿐이다. 이러다보니 매번 할 때마다 귀찮은 반복 작업이 필요하며, 성능에도 제한이 있어서 제대로 활용하기 힘든 AWS 를 대체하고 싶었다. Kuploy 에서는 건국대학교 학생 한정으로, Free tier 보다 높은 성능(물론, 상한선은 있다)을 제공하면서 동시에 배포 과정을 최적화시킬 수 있다. Kuploy 프로젝트의 진행 과정Cluster 구축: 우리는 우선 교수님으로부터 총 4대의 학교 컴퓨터를 대여 받았다.  1대의 Master Node 2대의 Worker Node 1대의 Ansible Control Node4대의 컴퓨터들은 이렇게 구성해서 클러스터를 구축하기로 결정하였고, 그 전에 4대의 컴퓨터들을 초기화해서 셋팅할 것이 많이 있었다. Master Node, Worker Node 공통 설정: Ubuntu 22. 04 Server LTS 설치: 일단 Ubuntu 22. 04 에서 설치할 OS 환경을 잘 골라서 Ubuntu 22. 04 live server 의 iso 파일을 다운받자. 이제 부팅 디스크로 USB를 만들어야하는데, 집에 USB가 다 어딘가로 사라지고 없어서 SanDisk 사의 USB 2개를 구매하였다. USB를 간단한 방법으로 설치 디스크로 만드는 툴은 크게 2개가 있다.  Rufus Ventoy이 중에 우리는 Ventoy 를 활용했다. 이유는 Rufus 보다 훨씬 쉽고 간단하다!우선 Ventoy 링크를 통해 본인의 OS에 맞는 Ventoy 툴을 다운받자. image. png 이후에 Ventoy2Disk 를 실행하고 부팅 디스크로 만들 USB 를 선택한 후, Install 을 누르면 준비 끝이다. 이후에 iso 파일을 USB에 넣어주기만 하면 자동으로 부팅디스크가 된다. 이렇게 만든 부팅 디스크를 꽂고 설치하면 된다!만약 위에서 만든 USB로 부팅되지 않는다면 BIOS 에서 부팅 순서를 바꿔보도록 하자. 스왑 메모리 끄기: shell$ sudo swapoff -a# 재부팅 후에 스왑 메모리가 자동으로 켜지는 것 방지$ sudo vim /etc/fstab# 스왑 파티션(아래 줄) 주석 처리# /swap. img   none  swap  sw   0    0쿠버네티스가 메모리를 할당할 때, 스왑메모리가 있으면 문제를 초래할 수 있다. 애초에 쿠버네티스가 리소스 관리를 하기 때문에 필요 하지도 않다. 스왑 메모리가 꺼졌는지 확인해보려면 free 커맨드를 사용해보면 된다.  swapoff -a 는 일시적으로 스왑 메모리를 끄는 것이다. 만약 /etc/fstab 에서 스왑 메모리를 영구적으로 끄지 않는다면 컴퓨터를 재부팅했을 때 자동으로 스왑 메모리가 켜지게 된다. 이 경우에 쿠버네티스가 정상적으로 작동하지 않는다.  스왑 메모리가 켜진 상태로 kubectl get nodes 명령어를 입력해보면 Ready여야 하는 노드가 NotReady 상태로 변경되고, 해당 노드에서 실행중인 Pod 들은 Terminating 상태나 Pending 상태에 무한히 빠진다. 방화벽(ufw) 포트 allow: 쿠버네티스 apiserver 가 6443 포트를 사용하기 때문에, 6443 포트를 뚫어주어야 api-server 와 통신할 수 있다. shell$ sudo ufw allow {port}kubectl, kubelet, kubeadm, docker 설치: 최신 버전을 사용하면서 충돌 날 것을 우려하여 1. 23. 6-00 버전으로 명시하여 설치하였다. 설치 이전에 epel, net-tools 를 설치한다. EPEL, net tools 는 뭘까?`EPEL` == Extra Packages for Enterprise Linux말 그대로 엔터프라이즈 리눅스를 위한 추가 패키지이다. 기본 레포에 없는 오픈 소스들을 사용할 수 있다. `net-tools` 는 리눅스의 네트워킹 관련 커맨드라인 도구 모음이다. &gt; A collection of programs that form the base set of the NET-3 networking distribution for the Linux operating system.  &gt; Includes: arp, hostname, ifconfig, netstat, rarp, route, plipconfig, slattach, mii-tool and iptunnel and ipmaddr.  &gt; A mirror of the sourcecode is available on https://github. com/ecki/net-tools 쿠버네티스 1. 24 미만 버전 ```shell# install Extra Packages for Enterpries Linux System, Dockersudo apt-get install epel-release -ysudo apt-get install net-tools -ysudo apt-get update &amp;&amp; sudo apt-get -y upgrade# Install Kubernetes Package# Add Google Cloud Public GPG Key to aptsudo curl -s &lt;https://packages. cloud. google. com/apt/doc/apt-key. gpg&gt; | sudo apt-key add -# Create new APT source listcat &lt;&lt;EOF | sudo tee /etc/apt/sources. list. d/kubernetes. listdeb &lt;https://apt. kubernetes. io/&gt; kubernetes-xenial mainEOF# Update APT Package and upgrade all package# flag -y means all yessudo apt-get update &amp;&amp; sudo apt-get -y upgrade# Install Kubernetes - kubectl, kubelet, kubeadmsudo apt-get install -y kubelet=1. 23. 6-00 kubeadm=1. 23. 6-00 kubectl=1. 23. 6-00# hold kubelet, kubeadm, kubectl versionsudo apt-mark hold kubelet kubeadm kubectlecho  sleep 3 sec sleep 3sudo apt-get update &amp;&amp; sudo apt-get upgradesudo systemctl daemon-reloadsudo systemctl restart kubeletecho  Installing Docker sudo curl -sSL &lt;https://get. docker. com/&gt; | shecho  sleep 1 sec sleep 1echo  sleep 3 sec sleep 3```쿠버네티스 버전을 1. 24 이상 버전을 사용하면서, docker 에서 containerd 로 옮겨가게 되었다. 쿠버네티스와 containerd 의 설치 방법은 k8s-docker-containerd 에 적어두었다. containerd 설정: shellsudo mkdir -p /etc/containerdcontainerd config default | sudo tee /etc/containerd/config. tomlsystemd 를 cgroup driver 로 설정하기: /etc/containerd/config. toml 에 아래 항목 추가 shell[plugins.  io. containerd. grpc. v1. cri . containerd. runtimes. runc] . . .  [plugins.  io. containerd. grpc. v1. cri . containerd. runtimes. runc. options]  SystemdCgroup = trueshellsudo systemctl restart containerdcontainerd 재시작. IPv4 포워딩, bridge iptable 규칙 수정: Container Runtime 쿠버네티스 공식 문서를 참조해서 진행했다. shellcat &lt;&lt;EOF | sudo tee /etc/modules-load. d/k8s. confoverlaybr_netfilterEOFsudo modprobe overlaysudo modprobe br_netfilter# sysctl params required by setup, params persist across rebootscat &lt;&lt;EOF | sudo tee /etc/sysctl. d/k8s. confnet. bridge. bridge-nf-call-iptables = 1net. bridge. bridge-nf-call-ip6tables = 1net. ipv4. ip_forward         = 1EOF# Apply sysctl params without rebootsudo sysctl --system아래 커맨드로 br_netfilter 와 overlay 모듈이 load 되어 있는지 확인해보라고 한다. shelllsmod | grep br_netfilterlsmod | grep overlay이제 sysctl에 저 3가지 항목의 값이 1로 설정되어 있는지 확인해보자. shellsysctl net. bridge. bridge-nf-call-iptables net. bridge. bridge-nf-call-ip6tables net. ipv4. ip_forwardcgroup: 리눅스는 cgroup가 프로세스의 리소스를 제한하는 역할을 한다. kubelet과 containerd 도 pod의 리소스를 제한하는데 cgroup driver 를 사용해야 하는데, 반드시 kubelet 과 containerd 는 같은 구성의, 같은 cgroup driver 를 사용해야 한다는 것이다. cgroup driver 에는 cgroupfs 와 systemd 가 존재하는데 만약 init 프로세스가 systemd 인 경우에는 kubelet 과 containerd 의 cgroup driver 또한 systemd 를 사용하도록 권장하고 있다. 이유는 systemd 가 cgroup 관리자는 하나라고 인식하기 때문이다.  추가로 cgroup v2 를 사용할 경우에도 systemd 를 사용하라고 한다. cgroup v2 가 무엇인지 문서를 읽어보니 cgroup 의 업그레이드 버젼인듯 하다. kubelet 의 cgroup driver 를 systemd 로 수정하기: kubelet-config-file kubeadm 을 통해 init 을 하고 난 후에, kubelet 의 config 파일을 아래처럼 수정해주면 된다. 아직 kubeadm init 후에 적용하면 되기 때문에 지금 당장 config 파일을 만들 필요는 없다. image. png Master Node 설정: kubeadm init: shell$ sudo kubeadm init --apiserver-advertise-address=[마스터 노드 IP] --pod-network-cidr=[CNI 네트워크 라우팅 대역]마스터노드에서 kubeadm 을 init 하면서 네트워크 라우팅 대역 을 입력해야하는데, Calico 를 사용한다면 192. 168. 0. 0/16 를 입력해 주면 된다. cidr 에 관해서는 Gateway,사설망,CIDR 을 참조하자! 해당 커맨드를 입력하면 아래와 같은 결과값이 반환되는데 이를 어딘가에 적어두거나 기억해두자. 추후 Worker Node 설정에 필요하다. shell# 딱히 건들지 않았다면 api-server 의 포트가 6443 으로 설정되어 있을 것이라서 마스터 노드의 6443 포트를 입력한다. sudo kubeadm join {마스터 노드 IP}:6443 --token {토큰값~}이후에 config 에 권한을 부여해줘야 한다. shellsudo mkdir -p $HOME/. kubesudo cp -i /etc/kubernetes/admin. conf $HOME/. kube/configsudo chown $(id -u):$(id -g) $HOME/. kube/configsystemctl restart kubeletsystemctl restart dockerCNI(Container Network Interface): CNI에 대한 자세한 내용은 따로 작성해둔 Kubernetes-CNI 문서를 참조하자. 우리는 CNI 로 calico 를 선택했다. 사실 이렇게 소규모 프로젝트에서 어떤 CNI를 쓰든 큰 차이가 나지는 않을 것이다. 가장 대중적으로 사용하는 Calico, Flannel, Weave Net 정도의 차이점만 알아보았는데 대규모 트래픽 연산(성능) 측면에서는 L3를 활용하는, 즉 모든 Container 마다 ip를 부여해서 통신하는 Calico의 성능이 제일 좋았고, 간편성만 따졌을 때는 Flannel 이 제일 좋았다. WeaveNet은 Mesh 네트워크 구조라서 성능이 나머지 두 플러그인에 비해 조금 떨어진다. 대규모 트래픽 연산에 성능이 좋은 Calico와, 소규모 프로젝트에 어울리는 Flannel 중에서 선택을 고민했고 Calico 가 조금 더 대중적인 이유를 고려해서 Calico를 선택했다. Calico 설치: shell# 2024-03-22 기준!$ kubectl create -f &lt;https://raw. githubusercontent. com/projectcalico/calico/v3. 27. 3/manifests/tigera-operator. yaml&gt;$ kubectl create -f &lt;https://raw. githubusercontent. com/projectcalico/calico/v3. 27. 3/manifests/custom-resources. yaml&gt;더 자세한 것은 Calico 공식 홈페이지 를 참조하자! calico 가 예전의 프로젝트에서 분리되어 나온건지 예전과 달라진 적이 있으므로 공식 홈페이지에서 안내해주는대로 설치하는 것을 권장한다. shell$ watch kubectl get pods -n calico-system이후에 위의 커맨드로 모든 calico pod 들의 Status 가 Running 으로 바뀌는지 확인한다. 대략 5~6분 정도 소요되는 것 같다. shellkubectl taint nodes --all node-role. kubernetes. io/control-plane-kubectl taint nodes --all node-role. kubernetes. io/master-마지막으로 control plane 의 taint 를 제거함으로써 pod 스케쥴링을 할 수 있도록 바꿔준다. Helm 설치: Helm 은 쿠버네티스의 Package managing tool 이다. Linux 의 APT 나 YUM, 맥에서는 homebrew 와 비슷하다고 보면 된다. shellcurl -fsSL -o get_helm. sh &lt;https://raw. githubusercontent. com/helm/helm/main/scripts/get-helm-3&gt;chmod +x get_helm. shhelm versionWorker Node 설정: Master Node 에서 kubeadm init 하면서 얻었던 join 커맨드를 입력해주자. shell# 딱히 건들지 않았다면 api-server 의 포트가 6443 으로 설정되어 있을 것이라서 마스터 노드의 6443 포트를 입력한다. sudo kubeadm join {마스터 노드 IP}:6443 --token {토큰값~}join 이 정상적으로 되었다면 master node 에서 kubectl get nodes 를 입력 했을 때, worker node 들의 상태가 NotReady 가 아닌 Ready 상태여야 한다. "
    
}, 

{
    "id": 21,
    "url": "https://hoex5h.github.io//Kubernetes-_kubelet_is_not_running",
    "title": "[Kubernetes] kubelet is not running",
    "body": "2025/01/14 - [Kubernetes] kubelet is not running쿠버네티스를 1. 29 버전으로 업그레이드 하면서 새롭게 kubeadm init 을 했는데, 아래 사진처럼 에러가 났다. image. png 원인을 알아보려고 우선 systemctl status kubelet 과 journalctl -xeu kubelet 을 실행해봤는데, image. png 아무래도 runc 의 문제인 것 같았다. containerd, runc 설치에 문제가 있었나?: 우선 which containerd 와 which runc 를 실행해봤더니, containerd 는 문제없이 잘 찾았는데 runc 를 찾지 못했다. 일단은 빠르게 해결하려고 심볼릭 링크를 걸어서 runc 를 찾을 수 있도록 했는데 이유가 뭔가 하고 봤더니. . usr/local/bin 에 runc 바이너리 파일이 아니라 runc 폴더 안에 runc 바이너리 파일이 있던 것이었다. 맨날 apt 같은 패키지 매니저를 사용하다가 이번에 처음으로 바이너리 파일로 다운받다보니 많이 헤매고 있다. . runc를 정상적으로 설치한 후에는 systemctl status kubelet 으로 kubelet 상태가 정상인 것도 확인할 수 있고, kubeadm init 도 정상적으로 동작하는 걸 확인할 수 있다! "
    
}, 

{
    "id": 22,
    "url": "https://hoex5h.github.io//Kubernetes-_Metrics_Server",
    "title": "[Kubernetes] Metrics Server",
    "body": "2025/01/14 - [Kubernetes] Metrics Server쿠버네티스에서는 Metric 이라는 것을 활용해서 Pod 들을 오토스케일링 할 수 있다고 한다. 그렇다면 metric 은 무엇일까? Metric 이 뭘까: metric 은 쿠버네티스에서 “어떤 시스템의 성능이나 상태” 정도로 생각하면 될 것 같다. 예를 들어 CPU 사용량, 메모리 사용량, 네트워크/디스크 입출력 등등이 있겠다. 스케일링: 스케일링에는 수직 스케일링과 수평 스케일링, 2가지 스케일링이 있다. 수직 스케일링(Vertical Scaling): 수직 스케일링은 어떤 시스템의 성능(CPU, RAM 등)을 조금 더 많이 배치해서 그 시스템의 성능을 끌어 올리는 것이다. 내가 사용하던 컴퓨터의 램을 16GB 에서 32GB 로 업그레이드하는 것을 수직 스케일링의 예로 들 수 있다. 수평 스케일링(Horizontal Scaling): 수평 스케일링은 시스템을 복제해서 여러 시스템을 배치하는 것이다. 내가 사용하던 16GB 의 컴퓨터와 동일한 사양으로 하나 더 준비해서, 컴퓨터를 총 2대 가용하는 것을 수평 스케일링의 예로 들 수 있다. HPA(Horizontal Pod Autoscale): Metric 을 이용해서 오토스케일링 한다는 것은 무엇일까? 공식 문서는 여기!(kubernetes_HPA) 쿠버네티스에서는 필요시에 Pod 를 복제하여 수평적으로 스케일링을 할 수 있다.  쿠버네티스는 Horizontal Pod Autoscaling을 간헐적으로(intermittently) 실행되는 컨트롤 루프 형태로 구현했다(지속적인 프로세스가 아니다). 실행 주기는 kube-controller-manager의 –horizontal-pod-autoscaler-sync-period 파라미터에 의해 설정된다(기본 주기는 15초이다). 각 주기마다, 컨트롤러 매니저는 각 HorizontalPodAutoscaler 정의에 지정된 메트릭에 대해 리소스 사용률을 질의한다. 컨트롤러 매니저는 scaleTargetRef에 의해 정의된 타겟 리소스를 찾고 나서, 타겟 리소스의 . spec. selector 레이블을 보고 파드를 선택하며, 리소스 메트릭 API(파드 단위 리소스 메트릭 용) 또는 커스텀 메트릭 API(그 외 모든 메트릭 용)로부터 메트릭을 수집한다. 쿠버네티스의 컨트롤러는 일정 주기(기본 15초) 마다 타겟의 파드를 선택해서 HorizontalPodAutoscaler 정의에 지정된 메트릭을 수집한다. 만약 내가 CPU 를 지정해두었다면, 15초마다 너 지금 CPU 얼마나 쓰고 있는지 볼게~ 하고 확인하는 것이다. 파드를 지정할 때, 컨테이너에 필요한 각 리소스(CPU와 메모리 등등)의 양을 선택적으로 지정할 수 있다. 예를 들어 특정 Pod에서 각각의 컨테이너가 최소 CPU 100m 은 써야하고(request), 200m 아래로 썼으면 좋겠어(limit) 라고 지정할 수 있다. 또한 HorizontalPodAutoscaler 에는 최소, 최대 레플리카의 개수를 지정할 수 있다. 예를 들어 아래와 같이 오토스케일러를 지정했다고 하자. yamlapiVersion: autoscaling/v1kind: HorizontalPodAutoscalermetadata: name: web-application-hpa namespace: defaultspec: scaleTargetRef:  apiVersion: apps/v1  kind: Deployment  name: web-application minReplicas: 2 maxReplicas: 10 targetCPUUtilizationPercentage: 50wen-application 이라는 Deployment는 Pod가 최소 2개 있어야 하며, 최대 10개까지 복제가 가능하다. 만약 존재하는 Pod 들의 평균 CPU 사용률이 50퍼센트가 넘어간다면, HPA 는 scale up 을 하게 될 것이다. 이렇게 생성된 Deployment 는 Pod 2개를 미리 생성해놓고 대기하고 있을 것이다. 이때 갑자기 많은 양의 사용자가 몰렸다고 하자. 첫 번째 Pod 의 CPU 사용률은 40퍼센트, 두 번째 Pod 의 CPU 사용률은 70퍼센트가 되었다. 15초마다 HPA는 정기적으로 CPU 사용률을 체크하다가,“`! CPU의 평균 사용률이 (40+70)/2 = 55% 네!, 50퍼센트를 넘었잖아!” 라고 확인하게 되면아래의 공식으로 레플리카 수를 측정해서 늘린다. 원하는 레플리카 수 = ceil[현재 레플리카 수 * ( 현재 메트릭 값 / 원하는 메트릭 값 )]  원하는 레플리카 수 = ceil[2 * (55 / 50)] = ceil(2. xx) = 3 ceil 은 소숫점 올림 함수현재 레플리카가 2개 존재 하기 때문에, 1개를 추가 생성해서 3개로 맞추게 된다. HPA는 이렇게 수평적으로 확장(Scale up)만 하는 것이 아니라 부하량이 줄어들고, 파드의 수가 최소 설정값 이상인 경우, HorizontalPodAutoscaler는 워크로드 리소스(디플로이먼트, 스테이트풀셋, 또는 다른 비슷한 리소스)에게 축소(Scale down)을 지시하기도 한다. Metrics Server: 그렇다면 이 Metric 은 어떻게 수집할 수 있을까? 이 역할을 바로 Metrics Server 가 담당한다. 이제 Metrics Server 를 설치해보자. shellkubectl apply -f &lt;https://github. com/kubernetes-sigs/metrics-server/releases/latest/download/components. yaml&gt;사실 Metrics Server 를 설치하고 CA 인증 절차를 거쳐야 하는데 지금은 간단하게 살펴만 보고 추후에 CA 인증을 하도록 하자. . shellkubectl edit deployment metrics-server -n kube-system이후 spec -&gt; container -&gt; args 항목에서  -kubelet-insecure-tls=true를 추가한다. 참고!: 파드 및 컨테이너 리소스 관리 "
    
}, 

{
    "id": 23,
    "url": "https://hoex5h.github.io//Kubernetes-_Kubernetes_Lens",
    "title": "[Kubernetes] Kubernetes Lens",
    "body": "2025/01/14 - [Kubernetes] Kubernetes LensLens 란 무엇인가:  Kubernetes Lens 는 클러스터 관리 GUI 툴이다. 쿠버네티스로 프로젝트를 할 때, 클러스터를 관리하는 것은 무조건 kubectl 만을 사용해야 하는 줄 알았다. 근데 우연히 어떤 분께서 작성하신 쿠버네티스 포스트를 읽다가 Kubernetes Lens 라는걸 알게 되었는데, 완전 편해보이는것이 아닌가. . 물론 CLI 도 장점이 있지만 역시 GUI가 깔끔하긴하니까. . Lens 설치: Lens 설치는 크게 어려울 것이 없다. 아래의 링크에서 운영체제에 맞게 다운로드 받으면 끝이다. Lens 공식 홈페이지 Lens 에 Add Cluster: VPN ?: 이 부분이 사실 조금 고민거리였는데, 애초에 쿠버네티스 클러스터는 대부분 private ip 로 구성되어 있을거고 내가 하고 싶은 것은 클러스터 외부에 있는 컴퓨터(내 맥북)에서 클러스터를 관리하고 싶은 것이다. 그러기 위해선 내 맥북이 클러스터와 같은 네트워크로 들어가거나, 아니면 apiserver 를 외부로 노출시켜야 했다. 아무래도 보안상 apiserver 를 외부로 노출시키는것 보단, vpn 을 쓰는게 좋지 않을까? 라는 생각이 들어서 iptime 공유기 vpn 설정을 따라해보려던 찰나, 내가 구매했던 iptime 공유기 버전이 N104E 였는데 이건 저가형이라서 vpn 기능이 없다고 한다. vpn 해보고 싶었는데. . 아무튼 그러해서 apiserver 를 외부로 노출시키는 방법을 선택했다. apiserver 에 certSANs 추가: 그래서 어떤 것을 해야 하느냐? apiserver 의 CA(Certificate Authority) 에 외부 ip를 추가하는 것인데, 쉽게 말해서 private 하게 구성된 apiserver 에 외부 도메인 혹은 ip 를 허용해줘야 한다. certSAN 를 추가하는 방법은 1. kubeadm init 과 동시에 flag 로 추가하는 방법 이 있고, 2. 이미 init 이 끝났다면 configMap 을 수정하는 방법 이 있다. kubeadm 과 동시에 flag 로 추가하려면, --apiserver-cert-extra-sans flag 와 함께 init 하면 된다. 그러나 대부분 Lens 를 사용하려는 사람들은 나처럼 이미 kubeadm init 을 통해서 클러스터를 구성하고 여러 작업을 진행한 사람들일 것이다. 이런 경우에는 init 을 다시하기엔 여태 해왔던 작업들이 너무 아까우니까, configmap 을 수정하고, apiserver 의 CA를 다시 발급하는 과정을 수행하면 됩니다. Configmap 수정: 1. 현재 configmap 을 가져온다. text$ kubectl get configmap kubeadm-config -n kube-system -o jsonpath='{. data. ClusterConfiguration}' &gt; kubeadm-conf. yaml2. certSANs 를 추가한다. image. png 위의 이미지에서 your. domain. or. ip 부분에 추가하고 싶은 도메인이나 ip 를 입력하면 된다. 3. apiserver CA 다시 발급받기 Cert 를 다시 발급받기 전에, 우선 기존의 Key 가 있으면 재발급이 되지 않기 때문에 기존의 key 들을 다른 곳으로 옮기거나 삭제해야한다. 혹시 잘못될 수 있으니 다른 곳으로 백업해두는 것을 권장. text$ cd /etc/kubernetes/pki$ mkdir temp$ mv apiserver. * temp이후, 재발급 받으면 된다. text$ kubeadm init phase certs apiserver --config ~/work/kubeadm-conf. yaml재발급 받은 뒤에는 1번에서 만들어두었던 configmap 에 변경사항을 적용한다. {your_directory}에는 1번에서 만들어둔 configmap 의 경로. textkubeadm init phase upload-config kubelet --config /{your_diretory}/kubeadm-conf. yamlLENS 에서 cluster 추가: image. png 공식 홈페이지에서 다운받은 뒤에, 사진처럼 Add Cluster 를 눌러주면 되는데, 단순히 ~/. kube/config 파일을 불러오는게 아니다. “Add_Cluster 이 문서를 참조하라고 하는데, 확인해보면 아래의 커맨드로 현재의 config 를 불러오면 된다. textkubectl config view --minify --raw이후에, 사진에서 보이는 server: https://{your. ip. or. domain}:{api-server-port} 를 위에서 certSANs 에 추가한 도메인이나 ip로 바꿔주면 된다! image. png 잘 완료되었다면 아래와 같이 클러스터의 상황을 볼 수 있다! image. png "
    
}, 

{
    "id": 24,
    "url": "https://hoex5h.github.io//Kubernetes-_Kubernetes_CNI",
    "title": "[Kubernetes] Kubernetes CNI",
    "body": "2025/01/14 - [Kubernetes] Kubernetes CNI쿠버네티스 클러스터 네트워킹의 두 가지 원칙: 쿠버네티스의 공식 문서를 참조해서 CNI 가 무엇인지, 어떠한 역할을 하는지 알아보자. 참고로 한국어로 번역된 페이지도 생각보다 많으니 공식 문서에서 한국어가 지원되는 페이지인지 확인해보는 것도 좋다. 쿠버네티스 클러스터의 네트워킹에서는 반드시 지켜야 하는 두 가지 원칙이 있다.  임의의 노드에 있는 파드와, 다른 모든 노드에 있는 모든 파드들과 물리적으로 통신할 수 있어야 한다.  임의의 노드에 있는 파드와, 같은 노드에 있는 모든 파드들과 물리적으로 통신할 수 있어야 한다. 2번의 경우, 같은 노드에 있는 파드들은 같은 네임스페이스를 공유하기 때문에 로컬 호스트로 각각 컨테이너에 할당된 포트로 통신이 가능하다. 같은 노드에 있는 A pod(8080) 와 B pod(7070) 은 서로 로컬호스트 포트로 통신이 가능하다는 뜻이다. 그렇다면 1번 원칙은 어떻게 될까? 뜬금없지만 사실 쿠버네티스 내부에는 네트워크가 구현되어 있지 않다. 위에서 언급했던 원칙처럼 어떻게 통신이 이루어져야 하는지에 대한 사양만 있고, 직접 구현되어 있지는 않다. 사용자가 자신의 환경을 고려해서 적절한 네트워크 플러그인을 가져다 써야하는데, 이때 사용하는 네트워크 스펙 인터페이스가 CNI 이다. CNI가 뭘까?: CNI Github CNI 공식 깃허브 문서에 따르면, CNI(Container Network Interface)는 리눅스 컨테이너에서 네트워크 인터페이스를 구성하기 위한 플러그인을 작성하는 사양(specification)과 라이브러리로 구성된 프로젝트이다. CNI는 오직 컨테이너간 네트워크 연결과 컨테이너가 삭제될 때, 할당된 리소스들을 지우는 것에만 관여한다. 이러한 점들 때문에, CNI는 넓은 범위의 지원을 받으며 구현하기 간단한 사양을 가지고 있다. CNI는 리눅스 컨테이너의 네트워크 인터페이스를 관리할 수 있는 사양(specification)에 대한 프로젝트다. 컨테이너가 생성되고 삭제될 때, 네트워크 인터페이스가 생성되고 삭제되는 것을 어떻게 처리할 것인지에 대한 내용을 포함하고 있다. CNI는 그럼 왜 필요할까?: 네트워크에 대한 구성은 인프라 환경에 따라서 구성이 매우 복잡하고 다양하다. 이렇게 다양한 네트워크 환경에서 수 많은 문제점들이 발생할 수 있는데, 이 수많은 문제점들에 대한 해결 방법도 천차만별이다. 결국 환경에 따라 네트워크 구성이 점점 복잡해지고 획일화되기는 쉽지 않게 될 것이다. CNI는 이렇게 네트워크 구성과 문제점이 복잡해지는 것을 막기 위해서 등장했다. 컨테이너 네트워크와 관련된 표준 스펙을 정의하고 있는 CNI를 통해서 네트워크를 구성하게 된다면 복잡했던 네트워크 구성과 문제점들을 어느정도 일축시킬 수 있을 것이다. Kubernetes CNI: 그 중에서 쿠버네티스를 위한 대표적인 플러그인들에는 Calico, Flannel, Cilium . . . 등이 있다. 더 자세한 내용은 쿠버네티스 네트워킹 애드온 을 참고하자. Calico: 우리는 이번 기회에 Calico CNI 를 사용해볼 예정이기 때문에, Calico 에 대해 조금 자세히 알아보자. What is Calico? 버전이 올라가면서 Calico 공식문서 및 깃허브 링크가 바뀌는 경우가 있다. 기존의 프로젝트에서 떼어져 나온다든지, 이러한 이유들이 있는걸로 보인다. What is Calico:  쿠버네티스에서는 기본적으로 파드 간 통신에 대한 기본 설정은 모두 허용(default-allow)이다. 네트워크 정책을 이용해서 이를 제한하지 않는다면, 모든 파드들은 다른 파드들과 제약없이 통신할 수 있다. Calico 는 네트워크 통신을 보호하는 네트워킹과 cloud-native 의 마이크로서비스/애플리케이션을 보호하기 위한 고급 네트워크 정책으로 구성되어 있다. Calico CNI는 containers, Kubernetes clusters, virtual machines, and native host-based workloads 를 보호하는 L3/L4 네트워킹 솔루션으로, 여러 데이터플레인을 프로그래밍하는 컨트롤 플레인이다.  데이터 플레인 : 데이터 패킷이 이동하는 부분으로 실제로 사용자 또는 시스템 간에 데이터를 전송하는 역할을 담당하는 플레인. 데이터 패킷을 처리하고, 패킷의 다음 목적지로 전달하는 실질적인 처리와 전송을 담당하는 플레인 (근육)  컨트롤 플레인 : 네트워크 경로를 결정하고, 정책을 설정하는 등의 네트워크 운영을 총괄하는 플레인. 네트워킹 장비에게 어떤 경로로 데이터 패킷을 전달해야 하는지 라우팅을 생성하고 관리하는 기능을 담당하는 플레인 (손,발) image. png "
    
}, 

{
    "id": 25,
    "url": "https://hoex5h.github.io//Gateway,_%EC%82%AC%EC%84%A4%EB%A7%9D,_CIDR",
    "title": "Gateway, 사설망, CIDR",
    "body": "2025/01/14 - IP 주소를 묶는 방법, CIDR란? [Network] Gateway, 사설망, CIDRIPv4: 우선 IPv4 는 총 32 비트의 숫자로 구성 되어 있다.  XXX. XXX. XXX. XXX 각각 8비트씩 -&gt; 2^32개! 이 중에서 일부 ip가 특수한 목적으로 선점되어 있다.  사설 ip, 루프백 ip, 멀티캐스트 ip, 0. 0. 0. 0(현 주소), 브로드캐스트 ip… 실제로 사용할 수 있는 ip 는 2^32 보다 적다예전에는 모든 기기에 하나씩 ip 를 할당해도 여유로웠겠지만, 기기들이 늘어난 요새는 매우 힘들다. 따라서 사설망(private network)이 필요로 해졌다. 사설망(private network): 여러대의 기기를 하나의 사설망으로 묶고, 중복되지 않도록 사설 ip를 할당해준다. 이때 외부와 통신하기 위해서는 public ip 를 가지고 있는 장비가 필요한데, 이것이 Gateway 이다. Gateway: 게이트웨이는 다른 대역으로 라우팅을 해줄 수 있는, 주로 L3 스위치급 이상의 장비이다. 가장 많이 보이는 것은 역시 어느 집에서나 볼 수 있는 공유기다. 사설망에 있는 장비들이 외부와 통신하려고 할 때, Gateway 를 거쳐서 통신하게 되는데 Gateway 는 public ip 를 하나 할당받고, NAT(Network Address Translation) 기능을 사용하여 내부 네트워크에서 오는 여러 요청을 public IP 주소를 사용하여 외부로 전송하고, 외부에서 돌아오는 응답을 올바른 내부 장치로 전달한다. 만약 사설망의 여러 장비가 같은 목적지의 같은 포트로 통신을 하려고 한다면 게이트웨이는 NAT 테이블을 활용하여 각각의 내부 소스 IP 주소와 포트 번호를 기반으로 이 요청들을 기억하고 구분한다. 외부로 나갈 때는 일반적으로 변형된 소스 포트를 사용하여, 응답이 돌아올 때 원래의 내부 IP 주소와 포트로 정확히 매핑한다.  예를 들어 A 컴퓨터(192. 168. 0. 2)가 (168. 126. 63. 1, 443포트)에, B 컴퓨터(192. 168. 0. 3)가 다음(168. 126. 63. 1, 80포트)에 동시에 접근한다고 하자. A와 B의 목적지 ip는 동일하지만 포트가 다르기 때문에 443포트와 80포트로 A/B 의 요청을 구분할 수 있다. 하지만 만약 A와 B 모두 443포트와 통신하려고 한다면 어떻게 될까?443포트에서 오는 응답을 A에게 전달해야할지, B에게 전달해야할지 구분해야 하기 때문에 Gateway는 A가 보낸 요청을 4430, B가 보낸 요청을 4431 포트로 변경해서 NAT 테이블에 저장한다. 이제 게이트웨이는 4430 포트로 응답이 돌아오는지, 4431 포트로 응답이 돌아오는지에 따라 A,B 컴퓨터에 알맞게 응답을 돌려줄 수 있다. CIDR(Classless Inter Domain Routing): CIDR는 여러 개의 사설망을 구축하기 위해 망을 나누는 방법이다. 여기서 왜 Classless 라는 단어가 사용되었냐 하면, 옛날에는 A/B/C/D 클래스를 이용해서 망을 나눴기 때문이다. CIDR 을 쓰면 ABCD 클래스를 포함해서 더 다양하게 망을 나눌 수 있기 때문에 굳이 클래스라는 단어로 표기하기 않아도 되기 때문이다. CIDR의 기본적인 표기법은 192. 168. 0. 0/24 처럼, A. B. C. D/E 와 형식이다. 그렇다면 이런 형식은 어떻게 해석할 수 있을까? 192. 168. 0. 0/24 라는 예시로 살펴보면, /24 의 뜻은 앞에서부터 24비트 이후에 오는 비트들을 사용할 수 있다 라는 뜻이다. 조금 더 풀어서 설명하자면 우선 A. B. C. D 를 각각 옥텟(=8비트, 1바이트)이라고 부르며 192. 168. 0. 0을 8비트로 변환해서 써보면, 11000000. 10101000. 00000000. 00000000 일것이다. 맨 왼쪽부터 24비트 이후에 오는 비트들을 사용할 수 있다 라는 의미는, 3번째 옥텟 이후 4번째 옥텟부터 전부 사용할 수 있다는 의미이다.  1번옥텟(8비트). 2번옥텟(8비트). 3번옥텟(8비트) «여기서부터 24비트 이후!» . 4번옥텟(8비트) 즉 4번째 옥텟을 전부 이용하면 2^8을 전부 다 사용할 수 있다는 의미고 0~255 까지 전부 사용할 수 있다. 192. 168. 0. 0/24 =&gt; 192. 168. 0. 0 ~ 192. 168. 0. 255 까지를 나타낸 것이다. "
    
}, 

{
    "id": 26,
    "url": "https://hoex5h.github.io//Express.js_%EB%94%B0%EB%9D%BC_%EB%A7%8C%EB%93%A4%EA%B8%B0",
    "title": "Express.js 따라 만들기",
    "body": "2025/01/14 - ts/js 로 express 따라 만들기 ts/js 로 express 따라 만들기node 로 웹 서버를 만들때부터 express 를 자주 사용했었는데 express 의 내부 동작은 잘 알지 못한채로 그저 app. use 를 사용했던 것 같다. 그래서 이번 기회에 express 를 socket 으로 구현해보려고 한다. http 모듈이 아니라 왜 socket 을 사용해야할까?: 일단 웹 서버 라는걸 생각해보면 당연히 http 모듈을 사용하는 것이 편하다. 소켓으로 구현한다고 해도 클라이언트가 http 프로토콜로 보낸다면 웹 서버에서도 소켓을 http 모듈처럼 사용해야할 것이다. 그럼에도 socket 으로 구현해보면 좋은 이유는 결국 http 통신도 소켓 통신 이라는 것이다. node 의 http 모듈에서 서버를 생성할 때, 결국 소켓 모듈 을 상속받아서 사용한다. 소켓 모듈은 EventEmitter 모듈을 상속받는데, 데이터가 들어오거나 소켓 연결이 생성되거나 끊어졌을 때 등의 이벤트가 발생하게끔 만들어졌기 때문이다. 그래서 소켓을 활용한다는것은 node. js 개발자로서 가장 아래까지 내려가볼 수 있는 기회라고 생각한다. 추후에는 handshake 과정까지 만들어보고싶다. 그래도 추후에 꼭 네트워크 공부는 추가로 하자. . Socket 으로 Listen: Socket 으로 Express 를 구현하기로 했으니 당연하게도 socket 을 활용해서 서버를 만들어야한다. 이는 위에서 언급했던 node 의 net. server 를 활용하면 된다. 여기서 우리가 만들 Express 가 net. server 를 상속받을지, 아니면 net. Server 객체를 그냥 사용할지에 대해서 고민을 할 수 있는데net. Server 를 상속받는다는 것은 net. Server 의 메소드를 오버라이딩 한다든지, 추가 메소드를 만드는 등의 작업이 있을 경우 유효하다고 생각한다. 우리의 커스텀 Express 에서는 net. Server 의 메소드들을 사용할 것이지, 추가로 메소드를 생성/변경 등을 하지 않을 것 같아서 상속 받지는 않을 예정이다. typescriptimport net from 'net';class Express { listen(port: number, callback: () =&gt; void) {  const server = net. createServer((socket) =&gt; {   socket. on('data', (data) =&gt; {    /*      . . .       TODO: socket 으로 넘어온 Request 를 Express 에서 쓸 수 있도록 파싱      . . .     */   });  });  server. listen(port, () =&gt; {   callback();  }); }}net. CreateServer 를 통해서 tcp 연결을 만들고, data 이벤트가 발생했을 때 데이터들을 가지고 우리가 사용하기 편하도록 파싱하는 과정을 거치면 될 것 같다. 🤔 socket 은 stream 이다!: 여기서 생각해보면 좋을 점은 socket 은 Stream 을 활용한다. node 의 net. Socket 은 stream. Duplex 를 상속받아서 사용하고 있다.  Stream 에 대한 정리는 여기서 볼 수 있다. 즉, 클라이언트가 보낸 모든 데이터 가 수신될 때까지 기다리는게 아니라 stream 으로 (순차적으로) 데이터가 들어오면 그 즉시 버퍼에 쌓이고버퍼에 데이터가 일정량 이상 쌓일 때마다 chunk 단위로 data 이벤트가 발생한다. (보통은 버퍼가 넘치기 전에, 혹은 데이터가 전부 다 들어왔을 때 ‘data’ 이벤트가 발생한다. ) 다시 말해서 들어오는 데이터가 크다면 데이터가 나누어져서 들어올 수 있다 ( === 한 번의 통신에 여러 번의 data 이벤트가 발생할 수 있다) 는 이야기이다. 따라서 buffer 의 크기보다 큰 데이터를 클라이언트에서 보냈다면, 여러 번의 data 이벤트가 발생할 수 있다. 실시간으로 데이터를 처리해야하는(ex. 유튜브 같은 영상 스트림) 경우에는 들어오는 데이터마다 처리를 해주면 된다. 반면에 용량이 매우 큰 이미지/동영상 파일은 모든 chunk 들을 다 합쳐서 하나로 만들어야할 것이다. 이에 대한 처리까지 있으면 좋을 것 같다! 소켓 통신을 http 통신 처럼: http 모듈을 사용한다면 소켓 통신으로 들어오는 Request 를 Express 의 Request 처럼 바꿔줄 필요가 있다. 예를 들어 socket 으로 들어온 HTTP Request 는 아래처럼 들어올 것이다. typescriptGET /favicon. ico HTTP/1. 1Host: localhost:3000Connection: keep-alivesec-ch-ua-platform:  macOS User-Agent: Mozilla/5. 0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537. 36 (KHTML, like Gecko) Chrome/129. 0. 0. 0 Safari/537. 36sec-ch-ua:  Chromium ;v= 129 ,  Not=A?Brand ;v= 8 DNT: 1sec-ch-ua-mobile: ?0Accept: image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0. 8Sec-Fetch-Site: same-originSec-Fetch-Mode: no-corsSec-Fetch-Dest: imageReferer: &lt;http://localhost:3000/index. html&gt;Accept-Encoding: gzip, deflate, br, zstdAccept-Language: ko-KR,ko;q=0. 9,en-US;q=0. 8,en;q=0. 7Cookie: sid=1234; coo=11helloworld this is a body이렇게 하나의 거대한 문자열을 잘 파싱해서 우리의 express 에서 쓰기 편하도록 쉽게 만드는 작업이 필요하다. 파싱의 큰 기준은 두 가지 이다.  Header 와 Body 는 \\r\\n\\r\\n 으로 구분된다.  Header 끼리는 \\r\\n 으로 구분된다. 맨 윗 줄을 보면, GET /favicon. ico HTTP/1. 1 이라고 나오는데 Method, url, Protocol/Version 으로 구분해주어 아래 처럼 하나의 객체로 만들어주자. typescriptclass ExpressRequest { headers?: Header; body: string | object = ''; method?: string; // 전체 url url?: string; protocol?: string; version?: string;}🤔 url 의 Query 부분이나 Params 부분도 처리하자: Params: 실제로 express 를 써보면 app. use('/card/:username/:cardId, . . . ) 와 같이 : 를 사용하는 것을 볼 수 있다. 사용자가 Request 를 보낼 때는 /card/hoeh/3?columnId=5&amp;columnName=today 같이 ? 문자 이후에 있는 query 도 있을 수 있다. 그렇다면 위에서 파싱한 url 부분에서 query, param 부분도 분리해주면 좋을 것 같다. typescriptclass ExpressRequest { headers?: Header; body: string | object = ''; method?: string; // 전체 url url?: string; protocol?: string; version?: string; // 여기까지가 SocketRequest // /user/:id 에서 id params: object = {}; // /search?keyword=naver { keyword : naver } query: object = {}; // /user?id=123 에서 /user path?: string;}이런 식으로, url 로 부터 params, query, path 까지 파싱해서 Request 에 넣어주도록 하자. 파싱할 때는 node 의 querystring 모듈과 path-to-regexp 을 활용해보자. app. use(): app. use 는 무엇을 하는 메소드일까? express 를 사용할 때면 이런식으로 사용했던 것 같다. typescriptapp. use('/card', (req, res, next) =&gt; { // . . .  res. send('helloworld');});/card 의 url 로 들어온 요청에 대해서 (req, res, next) =&gt; {} 에 해당하는 함수를 실행하는 것 같은데 그렇다면이 함수는 무슨 함수일까? Middleware, Layer, Router: express 에서는 이런 함수를 middleware 라고 부른다. 미들웨어는 말 그대로 중간에 끼어있는 함수로 보면 된다. Request 를 받아서, Response 를 보내기 전의 중간 처리 과정을 하는 함수를 전부 middleware 라고 부른다. 그러면 express 는 개발자가 middleware 를 등록해서 모든 요청마다 url 을 검사해서, url 조건을 충족하면 등록된 middleware 를 실행한다 의 흐름으로 진행하는구나! 라고 간단하게 생각했는데 여기서 Layer 와 Router 의 개념이 또 등장한다. Layer 가 무엇인지 보니, middleware 를 한 번 감싸주는 계층이다. 엥? Layer 로 굳이 감싸는 이유가 뭐야? 라고 생각이 들었는데 여러가지 이유가 있다.  Method, Url 검사middleware 가 실행되기 전에 사용자가 등록한 /card url 과 들어온 Request 의 url 을 비교해야 한다. 또한 특정 메소드의 요청에만 미들웨어를 실행되게 등록했다면 Method 비교도 해야할 것이다.  에러 처리사용자가 등록한 (req, res, next) =&gt; {} 함수에서 에러를 처리하려면, 미들웨어 안에서 try, catch 를 작성해야하는데 그렇다면 모든 미들웨어에 try, catch 가 있어야 한다. 실제 express 에서는 만약 에러가 발생하면 next(err) 의 방법으로 다음 미들웨어로 err 를 넘긴다. Layer 계층에서는 middleware 를 실행하기 전에 err 가 넘어왔는지 확인하고, 넘어왔다면 에러가 발생했다고 판단해서 에러를 처리할 수 있다. 즉 Layer 계층에서 전역적으로 에러를 처리해줄 수 있다.  같은 Url, Method 에 여러 개의 미들웨어 등록말 그대로 여러 개의 미들웨어가 등록된다면 이를 관리하는 계층이 필요할 수 밖에 없다. 🤔 Layer 안에서 next: 🤔 Middleware 가 비동기일 경우에 에러 핸들링: Response 객체로 socket. write: 🤔 Response 에 Header 와 Body 를 따로 write 하기: 커스텀 express 를 활용해서 웹 서버 코드 작성: 만들어둔 express 를 로컬 모듈로 바꿔보자: 타입스크립트 타입을 위한 d. ts 만들기: 만약 자바스크립트로 http, express 코드를 작성하고 실제 app 은 타입스크립트로 작성한다면 http, express 에 타입을 명시해주는 d. ts 가 필요하다. Express 에서는 Request 의 DTO 를 직접 검사해줘야한다: ZOD 를 활용해보자: "
    
}, 

{
    "id": 27,
    "url": "https://hoex5h.github.io//Dependency_Injection",
    "title": "Dependency Injection",
    "body": "2025/01/14 - Dependency Injection ( Angular 공식 문서 ) nest. js 공식문서를 읽어보는 도중 DI 관련한 문서에 대한 링크로 Angular 의 공식 문서를 걸려있었다. 자세하게, 한국어로도 적혀있는 것 같아서 nestjs 를 쓰기 전에 꼼꼼히 읽어보려고 한다. 의존성 주입은 왜 사용할까? Dependency Injection, or DI, is a design pattern and mechanism for creating and delivering some parts of an application to other parts of an application that require them. 의존성 주입은 애플리케이션의 일부(A)를 또 다른 곳(B)에서 필요로 할 때, A 를 만들고 전달하는 역할을 한다. 즉 다른 클래스에 있는 기능을 사용하고 싶을 때 의존성을 주입해주는 것이라고 생각하면 된다. 의존성을 사용하는 쪽, 제공하는 쪽단순하게 의존성을 주입 하는 것은 어떤 방식으로든 하나의 객체에 다른 클래스의 객체를 넘기기만 해도 의존성 주입이라고 부를 수 있다. typescriptclass Foo {  constructor(){    // . . .   }}class UsingDI {  constructor(private injectClass)}class DoNotUseDI {  constructor(){    this. injectClass = new Foo();  }}위의 예시대로라면 DoNotUseDI 객체는 직접 Foo 객체를 생성해서 사용한다. 따라서 아주 강한 결합을 가지게 되는데, 만약 나중에 Foo 가 아니라 Bar 라는 클래스로 바꾸고 싶다고 하면DoNotUseDI 클래스의 코드도 수정해야하고 테스트할 때도 꼭 Foo 객체를 넣어줘야 하기 때문에 복잡해지게 될 것이다. 반면 UsingDI 클래스는 생성자로 어떤 객체를 넘기느냐에 따라, 그 객체에 결합이 생긴다. 위의 예시에 비해 더 동적으로 의존성이 생긴다고 볼 수 있다. 따라서 Foo 의존성을 주입했다가, 나중에는 Bar 객체를 주입할 수도 있고, 또 나중에는 어떤 객체라도 덕타이핑에 문제가 없다면 (일단은) 주입할 수 있을 것이다. 또한 테스트할 때도 굳이 실제 객체를 넣어줄 필요 없이 간단한 모킹 객체를 넣어줄 수도 있을 것이다. 다만 여기서 기억해두어야 할 점은 의존성은 사용하는 쪽(주입 당하는 쪽) 과 제공하는 쪽(주입하는 쪽) 이 있다는 것이다. 의존성 주입Injector: injector 객체는, 미리 생성된 프로바이더(의존성을 제공당할 수 있는 클래스) 대로 의존성 객체를 생성하고 캐싱해둔다. 또한 인젝터는 (기본적으로) 의존성 객체들을 싱글톤 인스턴스로 관리한다. 모듈에 따라?: NestJS 의 컴포넌트는 크게 모듈, 프로바이더, 컨트롤러, 서비스 등으로 계층을 나눌 수 있다. typescript@Module({ providers: [CatsService], exports: [CatsService],})export class CatsModule {}@Module({ imports: [CatsModule], controllers: [DogsController],})export class DogsModule {}위의 예시처럼 NestJS 모듈 별로 프로바이더, 서비스, 컨트롤러 등등을 주입받는데 모듈별로 injector 가 생성되고 해당 인젝터가 의존성 객체들을 캐싱하고 있다. 또한 CatsModule 에 주입된 CatsService 를 export 함으로써 DogsModule 에서 CatsModule 의 프로바이더를 가져와서 사용할 수 있게 된다. @Injectable : 의존성 주입 가능: javascript@Injectableclass CatService {}클래스를 의존성으로 주입하려면 가장 먼저 @Injectable 데코레이터를 사용해야한다. @Module : 의존성 등록: NestJs 는 @Module 로 의존성을 등록해줄 수 있다. 프로바이더 Scope: 의존성 객체가 되는 프로바이더의 생명 주기는 애플리케이션의 생명 주기와 같다. 애플리케이션이 부트스트랩 될 때, 모든 의존성은 해결(사용 가능)이 되어야 한다. 그렇기 때문에 애플리케이션이 부트스트랩 될 때, 모든 프로바이더들이 인스턴트화 되어야 하고 애플리케이션이 종료될 때 모든 프로바이더들은 제거된다. 만약 프로바이더의 Scope 를 직접 정하고 싶다면, request-scoped 의 방법을 사용할 수 있다고도 한다. "
    
}, 

{
    "id": 28,
    "url": "https://hoex5h.github.io//Kubernetes-_Install_Metallb",
    "title": "[Kubernetes] Install Metallb",
    "body": "2025/01/14 - [Kubernetes] Install MetallbMetallb 를 설치하기 전에, k8s 버전에 따라 kube-proxy 버전을 확인하는 것이 좋다. kubectl get configmap kube-proxy -n kube-system -o yaml | grep mode 위의 커맨드의 결과가 ipvs 라면, strictARP 옵션을 True 로 설정해야한다고 한다. textkubectl get configmap kube-proxy -n kube-system -o yaml | \\sed -e  s/strictARP: false/strictARP: true/  | \\kubectl apply -f - -n kube-system이후, Metallb 를 설치하자 textkubectl apply -f &lt;https://raw. githubusercontent. com/metallb/metallb/v0. 11. 0/manifests/namespace. yaml&gt;kubectl apply -f &lt;https://raw. githubusercontent. com/metallb/metallb/v0. 11. 0/manifests/metallb. yaml&gt;이후 Metallb 에서 사용할 ip 대역을 설정해줘야하는데, 여기서 layer 2 모드와, BGP 모드를 사용할 수 있다고 한다. calico 가 BGP 를 이용하여 트래픽을 라우팅한다고 언뜻 들었던 것 같은데, 우선은 layer 2 모드를 사용해보자. metallb_config. yaml yamlapiVersion: v1kind: ConfigMapmetadata: namespace: metallb-system name: configdata: config: |  address-pools:  - name: default   protocol: layer2   addresses:   - ${IP 대역폭}textkubectl apply -f metallb_config. yaml이제 service 의 type 을 LoadBalancer 로 만들었을 때, external-ip 가 설정했던 대역폭으로 잘 설정된다면, Metallb 설치가 정상적으로 완료된 것이다. "
    
}, 

{
    "id": 29,
    "url": "https://hoex5h.github.io//Kubernetes-_%EB%AC%B4%EC%A4%91%EB%8B%A8_%EB%B0%B0%ED%8F%AC",
    "title": "[Kubernetes] 무중단 배포",
    "body": "2025/01/14 - [Kubernetes] 무중단 배포무중단 배포: 서비스를 사용자에게 배포할 때 변경사항이 생겨서, 중단 -&gt; 수정 -&gt; 배포 과정을 거치게 된다면 다시 배포되기 전까지 사용자는 서비스를 사용할 수 없다. 하지만 당연하게도 중단 -&gt; 수정 -&gt; 배포보다는 수정 -&gt; 배포 과정만 반복하고, 중단 과정이 없는 “무중단 배포”가 서비스 사용자들 입장에선 훨씬 좋지 않을까? 쿠버네티스는 이러한 무중단 배포 아키텍처를 여러가지 제공한다. 롤링 배포(Rolling Deployment): 롤링 배포는 전체 서버에서  일부분을 “ 라우팅 중단 -&gt; 새로운 버전으로 업데이트 -&gt; 배포 “ 하여 일부분을 새로운 버전으로 업데이트 한다. 이렇게 되면 Version 1 과 Version 2 서버가 공존하게 되고, 새로 업데이트 한 Version2 가 문제 없이 잘 작동한다면또 다시 일부분의 라우팅을 중단하고, 새로운 버전으로 업데이트하고, 배포하는 과정을 반복해서 전체 서버를 Version 2 로 업데이트 하는 것이다.  그리고 이 롤링 배포는 쿠버네티스에서 Default 배포 방식이다. 블루-그린 배포(Blue-Green Deployment): 블루-그린 배포는 블루를 구 버전, 그린을 신 버전으로 생각해서 만들어진 이름이다. 정상적으로 잘 작동하고 있던 Version 1(블루) 서버에 더해서 새로운 Version 2(그린) 서버를 준비한다.  이후 그린 서버를 테스트 해보고 그린 서버로 라우팅을 바꿔버리면 끝이다. 대신 블루 서버와 그린 서버, 두 서버를 감당해야 하므로 cost 가 많이 들어가는것은 당연하다. 카나리 배포(Canary Deployment): 카나리 배포는 롤링 배포와 상당히 유사하다. 처음 설명을 읽을 때 카나리 배포와 롤링 배포의 차이점을 구분하지 못해서 여기저기 찾아서 읽어보았다. “아니 이거 롤링 배포 천천히하면 그게 카나리 배포 아닌가?” 라는 생각이 계속 들었는데 대부분의 포스트에서는 카나리 배포가 블루-그린 배포랑 유사하다는 말이 많아서 더욱 헷갈렸다. 나처럼 헷갈리는 사람들을 위해 조금 더 설명해보자면, 롤링 배포는 그냥 임의의 서버들을 Version2 로 올리는거라면 카나리 배포는 소수의 특정 집단 “유저” 를 타게팅해서 바꾸는 것이다.  서버가 아니라 유저를 타게팅한다는 점이 중요하다. 예를 들어서, 내가 만든 어플리케이션이 한국, 일본, 미국, 그 외 나라에 서비싱 중이라고 가정해보자. 이번에 어플리케이션의 작동 속도를 대폭 상승 시킨 버전을 업데이트 하려고 한다. 아무래도 빨리빨리 문화가 우세한 한국에서는 다른 나라에 비해 이번 업데이트에 더 tolerant 하지 않을까? 그러므로 한국에 먼저 업데이트를 올려보고 한국 사용자들의 comment 를 면밀히 살펴보는 것이다. 임의의 서버에 배포하는 롤링 배포보다, 내가 배포한 유저들에 대한 control 이 훨씬 좋고 이슈나 버그들도 훨씬 잘 찾아주리라 기대 된다. 참고한 포스트에서 든 또다른 예시로는, 실 유저에게 배포하기 전에 회사 내부에 먼저 배포하는 것을 예시로 들었다. 출처: 쿠버네티스 공식 문서 Codefresh "
    
}, 

{
    "id": 30,
    "url": "https://hoex5h.github.io//Kubernetes-_%EB%8F%84%EC%BB%A4%EC%9D%98_%EC%BF%A0%EB%B2%84%EB%84%A4%ED%8B%B0%EC%8A%A4_%EC%82%AC%EC%9A%A9%ED%95%B4%EB%B3%B4%EA%B8%B0",
    "title": "[Kubernetes] 도커의 쿠버네티스 사용해보기",
    "body": "2025/01/14 - 도커의 쿠버네티스를 사용해보자먼저, 실습 환경은 M1 pro 맥북 프로임을 알립니다. 도커, 쿠버네티스: 도커에는 자체적으로 쿠버네티스를 지원한다. 아래 사진 처럼 도커에서  Setting -&gt; Kubernetes -&gt; Enable Kubernetes 를 활성화해주자. image. png Node 확인: 이후, 터미널에 textkubectl get nodes를 입력해보면 image. png 처럼 현재 실행중인 노드를 확인할 수 있는데, 여기서 기존에 GKE 나 Minikube 를 사용한 적이 있다면 Context 가 Docker 가 아니라, GKE 나 Minikube 로 잡혀있을 수도 있다. 만약 kubectl get nodes 명령어가 정상적으로 실행되지 않는다면, 우선은 kubectl version 으로 kubectl 이 설치되어 있는지 확인해보고 잘 깔려있다면 textkubectl config get-contexts를 입력해서 결과를 보자. 나의 경우에는 쿠버네티스 해보겠다고 GKE도 써보고 Minikube 도 써보고 했는데, 그 덕에 여러가지 Context 가 존재했다. 만약 Current 의 “*” 표시가 Docker-desktop 에 붙어있지 않다면, 지금 이 문서에서는 Docker-desktop 을 사용할 것이기 때문에 context 를 바꿔주자. textkubectl config use-context { NAME }“kubectl config use-context docker-for-desktop” 으로 context 를 변경하고, “kubectl get nodes” 를 다시 입력해보면 정상적으로 node 가 출력될 것이다. deployment: 다음으로는 간단한 application 을 실행해보자. textkubectl create deployment hellok8s --image=k8s. gcr. io/echoserver-arm:1. 8 --port=8080kubectl expose deployment hellok8s --type=NodePortM1 맥에서는 echoserver-arm 을 사용해야 이미지를 정상적으로 가져올 수 있다. 이제 expose 된 서비스를 확인하기 위해 textkubectl get servicesdeployment result: 를 입력해보면, 아까 배포한 hellok8s 의 포트가 8080:3{XXXX} 로 되어 있을텐데, localhost:3XXXX 로 접속해보면 expose_result 와 같은 결과를 얻을 수 있다. layout: post: title: “[Kubernetes] 도커의 쿠버네티스를 사용해보자”author: hoehcategories: [ kubernetes ]image: assets/images/kubernetes. pngtoc: true 도커의 쿠버네티스를 사용해보자먼저, 실습 환경은 M1 pro 맥북 프로임을 알립니다. 도커, 쿠버네티스: 도커에는 자체적으로 쿠버네티스를 지원한다. 아래 사진 처럼 도커에서  Setting -&gt; Kubernetes -&gt; Enable Kubernetes 를 활성화해주자. k8s_in_docker Node 확인: 이후, 터미널에 textkubectl get nodes를 입력해보면 kubectl_get_nodes 처럼 현재 실행중인 노드를 확인할 수 있는데, 여기서 기존에 GKE 나 Minikube 를 사용한 적이 있다면 Context 가 Docker 가 아니라, GKE 나 Minikube 로 잡혀있을 수도 있다. 만약 kubectl get nodes 명령어가 정상적으로 실행되지 않는다면, 우선은 kubectl version 으로 kubectl 이 설치되어 있는지 확인해보고 잘 깔려있다면 textkubectl config get-contexts를 입력해서 결과를 보자. 나의 경우에는 쿠버네티스 해보겠다고 GKE도 써보고 Minikube 도 써보고 했는데, 그 덕에 여러가지 Context 가 존재했다. 만약 Current 의 “*” 표시가 Docker-desktop 에 붙어있지 않다면, 지금 이 문서에서는 Docker-desktop 을 사용할 것이기 때문에 context 를 바꿔주자. textkubectl config use-context { NAME }“kubectl config use-context docker-for-desktop” 으로 context 를 변경하고, “kubectl get nodes” 를 다시 입력해보면 정상적으로 node 가 출력될 것이다. deployment: 다음으로는 간단한 application 을 실행해보자. textkubectl create deployment hellok8s --image=k8s. gcr. io/echoserver-arm:1. 8 --port=8080kubectl expose deployment hellok8s --type=NodePortM1 맥에서는 echoserver-arm 을 사용해야 이미지를 정상적으로 가져올 수 있다. 이제 expose 된 서비스를 확인하기 위해 textkubectl get servicesdeployment result: 를 입력해보면, 아까 배포한 hellok8s 의 포트가 8080:3{XXXX} 로 되어 있을텐데, localhost:3XXXX 로 접속해보면 image. png 와 같은 결과를 얻을 수 있다. "
    
}, 

{
    "id": 31,
    "url": "https://hoex5h.github.io//Kubernetes-_%EB%8F%84%EC%BB%A4%EB%A5%BC_%EA%B8%B0%EB%B0%98%EC%9C%BC%EB%A1%9C_%EB%A9%80%ED%8B%B0%EB%85%B8%EB%93%9C_%ED%81%B4%EB%9F%AC%EC%8A%A4%ED%84%B0%EB%A5%BC_%EA%B5%AC%EC%B6%95%ED%95%B4%EB%B3%B4%EC%9E%90",
    "title": "[Kubernetes] 도커를 기반으로 멀티노드 클러스터를 구축해보자",
    "body": "2025/01/14 - NOTE!먼저, 실습 환경은 M1 pro 맥북 프로임을 알립니다. kind 설치: 지난번에는 Docker Desktop 에서 쿠버네티스를 사용해봤는데, 그냥 Docker Desktop 를 이용하면 로컬에서는 싱글 노드로 밖에 클러스터를 구성할 수 없다고 한다. 로컬에서 여러 노드를 클러스터로 구성하려면, “kind” 를 사용해야 한다. m1 맥에서는 간단하게 brew 를 통해서 설치할 수 있다. textbrew install kindcluster 생성: kind 에서는 간단하게 textkind create clusterkind delete cluster --name { $NAME }으로 단일 노드 클러스터를 생성 및 삭제할 수 있다고 한다. 그러나 우리가 사용할 목적은 multiple node 로 구성된 클러스터이기 때문에, config yaml 파일을 하나 만들어야한다. cluster config file: yamlkind-make-own-cluster-config. yaml# four node (three workers) cluster config#  extraPortMappings  maps literally container to the host machine# On Mac, port 5000 is used by  ControlCE  which related to  AirPlay # If you want to use port 5000, turn off the  AirPlay Receive Mode  in mac os Settingkind: ClusterapiVersion: kind. x-k8s. io/v1alpha4name: klusternodes:- role: control-plane labels:  roles: master extraPortMappings: - containerPort: 5001  hostPort: 5001- role: worker labels:  roles: worker extraPortMappings: - containerPort: 5002  hostPort: 5002- role: worker labels:  roles: worker extraPortMappings: - containerPort: 5003  hostPort: 5003- role: worker labels:  roles: worker extraPortMappings: - containerPort: 5004  hostPort: 5004맥에서 5000번 포트는 Airplay 와 연관된 ControlCE 라는 놈이 이미 차지하고 있으므로, 5001번 포트부터 열었다. 만약 5000번 포트로 열고 싶으면 맥 설정에서 “공유 &gt; 시스템 환경설정 &gt; AirPlay 수신 모드 끄기” 를 하면 된다고 한다. (직접 해보지는 않았으니 반드시 확인해보고 할 것!) 나는 굳이 AirPlay 기능을 건들이고 싶지 않기 때문에 5001번 포트부터 열었다. 이후, 터미널에 textkind create cluster --config kind-make-own-cluster-config. yaml로, 위의 yaml 파일대로 클러스터룰 구성하면 된다. Result: 아래의 사진처럼 도커에 컨테이너가 생기고, 이 컨테이너들을 클러스터로 묶은 것을 알 수 있다. image. png image. png image. png "
    
}, 

{
    "id": 32,
    "url": "https://hoex5h.github.io//Kubernetes-_metric-server-setting",
    "title": "[Kubernetes] metric-server-setting",
    "body": "2025/01/14 - [Kubernetes] metric-server-settingHigh A metric server textkubectl apply -f &lt;https://github. com/kubernetes-sigs/metrics-server/releases/latest/download/high-availability-1. 21+. yaml&gt;kubectl edit deployments. apps -n kube-system metrics-server text- --kubelet-insecure-tls=true- --kubelet-preferred-address-types=InternalIP. . . .    dnsPolicy: ClusterFirst   hostNetwork: true # &lt;&lt; 추가   nodeSelector:    kubernetes. io/os: linuxsudo vim /etc/kubernetes/manifests/kube-apiserver. yaml text--enable-aggregator-routing=true"
    
}, 

{
    "id": 33,
    "url": "https://hoex5h.github.io//Kubernetes-_Node_Not_Ready",
    "title": "[Kubernetes] Node Not Ready",
    "body": "2025/01/14 - [Kubernetes] Node Not Ready컴퓨터 3대를 각각 Master 노드 1개와 Worker node 2개로 구성하고 클러스터를 만들어서 Join 시켜놓고 Calico를 설치해서 CNI 까지 설정을 완료하고 사용하고 있던 어느날, 컴퓨터를 한 번씩 재부팅 해야하는 일이 생겨서 재부팅 했었다. 재부팅하고 나서 보니까 Worker Node 들이 전부 Not Ready 가 되어있었다… Calico 도 이미 설치를 해놔서 CNI 설정 문제도 아니였는데 뭐가 문제였을까 하고 보니, Worker Node 들의 swap memory 가 켜져 있었다. . 이런일이 없도록 앞으로 추가할 Worker Node 들은 재부팅 후 swap memory 가 켜지지 않는 방향으로 ansible playbook 을 작성해야겠다. "
    
}, 

{
    "id": 34,
    "url": "https://hoex5h.github.io//%EC%BD%94%EB%94%A9_%EA%B5%90%EC%9C%A1_%EB%B4%89%EC%82%AC",
    "title": "코딩 교육 봉사",
    "body": "2025/01/14 - 코딩 교육 봉사여타 날들과 다를 바 없이 한가로이 지내던 나에게 “코딩 교육 봉사” 라는 홍보가 눈에 띄었다.  흠. . 초등학교 친구들에게 코딩 교육 봉사? 쉽지 않을까? 아무리 그래도 초등학교 교육 봉사인데. . 할 것도 별로 없을테고 한 번 해볼까? 라는 생각에 무작정 신청했다. 알고보니 시립서울청소년센터(시서청) 라는 곳에서 건대와 함께 진행하는 프로그램이였는데 초급반/중급반 으로 나누어 진행하게 되었고몇 주간 어떤 내용을 토대로 아이들을 가르칠지 회의하고 정해진 날짜인 2022년 10월 29일, 30일 / 11월 5일, 11월 6일, 총 4번에 걸쳐 수업을 진행하는 것이었다. 처음에 어떤 내용을 가르칠지 고민을 해보았는데 대부분 코딩을 해본 적이 없거나 조금 해본 수준이라고 해서 고민이 많이 되었다.  어떤 내용을 알려줘야 아이들이 흥미도 생길까? 개념만 가르쳐주면 재미가 떨어질거라 생각해서 개념 -&gt; 실습 으로 프로그램을 구성했고 이 프로그램이 이번이 처음이 아니라 이전부터 진행되어오던 프로그램이였는데, 이전부터 네이버의 ‘엔트리’ 를 활용했다고 한다. 엔트리는 스크래치랑 비슷하다고 생각하면 되는데, 코딩하는게 눈에 바로바로 보이다보니 아이들의 흥미를 유발하기에 적합해보였다. 엔트리에 대한 자세한 내용은 이쪽을 참고하기 바란다. 아무튼 우여곡절 속에 시작한 코딩교육봉사는 생각보다 어려웠다. “아이들에게는 쉬운 것만 가르쳐줘도 힘들어하지 않을까?” 라고 생각했지만 우선 초등학교 4~5학년들이 학교에서 배운 과정에 벗어나는 것을 가르치면 힘들기 때문에 최대한 벗어나지 않는 선에서 내용을 선택했다. 예를 들어, 우리에게는 집합이 매우 익숙하겠지만 초등학생들에게도 익숙할까? 이런 생각을 끊임없이 했어야 했고 정의가 너무 어려운 단어들은 쉽게 풀어쓰기도 했다. 변수가 무엇인지 설명해야했고, 왜 조건이 참일때 짜장면을 먹는지, 거짓일 때 짬뽕을 먹는지 설명해야 했다. 특히, 짜장면을 싫어하는 학생은 생각치 못한 큰 변수였다. 이렇게만 설명하면 어려운 말을 상당히 쉽게 풀어써야하고, 이해가 되지 않는 부분을 이해하기 쉽게 만들어주기만 하면 될 것 같지만 실상은 또 그렇지도 않다. 바로 이 포인트가 가장 어려웠던 것 같다.  수업에서 사용하는 단어들과 설명하는 과정은 쉬워야하지만, 막상 또 수업의 근본적인 내용은 또 쉬울 필요가 없다. 아이들이 한 번 이해하고 나면 잘 따라하기도 했고 아이들마다 이해하는데 걸리는 시간의 편차가 커서, 몇몇 아이들은 순식간에 끝내고 다음 강의를 기다리고 있었다. 그리고 대부분의 아이들이 틀에 박혀있지 않다보니 자신만의 다양한 알고리즘을 제시했고, 나는 그 알고리즘이 문제를 푸는데 있어서 크게 지장이 없다면 문제가 없으니 진행해도 될 것 같다고 얘기했다. 순수 본인의 힘으로만 생각해낸 알고리즘이 효율적이진 않을지 몰라도 작동한다는 그 뿌듯함이 이 프로그램의 목적이 아닐까 싶어서. 막상 또 이렇게 집중할때는 집중하다가 장난끼가 발동하면 못말리는게 영락없는 초등학생들이라서 나름 재밌었다. 수업하면서 제일 많이 들었던 말이  선생님 !!!!!! 이거 보세요 (해맑게 웃으며) !!!!! 였던것 같다. 그만 보여줘 자신이 만드는 프로그램을 웃기게 바꾼다든지, 일부러 버그를 만든다든지 등등 장난의 종류가 다양했다. 근데 특히 기억에 남는 친구가 하나 있었는데, 강아지와 고양이가 서로 번갈아가면서 대화하는 프로그램(코드가 순차적으로 진행됨을 설명하기 위해)을 작성하고 있었는데대부분의 아이들은 대사를 웃긴 걸로 바꿔서 만들고 있었다. 정말 원초적인 웃음코드들. . 또 다시 “선생님 이거 좀 보세요” 의 늪 속에서 헤매면서 어떤 아이의 대사를 보았는데 정말 너무 귀여웠다“안녕! 넌 이름이 뭐야? “난 누구누구야~” “우리 친하게 지내자!” 와 비슷한 대사들이였는데 이런 때 묻지 않은 순수함을 너무 오랜만에 봐서 정말 귀여웠다. 옆에 친구들이 기르는 고양이와 강아지는 “죽어라!” 하면서 둘 중에 하나는 화면 밖으로 나가고 있었다. 매번 수업이 끝나기 전에 복습 차원에서 퀴즈를 진행했는데 정말 1~2명을 제외하고는 전부 다 손을 들어서 대답하려고 했다. 저 귀여운 친구들의 마음이 변하지 않기를 바라면서도 내 자신을 돌아보게 하는 순간이였다. image. png 더 많은 사진 : 시립서울청소년센터 "
    
}, 

{
    "id": 35,
    "url": "https://hoex5h.github.io//%EB%84%A4%EC%9D%B4%EB%B2%84_%EB%B6%80%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%94%84_%EB%B2%A0%EC%9D%B4%EC%A7%81_%ED%9A%8C%EA%B3%A0",
    "title": "네이버 부스트캠프 베이직 회고",
    "body": "2025/01/14 - 이번에 처음으로 네이버 부스트 캠프 9기 베이직 과정에 참여해서 2주동안 ‘여행’을 떠났는데, 여행동안 느낀 점 몇가지를 소개해보려고 한다. 네이버 부스트 캠프살면서 첫 부트캠프가 네이버 부스트 캠프였는데, 사실 부트캠프에 대해서 잘 알고 신청한건 아니였다. 신청하게 된 계기는 node. js 를 한다는 사실에 솔깃했다. 한국에서는 스프링을 주로 사용하지만 node. js 가 소규모 프로젝트나, 마이크로서비스로 활용할 때 가치가 높다는 얘기를 들었다. 취업할 때는 무조건 스프링이 유리할 것 같아서 고민을 많이 했는데 스프링이든 자바든 하나를 정말 잘한다면 상관없지 않을까 라는 생각이 들어서 신청하게 되었다. 물론 ‘네이버’ 이름의 영향도 없지 않았다. 베이직 과정: 자바스크립트를 아주 예전에 잠깐 써본적이 있지만 제대로 알고 사용했던것도 아니였고 한참 오래전이라 적응할 수 있을까 걱정이 조금 들었다. 원래 코딩테스트도 파이썬으로 치뤘던지라 파이썬의 어마무시한 내장 라이브러리를 포기하고, 또 미세하게 다른 문법 차이들을 이겨낼 수 있을까 고민하면서 베이직 과정의 첫 발을 내딛었는데, 나같은 사람들이 많다고 판단했는지 베이직은 시작부터 어렵진 않고 점진적으로 난이도를 올리는듯한 느낌이 들었다. 그치만 베이직이 마냥 쉬웠냐고 물어본다면 그렇다고 할 수도 없었다. 여행?: 네부캠 과정을 진행하면서 네부캠의 코딩 미션은 마치 여행 과도 같다 라는 얘기를 들었다. 각자 자신들의 방법을 토대로 자유롭게 설계해서 떠나는 여행! 근데 원래 여행이란게 계획을 아주 빡빡하게 세워서 가는 사람도 있을테고 느슨하게 계획을 세우는 사람도 있을 것이다. 나는 역시나 후자에 훨씬 가깝다. 기존에 코드 짜면서 설계를 안하는 것은 아니지만 머릿속에서 굴려보는 정도의 아주 느슨한 설계를 한다. 여행지에 가서 이것저것 부딪혀보면서 원래 가려던 식당이 문을 닫았으면 그 자리에서 괜찮은 식당을 찾아 들어가는 것처럼, 머릿 속의 느슨한 설계가 잘 안맞아떨어지면 코드를 짜면서 바꿔버리는 그런 여행을 떠났었다. 베이직에서 설계의 완성도를 얼마나 요구했던건지는 잘 모르겠지만 설계의 중요성을 깨닫게 해주려고 했던 것은 확실한 것 같다. 하다못해 나처럼 머리속에서 자유여행 떠나는 사람들에게만큼은, 적어도 어딘가에 작성하면서 여행을 떠나보는건 어떨까 라고 말해주는 것 같았다. 그리고 다른 사람들은 어떻게 여행을 떠났나 깃허브에 들어가서 구경하면서 다양한 방식의 코드들을 접했고 좋아보이는건 다음 미션이나 다른 코드들에 써먹어보았다. 이렇게 열심히 베이직 과정을 수료했지만, 아직까지도 얼마나 깊은 설계를 하고 시작해야하는지는 여전히 너무 어렵다. 설계가 완벽하기도 힘들고 중간에 수정해야 할 일들이 분명 있을 것이다. 하지만 베이직을 들으면서 예전의 ‘어차피 중간에 수정할거면 설계를 열심히 해야하나?’ 라는 생각보다는, 설계대로 맞아 떨어지는 것의 성취감과 설계대로 코드를 짜면서 방향성을 잃지 않는 방법을 배운 것 같다. 앞으로 코드 짤 때마다 설계의 깊이는 다르겠지만 설계는 하고 가야한다! 라는 경험을 하게 된 베이직 과정이었다. 앞으로: 네부캠 1,2,3차 중에 몇 차로 마무리하게 될 지는 모르겠다. 그렇지만 네부캠이든 개인 프로젝트든, 아니면 회사의 업무든 설계에 있어서 최선을 다하고 동료들의 코드를 차분히 읽어보면서 나보다 더 나은 점은 가져오고 또 내가 그 동료에게 도움이 될 수 있다면 도와줄 수 있는 그런 코딩생활을 하고 싶다. 네부캠 베이직 과정을 수료해보니까 챌린지는 더더욱 어려워보이고 멤버쉽은 말할 필요도 없이 힘들어 보인다. 앞으로 남은 테스트에서 어떻게 될지는 모르겠지만 열심히 해보자~ image. png "
    
}, 

{
    "id": 36,
    "url": "https://hoex5h.github.io//Kotlin-_notifyItemRemoved(position)",
    "title": "[Kotlin] notifyItemRemoved(position)",
    "body": "2025/01/14 - notifyItemRemoved(position)RecyclerView 에서 특정 position 에 해당하는 item 만 제거할 때, kotlinnotifyItemRemoved(position)을 사용하는데, 만약 여러 개의 리스트에서 해당 함수를 사용할 때마다 position 값이 변경되어야 한다면 문제가 발생할 수 있다. kotlinnotifyItemRemoved(0)을 했다면 position 이 0 인 item 은 RecyclerView 에서 삭제될 것이지만 그렇다고 position 이 1이였던 item의 position 이 0이 되지 않는다. 만약 position 의 수정까지 해야한다면 kotlinnotifyItemRangeChanged(position, getItemCount() - position);을 통해 position 값을 변경해줄 수 있다. "
    
}, 

{
    "id": 37,
    "url": "https://hoex5h.github.io//Javascript-_%EC%9E%90%EB%B0%94%EC%8A%A4%ED%81%AC%EB%A6%BD%ED%8A%B8%EC%97%90%EC%84%9C_Full_Screen_%EC%A7%84%EC%9E%85",
    "title": "[Javascript] 자바스크립트에서 Full Screen 진입",
    "body": "2025/01/14 - [Javascript] 자바스크립트에서 Full Screen 진입게임에서 처음 시작하면 자동으로 전체화면이 되는걸 구현해보려고 javascriptdocument. documentElement. requestFullscreen()전체화면을, javascript$(document). ready()웹페이지가 로딩이 되고 난 후에 자동으로 전체화면이 되도록 코드를 짜봤다. 결과는 실패였다. 검색을 해보니 보안 상의 이유로 유저의 트리거가 없는 한, 자동으로 전체화면은 불가능하다고 한다. 유저의 트리거를 강제로 써야했기 때문에, 유저가 게임 시작 버튼(트리거)를 누르는 순간 전체화면으로 바꾸는 방향으로 노선을 틀었으나 또 하나의 문제가 있었다. 화면의 크기가 달라지는 순간, 맵의 크기도 달라져야 하기 때문에 전체화면이 된 후에, 맵의 사이즈를 결정할 수 있다. javascriptdocument. documentElement. requestFullscreen()requestFullscreen 는 javascriptPromisePromise, 즉 비동기로 실행이 되는데 then 과 catch 를 사용해서 resolve 를 실행해보아도, 이 requestFullScreen 의 로직이 독특한게 전체화면시키고자 하는 Element을 전체화면으로 만들 수 있는지 여부를 체크해서 resolve 와 reject 로 return 되고, 그 뒤에 element를 전체화면으로 만드는 것 같았다. 다시 말해서, 해당 element가 전체화면으로 만들 수 있는 것 까지만 Promise 이고, elment가 전체화면이 되기 전에 then 으로 넘어가버린다. . 나는 window 의 resize 이벤트를 통해서 위 문제를 해결할 수 있었는데, resize가 되는 순간에 맵의 크기도 resize 시키는 방식으로 해결했다. "
    
}, 

{
    "id": 38,
    "url": "https://hoex5h.github.io//Javascript-_Lexical_This",
    "title": "[Javascript] Lexical This",
    "body": "2025/01/14 - [Javascript] Lexical This이전 문서 Lexical_Scope AND Scope Chain 에서 렉시컬 스코프와 스코프 체인에 대해 알아보았다. 이번엔 내가 겪었던 문제로 돌아가서 이를 해결해보자. javascriptvar testNum = 100;var lexicalTest = {  testNum : 0,  thisTest1: function(){    console. log(this. testNum);  },  thisTest2 : ()=&gt;{    console. log(this. testNum);  },};lexicalTest. thisTest1();lexicalTest. thisTest2();위의 코드에서 thisTest1 과 thisTest2 는 같은 testNum을 참조할까? lexicalTest 라는 객체 리터럴의 testNum을 참조하지 않을까? 설마 혹시나 lexical Scope 처럼 상위 스코프를 참조해서 전역변수 testNum을 참조해버리는 건 아닐까? 우선 function으로 선언된 thisTest1 을 일반적인 함수, 화살표함수로 선언된 thisTest2를 화살표함수라고 부르기로 하자. 일반적인 함수와 화살표 함수에서의 this 는 사뭇 다르다. this는 애초에 변수가 아니라 흔히들 문맥 이라고 표현한다. 어떤 상황(문맥)에서 쓰였는지에 따라 달라진다는 의미인 듯 하다. 앞으로 this가 바인딩된다는 표현을 할텐데, this를 문맥과 엮는다는 느낌으로 보면 쉬울 것이다.  일반적인 함수에서 this와 문맥은 어떻게 엮이는지 살펴보자. 일반적인 함수에서는 두 가지로 나뉘는데,  [생성자 함수 / 객체 메소드] : 객체에 바인딩 [1을 제외한 모든 함수(내부함수, 콜백함수 …)] : 전역객체(window)에 바인딩lexicalTest 라는 객체 리터럴을 생성할 때 일반적인 함수(thisTest1)는 생성자 함수 안에 있는 함수이므로 객체(lexicalTest)에 바인딩 되어 있고 this. testNum은 lexicalTest. testNum과 동일하다.  이번엔 화살표 함수를 살펴보자. 이전에 Lexical Scope 와 Lexical this 를 얘기한건 사실 화살표 함수를 얘기하기 위해서 였다. 화살표 함수의 this 는 Lexical this 라고 불린다.  Lexical Scope가 그랬듯이, 이 this도 상위 객체를 참조 한다 는 뜻이다. 다만 이번에는 상위 스코프가 아니라 상위 객체 임을 알아둬야 한다. 모든 객체의 최종 상위 객체는 window 객체이고 전역 변수가 있듯이 window 는 전역 객체 이다. javascriptfunction upperInstance(){  this. lexical = 0;  return {    lexical : 1,    normalLog : function(){      console. log(this. lexical);    },    arrowLog : ()=&gt;{      console. log(this. lexical);    }  }}upperInstance(). normalLog();upperInstance(). arrowLog();위 코드에서 normalLog()는 1이, arrowLog()는 0이 나온다는걸 이해하면 Lexical this는 이해된 것이라 생각한다. lexical Scope가 상위 스코프를 참조한다는걸 생각해보면, lexical this 도 마찬가지 아닐까? "
    
}, 

{
    "id": 39,
    "url": "https://hoex5h.github.io//Javascript-_Lexical_Scope",
    "title": "[Javascript] Lexical Scope",
    "body": "2025/01/14 - [Javascript] Lexical Scope문제의 발단: 자바스크립트로 아이작과 비슷한 게임을 만들던 도중 화살표함수에서 $(this) 사용이 이상하다고 느꼈는데, 아래와 같은 코드를 짜고 실행시켰더니 javascript$(`#character`). fadeOut(2000,  swing , ()=&gt;{ console. log($(this)); // 이 부분에서 $(this)를 콘솔에 찍어보았더니});javascript $(window)윈도우 객체가 콘솔에 찍혔다. 그저 function(){} 을 간단히 쓰는 느낌은 아닌것이었다. . 화살표 함수에 대해 알아보기전에, Lexical Scope 를 먼저 알아야 할 것 같아서 화살표 함수는 이 뒤에 다루기로 하고, 먼저 렉시컬 스코프 (또는 Static Scope)에 대해 먼저 알아보자. 결론부터 미리!: 미리 얘기해두자면, 자바스크립트는 기본적으로 Lexical Scope 를 따른다. Lexical Scope 정의:  Lexical Scope(혹은 Static Scope) : 함수를 어디서 선언하였는지에 따라 상위 스코프를 결정한다 Scope 정의:  Scope : 참조 대상 식별자(변수,함수의 이름과 같이 어떤 대상을 다른 대상과 구분하여 식별할 수 있는 유일한 이름)를 찾아내는 규칙. 함수가 선언될 때 Scope가 생긴다! Sample Code: javascriptvar str = '1';var text = 'global';function consoleLog1(){  console. log(str);}function upperScope(){  var str = '2';  consoleLog1();  function consoleLog2(){    console. log(str);    console. log(text);  }  consoleLog2();}upperScope();코드 설명: 위의 코드를 설명해보자면, 전역변수 str 은 ‘1’ 로 선언되어 있다. str을 출력하는 함수는 consoleLog1과 consoleLog2가 있는데, consoleLog2 함수는 upperScope 라는 함수 안에 있다. 두 번째 전역변수 text를 보면, upperScope 함수 안에 있는 consoleLog2 에서 출력하게 되어 있다. 우선 위에도 써놨듯이, Scope는 함수가 호출될 때가 아니라 선언될 때 생긴다. 라는 걸 머릿 속에 넣어두고 넘어가자. 자바스크립트가 Lexical Scope 를 따른다고 말한건, 함수가 선언 되었을 때 우선 탐색하는 스코프는 상위스코프 라는 소리다. consoleLog1 함수를 보자. 선언되는 상황을 보면 상위 스코프에 (전역변수) str(1) 이 존재한다. 따라서 consoleLog1 이 upperScope 에서 실행된다고 한들, 전역변수인 str(1) 을 참조하는 것이다. 두 번째로 upperScope 안에 있는 consoleLog2 를 보자. 선언 된 순간을 보면 consoleLog2의 바로 상위 스코프인 upperScope 에 존재한다. 따라서 upperScope의 지역변수로 선언된 str(2)를 참조한다. 이번엔 두 번째 전역변수 text를 살펴보자. consoleLog2에서 text를 출력하려고 보니, 바로 상위스코프인 upperScope 에는 text라는 변수가 없다. 혹시 그럼 undefined를 출력할까? 아니다. 바로 위 스코프에 없다면, 그 위에 스코프를 탐색하고, 해당 변수를 찾을때까지 위의 위 스코프를 탐색한다. 그럼에도 찾지 못한다면 undefined가 될 것이다. text 함수는 전역변수로 선언되어 있기 때문에 upperScope 에서는 찾지 못했어도 전역스코프에서는 찾을 수 있다. 따라서 ‘global’ 이란 텍스트를 출력할 것이다. Scope Chain:  이렇게 꼬리를 물며 위 스코프를 탐색하는 방법을 ‘스코프 체인’ 이라고 한다. 이제 다음 문서에 lexical this 를 설명해보려고 한다. lexical Scope가 상위 스코프를 참조한다는걸 생각해보면, lexical this 도 마찬가지 아닐까? "
    
}, 

{
    "id": 40,
    "url": "https://hoex5h.github.io//Javascript-_Deep_Copy,_Swallow_Copy",
    "title": "[Javascript] Deep Copy, Swallow Copy",
    "body": "2025/01/14 - [Javascript] Deep Copy, Swallow Copy자바스크립트로 게임을 만들면서 캐릭터 객체가 들고 있는 방패 너머로 몬스터가 넘어오지 못하게 막는 방법을 구현하고 있었다. 이전에 방해물(Obstruct)클래스를 따로 만들어두었는데 방패도 몬스터의 방해물에 속하기 때문에, Obstruct 클래스에 방패도 프로퍼티로 넣어주었다. 이때 방패와 벽과 같은 방해물과의 다른 점이 있었는데, 방패는 캐릭터가 움직이면 같이 움직이는 방해물이고 벽은 움직이지 않는 방해물이었다. (후에 움직이는 벽을 만들거지만, 일단은 지금은 맵을 감싸는 벽이기 때문) javascript if( monsterObstacle. getStuck(monster. coordinates) )  continue; monster. move();코드를 간략하게 설명하면 이런 식으로, 몬스터의 좌표를 인자르 넘겨 확인해서 방해물에 걸리지 않는다면 움직이도록 구현되었는데 javascriptmonsterObstacle = { this. arr = [Wall, Shield];}몬스터의 방해물에는 현재까지 움직이지 않는 벽과 움직이는 방패가 있다. 여기서 Wall과 Shield의 좌표값이 주어지는데, 방패의 좌표값은 캐릭터가 움직일 때 마다 바뀌어야하는 것이 문제였다. 처음에는 방패라는 객체를 만들지않고 방패의 좌표값을 업데이트 시키는 방식을 택했었는데, 터무니 없는 방법이란걸 깨닫는데 10분도 걸리지 않았다. 자바스크립트에서 객체를 변수에 할당하면, 해당 객체의 참조값이 할당되는 얕은복사(자바스크립트에서는 얕은복사와 깊은복사라는 말을 잘 쓰지 않는다고 들었지만)를 선택하는 것이 더 좋은 판단이라고 생각했고 javascriptclass Shield{ this. #coordinates = {  x : 0,  y : 0, }}이런식으로 방패 객체를 만들어서 Monster Obstacle의 프로퍼티로 넣어주었더니, 캐릭터의 움직임에 따라 방해물인 방패도 움직이는 코드를 짤 수 있었다. C나 C++ 처럼 메모리를 직접 건들 수 없다는건 단점처럼 보이기도하고 장점처럼 보이기도 하는 것 같다. "
    
}, 

{
    "id": 41,
    "url": "https://hoex5h.github.io//Flutter-_VerifyReleaseResources_Error",
    "title": "[Flutter] VerifyReleaseResources Error",
    "body": "2025/01/14 - [Flutter] VerifyReleaseResources ErrorFlutter 를 활용한 채팅(상담) 어플을 만들고 있었는데, APK 빌드를 하려고 하니까 아래와 같은 오류가 발생했다. ($ flutter build apk –release) textFAILURE: Build completed with 2 failures. 1: Task failed with an exception. -----------* What went wrong:Execution failed for task ':keyboard_visibility:verifyReleaseResources'. &gt; A failure occurred while executing com. android. build. gradle. internal. tasks. Workers$ActionFacade  &gt; Android resource linking failed   /Users/hoeeeeeh/. gradle/caches/transforms-2/files-2. 1/c82bf40584b9a53c31dba07a8a48f301/core-1. 1. 0/res/values/values. xml:142:5-173:25: AAPT: error: resource android:attr/fontVariationSettings not found.   /Users/hoeeeeeh/. gradle/caches/transforms-2/files-2. 1/c82bf40584b9a53c31dba07a8a48f301/core-1. 1. 0/res/values/values. xml:142:5-173:25: AAPT: error: resource android:attr/ttcIndex not found. * Try:Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. Run with --scan to get full insights. ==============================================================================2: Task failed with an exception. -----------* What went wrong:Execution failed for task ':safe_area_height:verifyReleaseResources'. &gt; A failure occurred while executing com. android. build. gradle. internal. tasks. Workers$ActionFacade  &gt; Android resource linking failed   /Users/hoeeeeeh/. gradle/caches/transforms-2/files-2. 1/c82bf40584b9a53c31dba07a8a48f301/core-1. 1. 0/res/values/values. xml:142:5-173:25: AAPT: error: resource android:attr/fontVariationSettings not found.   /Users/hoeeeeeh/. gradle/caches/transforms-2/files-2. 1/c82bf40584b9a53c31dba07a8a48f301/core-1. 1. 0/res/values/values. xml:142:5-173:25: AAPT: error: resource android:attr/ttcIndex not found. * Try:Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. Run with --scan to get full insights. ==============================================================================keyboard_visibility 패키지와 safe_area_height 패키지에서 verifyReleaseResources 를 실패했다고 하는데, android/build. gradle 에 아래와 같은 코드를 넣어서 해결했다.  android/build. gradle dartsubprojects {  afterEvaluate { project -&gt;    if (project. name ==  keyboard_visibility  || project. name ==  safe_area_height ) {      android {        compileSdkVersion 28      }    }  }}gradle build lifecycle 중에 afterEvaluate 에서 사용할 버전을 맞추는 것이라고 한다. "
    
}, 

{
    "id": 42,
    "url": "https://hoex5h.github.io//ArgoCD_RBAC",
    "title": "ArgoCD RBAC",
    "body": "2025/01/14 - [ArgoCD] RBACArgocd 공식문서 Argocd RBAC: 졸업프로젝트로 쿠버네티스와 CD tool 로 Argocd 를 선택했다. Argocd 설치를 끝마치고, admin 계정의 초기 비밀번호까지 바꿔준 후에 간단하게 application 을 만들어보려고 했는데, Permission Denied 가 떴다. 아니 어째서? 알고보니 Argocd 도 쿠버네티스의 Service Account 처럼 RBAC 로 role 을 부여해야만 했던 것이었다. 그래서 우선 Argocd 의 RBAC config 파일을 불러왔다. textkubectl get configmap argocd-rbac-cm -n argocd -o yaml &gt; argocd-rbac. yml이후에 argocd-rbac. yml에 아래 권한들을 추가해 주었다. yamlapiVersion: v1data: policy. csv: |  # Built-in policy which defines two roles: role:readonly and role:admin,  # and additionally assigns the admin user to the role:admin role.   # There are two policy formats:  # 1. Applications, logs, and exec (which belong to a project):  # p, &lt;user/group&gt;, &lt;resource&gt;, &lt;action&gt;, &lt;project&gt;/&lt;object&gt;  # 2. All other resources:  # p, &lt;user/group&gt;, &lt;resource&gt;, &lt;action&gt;, &lt;object&gt;  p, role:readonly, applications, get, */*, allow  p, role:readonly, certificates, get, *, allow  p, role:readonly, clusters, get, *, allow  p, role:readonly, repositories, get, *, allow  p, role:readonly, projects, get, *, allow  p, role:readonly, accounts, get, *, allow  p, role:readonly, gpgkeys, get, *, allow  p, role:readonly, logs, get, */*, allow  p, role:admin, applications, create, */*, allow  p, role:admin, applications, update, */*, allow  p, role:admin, applications, delete, */*, allow  p, role:admin, applications, sync, */*, allow  p, role:admin, applications, override, */*, allow  p, role:admin, applications, action/*, */*, allow  p, role:admin, applicationsets, get, */*, allow  p, role:admin, applicationsets, create, */*, allow  p, role:admin, applicationsets, update, */*, allow  p, role:admin, applicationsets, delete, */*, allow  p, role:admin, certificates, create, *, allow  p, role:admin, certificates, update, *, allow  p, role:admin, certificates, delete, *, allow  p, role:admin, clusters, create, *, allow  p, role:admin, clusters, update, *, allow  p, role:admin, clusters, delete, *, allow  p, role:admin, repositories, create, *, allow  p, role:admin, repositories, update, *, allow  p, role:admin, repositories, delete, *, allow  p, role:admin, projects, create, *, allow  p, role:admin, projects, update, *, allow  p, role:admin, projects, delete, *, allow  p, role:admin, accounts, update, *, allow  p, role:admin, gpgkeys, create, *, allow  p, role:admin, gpgkeys, delete, *, allow  p, role:admin, exec, create, */*, allow  g, admin, role:admin  g, admin-k8s, role:adminadmin 과 read-only 권한을 정의하고, admin 과 admin-k8s 에 admin 권한을 부여한다 이후 적용해주면 된다. textkubectl apply -f argocd-rbac. yml"
    
}, 

{
    "id": 43,
    "url": "https://hoex5h.github.io//ArgoCD_Getting_Started",
    "title": "ArgoCD Getting Started",
    "body": "2025/01/14 - [Argocd] Getting StartedArgocd 공식문서 Install Argo CD: 졸업프로젝트로 k8s 를 써보면서, CD tool 로 Argocd 를 사용해보려고 한다. 공식문서를 참고해서 설치해보자. textkubectl create namespace argocdkubectl apply -n argocd -f &lt;https://raw. githubusercontent. com/argoproj/argo-cd/stable/manifests/install. yaml&gt;설치가 잘 되었는지 확인해보자. textkubectl get all -n argocd 결과 . . . pod/argocd-redis-774dbf45f8-mvw4n            1/1   Running  0     4m56spod/argocd-repo-server-76d7968f94-j99cw         1/1   Running  0     4m56spod/argocd-server-5db7487c98-ljqhr           1/1   Running  0     4m56s. . . NAME                       TYPE    CLUSTER-IP    EXTERNAL-IP  PORT(S)           AGEservice/argocd-server               ClusterIP  10. 96. 132. 227  none    80/TCP,443/TCP        4m57s. . . 여기서 우리가 봐야할 것은 service/argocd-server 이다. TYPE이 ClusterIP 라서 External-IP도 없고, 외부로 expose 도 되어 있지 않다. 공식 문서에서는 타입을 Load Balancer 로 바꾸거나, Ingress 설정을 하거나, 포트 포워딩을 하라고 하고 있다. 여기선 제일 간단하게 타입을 로드 밸런서로 바꿔보자. kubectl patch svc argocd-server -n argocd -p '{ spec : { type :  LoadBalancer }}' 혹시 여기서, External ip 가 pending 되고 있다면 on-premise 환경에서 사용할 loadbalancer 가 필요하므로, MetalLB 같은 것을 설치해야한다. Install Metallb Argocd CLI 설치: textcd /usr/local/binsudo curl -sSL -o argocd-linux-amd64 &lt;https://github. com/argoproj/argo-cd/releases/latest/download/argocd-linux-amd64&gt;sudo install -m 555 argocd-linux-amd64 /usr/local/bin/argocdsudo rm argocd-linux-amd64Argocd 초기 비밀번호 확인: textkubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath= {. data. password}  | base64 -d; echoargocd cli가 깔려있다면, 아래의 커맨드로도 확인할 수 있다. textargocd admin initial-password -n argocdArgocd 로그인 및 비밀번호 변경: textargocd login &lt;ARGOCD_SERVER ip&gt;argocd account update-passwordusername = admin passwd = 위에서 설명했던 초기 비밀번호 로 로그인 하면 된다. "
    
}, 

{
    "id": 44,
    "url": "https://hoex5h.github.io//ArgoCD_Create_Application",
    "title": "ArgoCD Create Application",
    "body": "2025/01/14 - [Argocd] Create ApplicationCreate Application Error: 어플리케이션 등록하면서 겪었던 에러들 위주로 작성해보려고 한다. 추후 option 들에 대해서도 공부하고 작성해야겠다. PATH absolute error: Path 에 / 를 입력하면 발생하는 에러이다. 절대경로로 설정하면 안되는 것 같다. Permission Denied: 현재 사용중인 account 에 권한이 없을 때 나타나는 에러이다. argocd RBAC 권한 을 참조해서 권한 설정을 하자  -2023-09-26 추가Application 을 생성할 때 Project Name 을 선택하게 되는데, 정말 IDE 열어서 프로젝트 이름 짓는거 마냥 지어버리면 Permission Denied 가 뜬다…!! Argocd 에 Project 라는 항목이 존재 하는데, 이 항목을 고르는거라서 존재하지 않는 Project Name 을 써버리면 Permission Denied 로 튕겨내버린다. . RBAC로 아무리 권한을 부여해도 뜬다. 에러를 조금만 더 친절하게 써줬으면 좋지 않았을까? 하루종일 RBAC 문제인 줄 알았다. Repo URL error: 없는 레포나, private 레포일 경우 발생한다. private 레포의 경우 따로 credential 이 필요하다. Failed to unmarshal jsconfig. json:: k8s 와 관련 없는 manifest 들을 읽으려고 할 때 발생하는 문제이다. SOURCE 의 repo 와 path 를 잘 지정해서, k8s 에 필요한 yaml 파일들만 읽도록 하면 된다. 즉, k8s 에서 필요한 pod, deploy, svc. . 등등의 yaml 을 한 폴더에 넣어두고, 해당 폴더의 경로를 Path 에 지정해주면 된다. "
    
}, 

{
    "id": 45,
    "url": "https://hoex5h.github.io//Ansible_%EC%9D%B8%EB%B2%A4%ED%86%A0%EB%A6%AC_%EB%A7%8C%EB%93%A4%EA%B8%B0",
    "title": "Ansible 인벤토리 만들기",
    "body": "2025/01/14 - [Ansible] 앤서블 인벤토리 만들기Ansible: Ansible 공식 문서 Building Ansible Inventories 앤서블에는 앞선 포스팅에서 언급했던 앤서블을 실행하는 제어 노드(Control Node) 와, 앤서블로 관리할 매니지드 노드(Managed Node) 외에도 여러가지 개념들이 존재하는데, 이번에 자세히 다룰 개념은 인벤토리 이다.  Inventory : 매니지드 노드들의 목록을 인벤토리(Inventory) 라고 하는데, 각 매니지드 노드들에 대한 IP, 호스트 정보, 변수와 같은 정보를 저장하고 있다. 우선 hosts. ini 를 작성해보자 text[Worker Nodes]192. 168. 0. 11192. 168. 0. 12현재 Kluster 에서 사용중인 Worker Nodes 2개의 고정 ip 값을 적어 주었다. 이렇게 적는 방법 외에도, 마치 python 의 index slicing 처럼 적는 방법이 있다고 한다. text[Worker Nodes]192. 168. 0. [11:50]이렇게 하면 xxx. xxx. xxx. 11 ~ xxx. xxx. xxx. 50 까지가 포함된다. "
    
}, 

{
    "id": 46,
    "url": "https://hoex5h.github.io//Ansible_%EC%8B%9C%EC%9E%91%ED%95%98%EA%B8%B0",
    "title": "Ansible 시작하기",
    "body": "2025/01/14 - [Ansible] 앤서블 시작하기Ansible: Ansible 공식 문서  Ansible is an IT automation tool. It can configure systems, deploy software,and orchestrate more advanced IT tasks such as continuous deployments or zero downtime rolling updates. Ansible은 공식 홈페이지의 설명대로 CD(continous deployment) 혹은 실시간 롤링 업데이트와 시스템 및 소프트웨어 배포 등을 위한 자동화 도구이다. 클러스터를 구축하는데 Ansible 을 쓰기로 결정한 이유는, 클러스터에 새로운 노드들이 추가될 때마다 매번 셋팅 후 join 시킬 수는 없기 때문이다. 앞으로 무언가를 추가할 때마다 새로운 노드를 셋팅하는 과정이 복잡해지고 시간이 오래 걸리기 때문에 ansible 을 통해 자동화 하는 방법을 택했다. Ansible은 관리 되는 노드인 Managed node 와, Ansible 을 직접 실행하며 Managed node 를 관리하는 Control Node 로 나뉜다. Control Node 와 Managed Node 는 모두 파이썬을 필요로 하며, Managed Node 는 ssh 가 허용되어 있어야 한다. Ansible 은 light 버전인 Ansible-core 와, Ansible 로 나뉜다. Kluster 에는 우선 Ansible 을 설치할 것이다. 어느 노드를 Control Node 로 선택할 지 고민을 좀 했는데, 클러스터의 마스터 노드에 직접 Ansible 을 설치하는 것 보다 기존에 Worker Node 중에서 Jenkins 서버를 설치했던 노드에 통합하는 것이 보안 등 여러가지 이유로 좋을 것 같아 Worker Node3(zenkins) 에 설치하기로 했다. 공식 홈페이지에서는 pip 를 이용하여 Ansible 을 설치하는 것을 권장하는 것 같다. 우선 pip 가 설치되어 있는지 확인해보자 text$ python3 -m pip -V $ /usr/bin/python3: No module named pip 파이썬 설치가 안되어있으니 빠르게 python 을 설치하자. text$ curl &lt;https://bootstrap. pypa. io/get-pip. py&gt; -o get-pip. py$ python3 get-pip. py --user pip 23. 2 from /home/zenkins/. local/lib/python3. 10/site-packages/pip (python 3. 10) 다시 한 번 pip 버전을 확인해보니 잘 나오는 모습을 확인할 수 있다. 이제 Ansible 을 설치하자. text$ python3 -m pip install --user ansible$ python3 -m pip install --user ansible-core==2. 12. 3ansible 과 ansible-core 중에 선택해서 하나만 설치하면 된다. 위에서 언급했듯이 우리는 ansible 을 먼저 사용해보고 lightweight 버전을 사용할 수 있으면 다운그레이드 하려고 한다. 실행 결과Collecting ansibleObtaining dependency information for ansible from [https://files. pythonhosted. org/packages/01/43/b5620e57e14a84d9bf257ba4797c2abfa13f7c3be04bc8cd35f86d2e6a2d/ansible-8. 1. 0-py3-none-any. whl. metadata](https://files. pythonhosted. org/packages/01/43/b5620e57e14a84d9bf257ba4797c2abfa13f7c3be04bc8cd35f86d2e6a2d/ansible-8. 1. 0-py3-none-any. whl. metadata)Downloading ansible-8. 1. 0-py3-none-any. whl. metadata (7. 9 kB)Collecting ansible-core~=2. 15. 1 (from ansible)Obtaining dependency information for ansible-core~=2. 15. 1 from [https://files. pythonhosted. org/packages/80/71/d2582d918dda323ddf998f8e4bdcc35c5b7fabea27917d480670186d3f39/ansible_core-2. 15. 1-py3-none-any. whl. metadata](https://files. pythonhosted. org/packages/80/71/d2582d918dda323ddf998f8e4bdcc35c5b7fabea27917d480670186d3f39/ansible_core-2. 15. 1-py3-none-any. whl. metadata)Downloading ansible_core-2. 15. 1-py3-none-any. whl. metadata (7. 5 kB)Requirement already satisfied: jinja2&gt;=3. 0. 0 in /usr/lib/python3/dist-packages (from ansible-core~=2. 15. 1-&gt;ansible) (3. 0. 3)Requirement already satisfied: PyYAML&gt;=5. 1 in /usr/lib/python3/dist-packages (from ansible-core~=2. 15. 1-&gt;ansible) (5. 4. 1)Requirement already satisfied: cryptography in /usr/lib/python3/dist-packages (from ansible-core~=2. 15. 1-&gt;ansible) (3. 4. 8)Collecting packaging (from ansible-core~=2. 15. 1-&gt;ansible)Downloading packaging-23. 1-py3-none-any. whl (48 kB)━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 48. 9/48. 9 kB 579. 9 kB/s eta 0:00:00Collecting resolvelib&lt;1. 1. 0,&gt;=0. 5. 3 (from ansible-core~=2. 15. 1-&gt;ansible)Downloading resolvelib-1. 0. 1-py2. py3-none-any. whl (17 kB)Downloading ansible-8. 1. 0-py3-none-any. whl (44. 8 MB)━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 44. 8/44. 8 MB 1. 1 MB/s eta 0:00:00Downloading ansible_core-2. 15. 1-py3-none-any. whl (2. 2 MB)━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2. 2/2. 2 MB 1. 1 MB/s eta 0:00:00DEPRECATION: distro-info 1. 1build1 has a non-standard version number. pip 23. 3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of distro-info or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at [https://github. com/pypa/pip/issues/12063](https://github. com/pypa/pip/issues/12063)DEPRECATION: python-debian 0. 1. 43ubuntu1 has a non-standard version number. pip 23. 3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of python-debian or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at [https://github. com/pypa/pip/issues/12063](https://github. com/pypa/pip/issues/12063)Installing collected packages: resolvelib, packaging, ansible-core, ansibleWARNING: The scripts ansible, ansible-config, ansible-connection, ansible-console, ansible-doc, ansible-galaxy, ansible-inventory, ansible-playbook, ansible-pull and ansible-vault are installed in '/home/zenkins/. local/bin' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location. WARNING: The script ansible-community is installed in '/home/zenkins/. local/bin' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location. Successfully installed ansible-8. 1. 0 ansible-core-2. 15. 1 packaging-23. 1 resolvelib-1. 0. 1Ansible 을 업그레이드 하기 위해서는 단순히 --upgrade 만 붙이면 된다고 한다 text$ python3 -m pip install --upgrade --user ansibleansible-core 버전 확인은 text$ ansible --versionansible 버전 확인은 textpython3 -m pip show ansible을 통해서 하면 된다. "
    
}, 

{
    "id": 47,
    "url": "https://hoex5h.github.io//Ansible_%EC%84%A4%EC%A0%95%ED%95%98%EA%B8%B0",
    "title": "Ansible 설정하기",
    "body": "2025/01/14 - [Ansible] 앤서블 설정하기Ansible: Ansible 공식 문서 ansible. cfg 파일을 수정하여 여러가지 설정을 하려고 할 때, pip 로 설치한 경우 공식 문서에서 example file 을 제공한다길래 들어가봤더니 파일이 삭제되어 있었다…  Starting with Ansible version 2. 4, you can use the ansible-config command line utility to list your available options and inspect the current values. 아무래도 ansible 2. 4 이후로는 ansible-config 커맨드 라인을 통해 설정을 하는 것 같다. 2023년 7월달 기준으로 ansible 이 8. 1 버전인걸 보아하니, 아마도 한참 전에 바뀌었나보다. Ansible 은 아래와 같은 순서대로 cfg 파일을 찾는다고 하는데, 우선 /etc/ansible 에 간단한 sample cfg 파일을 생성하기로 했다.  ANSIBLE_CONFIG (env 변수가 설정되어 있을 경우) ansible. cfg (현재 디렉토리에서) ~/. ansible. cfg (home 디렉토리에서) /etc/ansible/ansible. cfg (Default 값 인듯) sample ansible. cfg 파일을 생성 text$ ansible-config init --disabled &gt; ansible. cfg$ ansible-config init --disabled -t all &gt; ansible. cfg명령어 둘 중에 하나를 선택하면 되는데, 둘 다 주석으로 자세히 설명해둔 sample config 파일을 만들어준다고 하고 아래의 명령어가 현재 존재하는 plugin 까지 포함하는 방법이라고 한다. 꽤나 많은 옵션이 존재해서, 필요할 때마다 수정해야겠다. 공식 문서 옵션 설명 ansible. cfg[ter!efaults]# (boolean) By default Ansible will issue a warning when received from a task action (module or action plugin)# These warnings can be silenced by adjusting this setting to False. ;action_warnings=True# (list) Accept list of cowsay templates that are 'safe' to use, set to empty list if you want to enable all installed templates. ;cowsay_enabled_stencils=bud-frogs, bunny, cheese, daemon, default, dragon, elephant-in-snake, elephant, eyes, hellokitty, kitty, luke-koala, meow, milk, moofasa, moose, ren, sheep, small, stegosaurus, stimpy, supermilker, three-eyes, turkey, turtle, tux, udder, vader-koala, vader, www# (string) Specify a custom cowsay path or swap in your cowsay implementation of choice;cowpath=# (string) This allows you to chose a specific cowsay stencil for the banners or use 'random' to cycle through them. ;cow_selection=default# (boolean) This option forces color mode even when running without a TTY or the  nocolor  setting is True. ;force_color=False# (path) The default root path for Ansible config files on the controller. ;home=~/. ansible# (boolean) This setting allows suppressing colorizing output, which is used to give a better indication of failure and status information. ;nocolor=False# (boolean) If you have cowsay installed but want to avoid the 'cows' (why????), use this. ;nocows=False# (boolean) Sets the default value for the any_errors_fatal keyword, if True, Task failures will be considered fatal errors. ;any_errors_fatal=False# (path) The password file to use for the become plugin. --become-password-file. # If executable, it will be run and the resulting stdout will be used as the password. ;become_password_file=# (pathspec) Colon separated paths in which Ansible will search for Become Plugins. ;become_plugins={ ANSIBLE_HOME ~  /plugins/become:/usr/share/ansible/plugins/become  } # (string) Chooses which cache plugin to use, the default 'memory' is ephemeral. ;fact_caching=memory# (string) Defines connection or path information for the cache plugin;fact_caching_connection=# (string) Prefix to use for cache plugin files/tables;fact_caching_prefix=ansible_facts# (integer) Expiration timeout for the cache plugin data;fact_caching_timeout=86400# (list) List of enabled callbacks, not all callbacks need enabling, but many of those shipped with Ansible do as we don't want them activated by default. ;callbacks_enabled=# (string) When a collection is loaded that does not support the running Ansible version (with the collection metadata key `requires_ansible`). ;collections_on_ansible_version_mismatch=warning# (pathspec) Colon separated paths in which Ansible will search for collections content. Collections must be in nested _subdirectories_, not directly in these directories. For example, if `COLLECTIONS_PATHS` includes `'{ ANSIBLE_HOME ~  /collections  }'`, and you want to add `my. collection` to that directory, it must be saved as `'{ ANSIBLE_HOME} ~  /collections/ansible_collections/my/collection  }'`. ;collections_path={ ANSIBLE_HOME ~  /collections:/usr/share/ansible/collections  }# (boolean) A boolean to enable or disable scanning the sys. path for installed collections;collections_scan_sys_path=True# (path) The password file to use for the connection plugin. --connection-password-file. ;connection_password_file=# (pathspec) Colon separated paths in which Ansible will search for Action Plugins. ;action_plugins={ ANSIBLE_HOME ~  /plugins/action:/usr/share/ansible/plugins/action  }"
    
}, 

{
    "id": 48,
    "url": "https://hoex5h.github.io//Ansible_Managed_Node_%EC%97%90%EC%84%9C%EC%9D%98_ssh_%EC%84%A4%EC%A0%95",
    "title": "Ansible Managed Node 에서의 ssh 설정",
    "body": "2025/01/14 - [Ansible] Managed Node 에서의 ssh 설정SSH: Ansible 공식 문서 Building Ansible Inventories  우선 테스트로 진행할 Worker Node 1 에, ansible 이 ssh로 접근해서 사용할 사용자 계정을 하나 만들어주었다. text$ sudo adduser ansible$ sudo passwd ansible 이후 ansible 계정이 sudo 권한을 비밀번호 없이 사용할 수 있도록 설정했다. text$ sudo visudo. . . # User privilege specificationroot  ALL=(ALL:ALL) ALLansible ALL=(ALL:ALL) NOPASSWD:ALL. . .  Control Node 에 sshpass 를 설치한다. text$ sudo apt-get install sshpass Control Node 에서 공개키/개인키 를 생성한다. 공개키/ 개인키는 /home/{user}/. ssh 에 성생된다. text$ sudo ssh-keygen 공개키/개인키를 Managed Node 에 보내기text$ ssh-copy-id -p {port} ansible@{ip_address}ssh-copy-id -p 6001 ansible@192. 168. 0. 11Managed Node 에서 ssh 포트를 바꾸었기 때문에 -p 옵션을 사용했다. "
    
}, 

{
    "id": 49,
    "url": "https://hoex5h.github.io//%EC%9D%B4%EB%B6%84_%ED%83%90%EC%83%89",
    "title": "이분 탐색",
    "body": "2025/01/14 - 이분 탐색알고리즘 문제에서 단골 문제로 등장하는 이분 탐색을 정리하려고 한다. 단순히 Target 을 찾는 것부터, Target 이 들어갈 자리(lower bound, upper bound)를 찾는 방법에 대해서도 한 번 정리해보자. 이분 탐색(Target과 동일한게 있을 때): 이분 탐색의 가장 기초적인 형태로, 찾고 싶은 Target 이 몇 번째 인덱스에 존재하는지 찾는 방법이다. pythondef simple_binary_search(target: int, arr: list):  # 오름차순이라고 가정, 만약 arr 의 정렬이 보장되어 있지 않다면 정렬을 해야한다.   arr. sort()  start = 0  end = len(arr) - 1  while start &lt;= end:    mid = (start + end) // 2    print(mid, arr[mid])    if target &gt; arr[mid]:      start = mid + 1    elif target &lt; arr[mid]:      end = mid - 1    else:      # target == arr[mid]      return mid  # target 을 찾지 못함  return -이분 탐색은 말 그대로, 이등분하여 탐색하는 알고리즘이다. 로직을 자세히 보아도 이해하는데 크게 어려움은 없다. 그렇다면 target 이상인 것 중에 가장 첫 번째 인덱스를 찾고 싶거나, target 보다 큰 것중에 가장 작은 인덱스를 찾는 방법을 알아보자. Lower Bound: (target &lt;= arr[idx]) target 보다 크거나 같은 것 중에 가장 첫 번째 인덱스를 찾고 싶다면 어떻게 해야할까? pythonarr = [1, 3, 5, 5, 5, 7, 9]target = 5# 원하는 정답 = 2 (5, 5, 5의 인덱스 중에서 가장 작은 인덱스)# 위의 simple_binary_search() 함수의 결과값 = 3def lower_bound_bisect(target: int, arr: list):  # 오름차순이라고 가정, 만약 arr 의 정렬이 보장되어 있지 않다면 정렬을 해야한다.   arr. sort()  start = 0  end = len(arr)  while start &lt; end:    mid = (start + end) // 2    print(mid, arr[mid])    if target &gt;= arr[mid]:      end = mid    elif target &lt; arr[mid]:      start = mid + 1           만약 target 보다 작다면, start 를 mid + 1 로 바꿔서 이분한 배열의 오른쪽만 다시 확인하게 하고    만약 target 보다 크거나 같다면, mid 를 포함한 채로, 배열의 왼쪽을 다시 살펴본다.     왜 mid - 1 이 아니라, mid 일까? -&gt; 찾고 싶은 것이 `target 보다 크거나 같은 것` 이므로, target &gt;= arr[mid] 를 만족하는 순간    우리가 찾고자 하는 것이다. 그렇기 때문에 정답 범주에 넣어두어야 한다.          return startUpper Bound: 이번에는 target 보다 큰 것 중에, 가장 작은 인덱스 값을 알아보자. pythonarr = [1, 3, 5, 5, 5, 7, 9]target = 5# 원하는 정답 = 5 (1, 3, 5, 5, 5까지는 target 보다 작거나 같은 값이고, 7부터 target 보다 큰 값이므로)def upper_bound_bisect(target: int, arr: list):  # 오름차순이라고 가정, 만약 arr 의 정렬이 보장되어 있지 않다면 정렬을 해야한다.   arr. sort()  start = 0  end = len(arr)  while start &lt; end:    mid = (start + end) // 2    print(mid, arr[mid])    if target &gt; arr[mid]:      end = mid    elif target &lt;= arr[mid]:      start = mid + 1           만약 target 보다 작거나 같다면, start 를 mid + 1 로 바꿔서 이분한 배열의 오른쪽만 다시 살펴보게 하고    만약 target 보다 크다면, mid 를 포함한 채로 왼쪽을 다시 살펴보게 한다.     왜 mid - 1 이 아니라, mid 일까? -&gt; 찾고 싶은 것이 `target 보다 큰 것` 이므로, target &gt; arr[mid] 를 만족하는 순간    우리가 찾고자 하는 것이다. 그렇기 때문에 정답 범주에 넣어두어야 한다.          return start차이점: 단순 이분 탐색과 lower / upper bound 의 while 종료 조건 차이점을 살펴보자. while start &lt;= end 와 while start &lt; end 이 있는데, Lower/Upper 탐색은 1캰씩 움직이며 찾아나가는데 start &lt;= end 로 하게 되면 소수점 버림으로 인해서 무한 루프에 빠질 수 있다. 또한 Lower/Upper 의 return 은 mid 가 아니라 start 인데, 이는 당연하게도 Lower 과 Upper 이 각각 크거나 같은 것 중에 최소값, 큰 것 중에 최소값을 return 해야하기 때문이다. Lower/Upper 의 end 초기값도 조금 다른데, len(arr) - 1 이 아니라, len(arr) 이다. 이는 조건을 만족하는 곳이 없으면 무조건 len(arr) 의 위치에 넣으면 되기 때문이다. "
    
}, 

{
    "id": 50,
    "url": "https://hoex5h.github.io//%EC%9A%B0%EC%84%A0%EC%88%9C%EC%9C%84_%ED%81%90",
    "title": "우선순위 큐",
    "body": "2025/01/14 - 이번에 네이버 부스트 캠프 베이직 과정을 진행하면서 자바스크립트를 쓰게 되었는데, 평소에는 파이썬을 자주 사용했었기 때문에 자바스크립트로 파이썬을 대체해보려고 노력했다. 다만 파이썬에는 여러가지 자료구조와 함수들이 잘 구현되어 있는데 자바스크립트를 써보니까 우선순위 큐가 내장 라이브러리에 없었다! 이번 기회에 자바스크립트 공부 겸 힙과 우선순위 큐에 대해 다시 복습하고 직접 구현해보려고 한다. Heap우선 순위 큐는 힙 자료구조를 사용하기 때문에 힙이 무엇인지 알고 있어야 한다. 힙(Heap) 는 최댓값 혹은 최솟값을 빠르게 찾아내는 연산을 위해 설계된 완전 이진 트리 자료구조이다. 최댓값을 빠르게 찾고 부모노드의 키 값이 자식 노드의 키 값보다 항상 큰 힙을 최대 힙, 최솟값을 빠르게 찾고 부모노드의 키 값이 자식 노드의 키 값보다 항상 작은 힙을 최소 힙이라고 부른다. 우선 순위 큐에서도 작을 수록 우선순위가 높은지, 클 수록 우선순위가 높은지에 따라 최대 힙, 최소 힙을 적절히 사용해야 한다. 보통 최소 힙 라이브러리가 내장되어 있기 때문에, 만약 클 수록 우선순위가 높다면 데이터 전체에 -1 을 곱하는 등의 처리를 통해 최소 힙을 최대 힙처럼 사용할 수도 있다. 우선 순위 큐그렇다면 우선 순위 큐는 어떤 식으로 구현되어 있을까? 최소 힙과 최대 힙은 거의 유사하기도 하고, 위에서 말한 것처럼 활용할 수도 있기 때문에 최소 힙 우선순위 큐를 한 번 살펴보려고 한다. javascriptclass PriorityQueue { constructor() {  this. heap = []; // 데이터를 담을 배열 } enqueue(value) {  // 1. 힙 자료구조에 value 를 넣고  // 2. value 가 알맞은 자리를 찾아갈 수 있도록 한다. -&gt; 아래의 bubbleUp 함수를 사용할 예정 } dequeue() {  // 1. 힙 자료구조의 루트 노드에 해당하는 값을 임시로 저장해두고  // (pop 이 아님! pop(0)은 O(n) 의 시간복잡도를 가지기 떄문에 힙 자료구조를 사용하는 의미가 없어짐  // 2. 맨 마지막 리프 노드에 해당하는 값을 루트 노드에 저장하고,  // 3. 해당 루트 노드의 값이 알맞은 자리를 찾아갈 수 있도록 한다. -&gt; 아래의 bubbleDown 함수를 사용할 예정 } peek() {  // 루트 노드를 pop 하는 것이 목적이 아닌, 그저 루트 노드를 참조하고자 할 때  return this. heap. length &gt; 0 ? this. heap[0] : null; } bubbleUp(index) {  // index 번 째에 해당하는 노드의 값보다 작은 값이 있다면, index 노드를 위로 올려보내야 한다.  } bubbleDown(index) {  // index 번 째에 해당하는 노드의 값보다 큰 값이 있다면, index 노드를 아래로 내려보내야 한다.  } swap(i, j) {  // i, j 값 스왑  [this. heap[i], this. heap[j]] = [this. heap[j], this. heap[i]]; }}함수를 구현하기 전에 간략한 설명과 함께 하나씩 구현해보자. bubbleUp: javascriptbubbleUp(index) { // index 번 째에 해당하는 노드의 값보다 작은 값이 있다면, index 노드를 위로 올려보내야 한다.   while (index &gt; 0) {  const parentIndex = Math. floor((index - 1) / 2); // index 노드의 부모 노드 구하기.   if (this. heap[parentIndex] &lt;= this. heap[index]) break; // 최소 힙이므로 부모노드는 항상 자식 노드보다 작거나 같다!  this. swap(index, parentIndex); // index 랑 부모노드랑 스왑  index = parentIndex; // 부모노드로 올라간 index 부터 반복 }}자식노드(index) 와 부모노드를 비교하는 bubbleUp 함수. bubbleUp 함수는 우리가 버블정렬을 하는 것 처럼, 두 값을 비교해서 위치를 바꾸는 함수이다. 최소 힙을 구현하려고 하므로 부모노드는 항상 자식 노드보다 작거나 같아야 한다. 부모노드의 값을 구하는 방법은 Math. floor((index - 1) / 2) 인데  부모노드(0) -&gt; 자식 노드(1, 2) 부모노드(1) -&gt; 자식 노드(3, 4)부모노드(n) -&gt; 자식 노드(2n + 1, 2n + 2) 임을 알 수 있다. 따라서 자식 노드가 +1, +2(왼쪽, 오른쪽) 중에 어떤 쪽인지에 상관없이 부모노드를 구하는 방법이 위의 방법인 것이다. bubbleDown: javascript bubbleDown(index) {  // index 번 째에 해당하는 노드의 값보다 큰 값이 있다면, index 노드를 아래로 내려보내야 한다.   while (true) {   const leftChildIndex = 2 * index + 1; // 왼쪽 자식 노드   const rightChildIndex = 2 * index + 2; // 오른쪽 자식 노드   let smallestIndex = index; // 부모노드, 왼쪽 자식노드, 오른쪽 자식 노드 중에 가장 작은 값을 저장할 변수   if (leftChildIndex &lt; this. heap. length &amp;&amp; this. heap[leftChildIndex] &lt; this. heap[smallestIndex]) {    // 만약 부모노드보다 왼쪽 자식 노드가 작다면 smallestIndex 를 왼쪽 자식 노드로 변경    smallestIndex = leftChildIndex;   }   if (rightChildIndex &lt; this. heap. length &amp;&amp; this. heap[rightChildIndex] &lt; this. heap[smallestIndex]) {    // 만약 위의 if 문에서 왼쪽 자식 노드가 작았다면 smallestIndex 는 왼쪽 자식 노드,    // 아니라면 부모 노드    // 똑같이 오른쪽 자식 노드랑 비교    smallestIndex = rightChildIndex;   }   // 만약 가장 작은 노드가 부모 노드라면 최소 힙의 조건을 만족하므로 break   if (smallestIndex === index) break;   // 자식 노드 중에 더 작은 노드(smallestIndex) 와 부모노드를 스왑   this. swap(index, smallestIndex);   //더 작은 노드의 위치에 index 노드를 두고 반복   index = smallestIndex;  } }bubbleDown 도 bubbleUp 와는 반대로 내려오는 것이기 때문에, index에 해당하는 노드가 제자리를 찾을 때까지 자식 노드와의 비교를 반복한다. enqueue, dequeue: javascript enqueue(value) {  // 1. 힙 자료구조에 value 를 넣고  this. heap. push(value);  // 2. value 가 알맞은 자리를 찾아갈 수 있도록 한다. -&gt; 아래의 bubbleUp 함수를 사용할 예정  this. bubbleUp(this. heap. length - 1); } dequeue() {  if (this. heap. length === 0) return null;  // 1. 힙 자료구조의 루트 노드(0번째 Index)에 해당하는 값을 임시로 저장해두고  // (pop 이 아님! pop(0)은 O(n) 의 시간복잡도를 가지기 떄문에 힙 자료구조를 사용하는 의미가 없어짐  const min = this. heap[0];  // 2. 맨 마지막 리프 노드에 해당하는 값을 루트 노드에 저장하고,  const last = this. heap. pop();  if (this. heap. length &gt; 0) {   this. heap[0] = last;   // 3. 해당 루트 노드의 값이 알맞은 자리를 찾아갈 수 있도록 한다. -&gt; bubbleDown   this. bubbleDown(0);  }  return min; }"
    
}, 

{
    "id": 51,
    "url": "https://hoex5h.github.io//Memory_Swap",
    "title": "Memory Swap",
    "body": "2025/01/14 - Swap Memoryt2. micro 를 사용하면서, npm run build 혹은 npm run start 가 특정 커맨드 라인에서 멈추는 현상이 있었다. 열심히 검색해보니 메모리 부족이라는 얘기가 있었는데, 실제로 고사양의 컴퓨터에 해보았을때 문제없이 돌아갔다. 해결방법을 총 2가지를 찾았는데,  메모리 스왑textsudo dd if=/dev/zero of=/mnt/swapfile bs=1M count=2048sudo mkswap /mnt/swapfilesudo swapon /mnt/swapfile디스크를 메모리처럼 사용하여 임시방편을 메모리를 늘리는 방법이다. 하지만 디스크는 메모리에 비해 속도가 매우 느리므로 성능이 저하될 수 밖에 없다.  다른 사양에서 build 후 가져오기. 이 방법은 사용을 해보진 않았지만, 고사양의 서버가 있다면 사용해볼 수 있겠다. 그렇지만 굳이 이렇게 할 이유가 있나?"
    
}, 

{
    "id": 52,
    "url": "https://hoex5h.github.io//%EC%9D%B8%EA%B0%84_%EA%B4%80%EA%B3%84%EB%A1%A0_-_%EB%8D%B0%EC%9D%BC_%EC%B9%B4%EB%84%A4%EA%B8%B0",
    "title": "인간 관계론 - 데일 카네기",
    "body": "2025/01/14 - 인간 관계론 - 데일 카네기인간 관계에 있어서 정답이 있을까? 사실은. . 이 책을 읽다보니 시작부터 썩 맘에 들지는 않았다. 마치 사이비 교주같이 이 책을 따라하기만 하면 내 삶은 더 나아질 것이고 모르는 것이 있을 땐 이 책의 내용을 반추하면 해결할 수 있다는 뭐 그런… 그러나 사람들이 열광하는데는 이유가 있을 것 같아서 읽어보기로 했다. 사람을 다루는 기본 방법사람을 다루는 기본 방법 1.  비판하거나 비난하거나 불평하지말라 이 말은 100프로 적용하기는 힘들지라도 이해는 가는 대목이다.    경험   최근에 팀프로젝트에 아랍에서 오신 분이 있었는데, 한국어를 알아듣는데는 문제가 없으나 말하는게 살짝 어렵다고 하셨다. 팀 회의록을 화요일까지 제출해야해서 토/일에 아무때나 회의를 하자고 했더니 주말은 절대 안된다고 하셨다. 이유는 한양대학교에서 열리는 외국인 축제를 가야하기 때문. 처음엔 이해가 안됐다. 축제가 24시간 하는 것도 아니고 축제시간 피해서 오전이나 밤에 줌으로 하면 되는거 아닌가? 나말고도 다른 한국인 팀원도 이해가 안됐는지 다짜고짜 화를 내기 시작했다. 나도 조금은 화가 나려고 했는데 이미 화를 내는 사람이 있어서 나까지 내면 시작하기도 전에 망할 것 같아서 최대한 차분하게 얘기를 했다. 아랍 분도 화가 좀 나신 것 같았는데 차분하게 얘기하니 차분하게 대답하려고 노력하셨다. 알고보니 축제 시간 전에 시간이 비긴 하는데, 축제 전에 친구 집에 갈 예정이어서 언제 시간이 나는지 정확히 몰랐고, 이걸 한국말로 표현하기가 어려워서 주말은 통째로 안된다라는 뉘앙스로 얘기하신 것 같다. 주말에 회의는 무난하게 이뤄졌고 지금까지 큰 애로사항은 없다. 만약 나도 화를 냈다면 내 팀 프로젝트는 어떻게 되었을까?  보통 사람을 “비난” 하지 말라는 얘기를 많이 한다. 비판과 다르게 근거 없는 헐뜯기이기 때문이다. 비판은 그보다는 조금 더 긍정적인 뉘앙스를 담고 있다고 생각한다. 상대방에게 잘못의 원인이 있으며 그저 그 잘못됨을 바로잡아주려고 하는 긍정적인 피드백. 근데 책에서는 비판도 하지 말란다. 불평도 하지말란다. 사람과의 “관계” 를 위함이지, 사람과의 “시너지” 를 위함이 아니라서 그런가? 모든 사람에게 100퍼센트 적용 가능한 지침을 위해서인가? 라는 생각이 들었다. 사람을 다루는 기본 방법 2.  솔직하게, 진심으로 인정하고 칭찬하라 책의 극 초반부이긴 하지만 여기까지 읽고 든 생각은, 역시 모든 사람에게 적용 가능한 지침이겠구나 라는 생각을 했다. 성인군자가 되는 방법 자체는 명료하지만 실제로 성인군자가 되기는 어렵듯이, 이 책도 마찬가지 아닐까 싶은 생각이 들었다. 사람을 다루는 기본 방법 3.  다른 사람에게 열렬한 욕구를 불러일으켜라 말 그대로 상대방이 내가 원하는 것을 하게 만들기 보다는, 상대방이 그것을 함으로써 얻는 것들을 나열해서 열렬한 욕구를 불러일으키라는 것이다. 이 말은 가능하다면 굉장히 좋은 도구가 될 것같다. 다만 나의 관심사가 상대방에게도 도움이 될 수 있다는 가정하에 좋은 도구다. 다른사람이 자신을 좋아하게 만드는 규칙규칙1.  다른 사람에게 진심으로 관심을 가져라 관심을 부담스러워 하는 사람은 없을까? 관심에도 정도가 있을텐데 어느정도의 관심을 가지는것이 좋을까? 규칙 2.  웃어라 행동이 감정을 따르는 것 같지만 사실 행동과 감정은 같이 간다. 따라서 자신의 의지로 통제할 수 있는 행동을 조절하면 직접적인 통제가 불가능한 감정도 간접적으로 통제할 수 있게 된다. 따라서 즐거움을 잃었다면, 자발적인 즐거움으로 가는 최고의 길은 즐거운 자세를 가지고 이미 즐거운 사람처럼 행동하고 말하는 것이다 - 중에서 진실된 미소를 띄어라. 규칙 3:  상대방의 이름은 그에게 있어서 모든 말 중에서 가장 달콤하고 중요한 말로 들린다는 점을 명심하라. 규칙 4:  잘 듣는 사람이 되어라. 다른 사람들이 자신에 대해 이야기하도록 만들어라. 규칙 5:  다른 사람의 관심사에 맞춰 이야기하라. 규칙 6:  다른 사람으로 하여금 자신이 중요한 사람이라고 느끼게 만들어라. 진심으로 그렇게 행동하라. 상대방을 설득하는 방법규칙 1:  논쟁을 이기는 유일한 방법은 논쟁을 피하는 것이다. 규칙 2:  다른 사람의 의견을 존중하라. 절대로 그 사람이 틀렸다고 이야기하지 마라. 규칙 3:  당신이 틀렸다면 빨리, 분명히 인정하라 이 부분은 정말 옳다고 생각하지만 정말 어렵다고 생각한다. 자신의 실수를 인정하는것이 말은 쉽지만 생각보다 어렵기 때문이다. 규칙 4:  우호적으로 시작하라. 규칙 5:  다른 사람들로 하여금 당장 ‘네,네’라고 말하게 하라. 다른 사람과 대화할 때 서로 의견이 다른 부분부터 이야기하지 마라. 상대방과 당신이 동의하고 있는 부분을 강조하면서 시작하고, 계속 그 부분을 강조하라. 가능하다면 둘 다 같은 목적을 추구하고 있으며, 단지 방법이 다를 뿐이라는 점을 계속해서 강조하라. 어떤 사람이 아니요라고 말하는 순간 그의 자존심은 그가 일관성 있는 사람이 되도록 요구한다. 그가 나중에 아니요라는 반응이 경솔했다고 느낄 수도 있다. 하지만 그의 소중한 자존심을 고려해 주어야 한다. 규칙 6:  다른 사람이 말을 많이 하도록 만들어라. 규칙 7:  다른 사람으로 하여금 스스로 생각해 냈다고 여기도록 만들어라. 규칙 8:  진심으로 다른 사람의 관점에서 사물을 보려 애써라. 규칙 9:  다른 사람들의 생각과 욕망에 공감하라 규칙 10:  고상한 동기에 호소하라 사람들이 어떤 일을 하는 데는 두 가지 이유가 있기 마련이다. 하나는 듣기에 그럴듯한 이유이고, 나머지 하나는 실제 이유이다. “제 사진을 더 이상 공개하지 말아주세요. 어머니가 싫어하십니다. ” 규칙 11:  당신의 생각을 극화하라. 쇼맨십을 활용해라. 규칙 12:  도전 의욕을 불러일으켜라 다른 사람보다 더 뛰어나고자 하는 욕망 기분 상하게 하거나,적개심을 불러일으키지 않고 사람을 바꾸는 9가지 방법  칭찬과 진심에서 우러나온 감사로 대화를 시작하라. 2 사람들의 잘못을 간접적으로 지적하라. 3 다른 사람을 비판하기 전에 자신의 실수부터 이야기하라. 4 직접 명령을 내리기보다는 질문을 하라. 5 다른 사람의 체면을 세워 주어라. 6 약간의 발전만 있어도 칭찬하고, 발전이 있을 때마다 칭찬하라. “진심으로 인정하고 칭찬을 아끼지 말라. ” 7 기꺼이 부응할 만한 평판을 부여하라. 8 격려하라. 고쳐 주고 싶은 잘못은 고치기 쉬운 잘못처럼 보이게 하라. 다른 사람이 해 주었으면 하는 일은 쉬운 일처럼 보이게 만들어라. 9 당신이 제안하는 바를 다른 사람이 즐겁게 행하도록 만들어라. "
    
}, 

{
    "id": 53,
    "url": "https://hoex5h.github.io//%EB%B2%8C%EA%B1%B0%EB%B2%97%EC%9D%80_%ED%95%9C%EA%B5%AD%EC%82%AC_-_%EC%82%AC%EA%B1%B4%ED%8E%B8",
    "title": "벌거벗은 한국사 - 사건편",
    "body": "2025/01/14 - 벌거벗은 한국사 - 사건편 소설책처럼 술술 읽히는 한국사 이 책은 TV 프로그램 를 글로 옮긴 것이다. 이 책의 장점은 명확하다. 바로 책이 재밌다. 수능 공부때 필수 과목이였던 한국사를 공부할 때는 사실 년도를 외우고, 사건을 암기하는 말 그대로 시험을 위한 한국사였다. 이번에 책을 고르면서 한국사, 특히 사건편을 고른 이유는 한국사를 재밌게 읽고 싶었기 때문이다. 책에는 여러 가지 사건들이 나열되어 진행되는데, 무신정변 -&gt; 여몽전쟁 -&gt; 임진왜란 -&gt; 병자호란 -&gt; 환관 -&gt; 경술국치 -&gt; 조선어학회 -&gt; 광복 의 순서로 진행된다. 역시 가장 흥미롭게 읽었던 부분은 경술국치 부터 광복 까지, 일제강점기 시대 이야기이다. 하지만 재미있는 부분은 마지막에 얘기하도록 하고 차례대로 흥미로웠던 부분을 적어보려고 한다. 무신정변: 무신정변은 말 그대로 무신의 정변인데, 고려는 무신정변을 기점으로 고려 전기, 고려 후기로 나뉠만큼 꽤나 임팩트가 강렬했던 사건이었다. 문신은 오늘 날의 공무원, 법관, 국회의원 등의 역할이었고 무신은 군인, 경찰과 같은 일을 하는 사람들을 일컫는 말이었다. 문신이 아무래도 국가 정책에 관련된 일에 참여하였기 때문에 무신에 비해 권력을 독점할 수 있었고 무신을 차별하다가 일어난 일이 무신정변 이었다. 무신이 차별받은, 그리고 무신정변이 일어나게 된 주요 계기로 무신 정중부와 문신 김돈중의 이야기가 나오는데 김돈중이 정중부보다 직위가 낮음에도 불구하고 정중부의 수염을 태워버리는 일이 발생한다. 무신과 문신의 사회적 지위가 다르기도 달랐지만 김돈중에게는 아주 든든한 뒷배가 있었기 때문인데 바로 김돈중의 아버지가 김부식 이었다. 김부식은 삼국사기를 엮은 인물로 아주 유명하다. 무신은 참고 참다가 결국 무신정변을 일으키고 왕을 유배보내고 그동안 쌓아왔던 분노로 문신들을 처참하게 살육하며 권력을 쟁탈한다. 무신정변은 대한민국의 무신들에 대한 대우를 다시 생각해보게 했다. 여몽전쟁: 팔만대장경의 유래는 무엇일까? 석가모니의 설교, 제자들의 해석, 불교의 계율 등을 기록한 경전이 81,258장이나 되는 팔만대장경은 왜 만들어졌을까? 그때의 몽골은 거의 지금의 미국과 같은 군사력을 지녔다고 해도 무방할 정도였다. 기마부대를 주축으로 한 몽골의 군사들은 동유럽까지 진출해서 대제국을 건설했다고 한다. 하지만 당시의 고려는 위에서 말한 무신정변 이후, 최씨 집안이 권력을 꽉 잡고 있었는데 백성들을 수탈하기만 하지 혼란한 정세를 바로잡을 생각은 하지 않았다. 모종의 사건으로 몽골이 고려를 침략하는데 지배층은 몽골을 그냥 오랑캐 즈음으로 취급할 뿐 사치나 부리면서 대응을 하지 않았다. 세계 최강의 군사력을 가진 몽골은 순식간에 고려의 지역을 하나하나씩 박살냈는데, 고려가 늘 당하고만 있지는 않았다. 귀주성의 강감찬 장군, 은 아니고 같은 귀주성에서 박서 라는 장군과 귀주성의 백성들이 몽골의 진군에 발목을 잡기도 하고 (강감찬 장군은 거란의 침입에 맞서 싸웠다)고려 승려 김윤후 가 화살 한 방으로 몽골의 총사령관 살리타를 죽이는 사건도 있었다. 이 외에도 고려의 수 많은 백성들이 투쟁하여 몽골을 당황케했다. 하지만 결국 고려의 조정은 몽골에게 항복한다. 수 많은 이름 없는 백성들이 죽어나가면서 지키려했지만 조정은 쉽게 놓아버렸다. 어느 책에서도 죽어나간 백성들이 이름은 적혀있지 않고 몽골에 항복한 최씨 가문의 이름만 구전되어 내려오고 있다. 임진왜란: 임진왜란하면 이름 석 자가 떠오르지 않을 수 없다. 하지만 이번의 얘기는 조금 다르다. 이 시기동안 우리가 빼앗긴 문화 에 관한 얘기다. 일본은 임진왜란을 일으키고나서 조선의 도자기, 바느질, 세공 기술자들을 모두 납치했고 평범한 조선인들은 노예로 팔았다. 심지어 서책도 훔쳐서 가져가기도 했다. 이 시기에 일본의 문화는 괄목할만한 성장을 이루었는데 조선의 기술자를 훔쳐간 덕택이었다. 일본에 끌려간 기술자들은 고국으로 돌아오지도 못하고 평생을 일본에서 살며 자신의 기술로 도자기를 만들고, 바느질을, 세공을 하며 살아야만 했다. 누군가는 그나마 대접이라도 받으며, 누군가는 대접도 받지 못하며 남의 나라를 위한 일을 했다. 병자호란: 조선의 왕, 인조가 땅에 대고 세 번 큰절을 하는 삼전도의 굴욕은 공부를 통해 알고는 있었다. 그리고 분명 공부하면서 이를 기록한 비석인 삼전도비 가 어디 있는지도 들었던 것 같았는데 삼전도비가 어디에 있는지는 시험에 나오지 않았기 때문에 외우지 못했다. 근데 알고보니 내가 자주가는 석촌호수 에 있다. 공부하면서는 별 생각이 없었는데 이번 책을 읽으면서는 석촌호수에 꼭 한 번 가서 봐야겠다는 생각이 들었다. 소현세자는 조선을 이긴 청나라의 볼모로 잡혀 청나라에 끌려가게 되었다. 비록 모진 생활을 겪으면서도 장사에 수완을 발휘하는 등, 여러 위기를 잘 겪어왔지만 자신의 왕위를 위협받는다는 이유로 소현세자를 내쳐버린, 소현세자 아버지 인조에 결국 삶의 의지가 꺾인다. 환관: 환관은 고려시대와 조선시대에서의 의미가 좀 다르다. 고려 시대에는 환관과 내시가 구별되었는데 조선 시대때는 환관과 내시는 같은 말이었다. 환관은 왕실 사람들 옆에서 마치 비서처럼 일을 했었다. 우리는 보통 내시를 생각하면 별거 아닌 것 처럼 생각하지만 환관은 왕의 최측근이다보니 자연스럽게 권력을 쥘 수 있는 직책이었다. 왕의 말을 전하는 것 부터, 왕의 음식을 담당하기도 하고 술, 다과, 약에 대해 담당하기도 했다. 심지어 환관은 신분에 제한이 없었으므로 신분 상승을 노리던 모든 사람들이 노리는 직책이었던 것이다. 경술국치: 역시 제일 흥미롭게 읽은 부분은 경술국치부터 시작해서 광복까지이다. 을사오적중에 하나인 이완용은 처음부터 친일파였을까?이완용은 고종에게 있어서 엘리트 코스를 밟아온 인재 그 자체였다. 심지어 고종이 미국과 친하게 지내기 위해 설립한 영어 학교, 육영공원에서도 이완용은 뛰어난 재능을 보였교 그렇게 주미 조선공사관의 관원 중 한 명으로 발탁되었다. 당시 조선은 아직도 청나라의 간섭 아래에 있었는데 고종은 청나라로부터 벗어나 미국과의 친목 도모를 꾀했다. 이완용은 이렇게 엘리트 코스를 밟으며 고종의 최측근이 되었고 주미 조선공사관으로 일하며 친미 성향에 더욱 가까웠을 것이다. 영어를 잘하는 사람이 없던 조선에서 영어를 유창하게 할 줄 아는 이완용은 친미파의 대표 인물이였으며, 대미 외교의 1인자였다. 심지어 명성황후 시해 사건(을미사변)이 일어나고 왕이 러시아 공사관으로 피신하는 아관파천조차도 이완용과 함께 도모한 일이었다. 그런데 왜 이완용은 순식간에 일본 앞잡이로 돌변한 것일까? 이완용은 자기의 이익을 위해서라면 과정은 전부 개의치 않았던 모양이다. 고종, 미국, 일본, 러시아 모든 것이 그저 자신의 출세를 위한 도구쯤으로 여겼나보다. 나랏돈을 횡령한 혐의가 있었다고도 하니 애국심은 분명 없었을 것이다. 러일전쟁에서 일본의 승리, 은근히 일본을 지지해주는 것 같은 미국, 이런 상황 속에서 이완용은 자신의 다음 출세길을 찾아 친일파가 된 것이었다. 조선의 대신들을 중명전에 무력으로 몰아놓고 강제로 을사늑약을 체결한 이토 히로부미는 8명의 대신에게 을사늑약에 대한 의견을 묻는다. 여기서 참정대신 한규설 과 민영기 는 절대 반대를 외쳤다. 이 중에서 한규설은 끝까지 조약 체결에 반대했고 민영기는 이후 친일파가 되었다. 이완용보다 더 기억되어야 할 인물은 한규설이 아닐까 싶다. 이 중명전에서 을사늑약에 찬성을 외친 이완용, 이지용, 박제순, 이근택, 권중현을 을사오적이라 부른다. 경술국치는 경술년에 당한 나라의 수치라는 뜻이다. 한국과 일본을 하나로 만들겠다는 아주 치욕적인 조약인, 한일강제병합조약을 이완용이 ‘먼저’ 일본에게 제시했고 이 조약이 경술년에 조인되었다. 이완용은 몇 번의 암살시도를 당했지만 결국 자연사했다고 한다. 죽으면서 마치 조선을 신세대로 개척했다고, 자기가 사람들을 계몽한 것처럼 느끼면서 죽었을까? 조선어학회: 이렇게 일제강점기시대가 쭉 이어지며 일본은 우리나라의 문화를 죽이려고 했다. 특히 한글은 우리나라 정체성이었고 일본은 재빠르게 국어를 한글에서 일본어로 바꾸어버렸다. 하지만 조선어학회 는 한글이 없어지는걸 두고 보지 않았다. 사실 한글은 누군가가 이름 붙여주기 전까지는 한글이라는 이름이 없었다. 세종대왕이 훈민정음을 창제했지만 한글이라 불린게 아니라 말 그대로 글자를 만들었던 것이었다. 한글에 이름을 붙여준 사람은 바로 국어학자 주시경 이다. 주시경은 한글을 연구하기 위해 국어연구학회 를 설립했고 국어연구학회는 추후에 배달말글몯음 -&gt; 한글모 등으로 바뀌었다. 주시경은 일생의 소망이 하나 있었는데 바로 사전 을 편찬하는 것이었다. 주시경은 경술국치 이듬 해인 1911년에 제자들과 함께 말모이 라는 최초의 우리말 사전 편찬을 계획하게 된다. 하지만 주시경은 마음 먹은지 4년 만에, 39세의 나이로 돌연 사망해버린다. 하지만 유학파 이극로가 주시경의 제자들이 활동하던 ‘한글모’ 라는 단체를 재건한 조선어연구회 라는 단체를 만들고 다시 사전을 편찬하기 시작한다. 조선어에는 너무나도 많은 사투리들이 있었는데 고작 몇 명의 사람들이 이 사투리를 전부 알아내는건 불가능에 가까웠다. 조선어연구회는 학교 선생님들이 많았던 점을 이용해서 방학 숙제로 학생들에게 방언을 기록해오라는 숙제를 주고 이를 활용해 사전을 편찬한다. 정말 엄청난 아이디어 같다. 하지만 끝내 일본이 조선어연구회를 독립운동가들과 엮어서 저물어가는듯 했는데. . 광복: 조선어학회는 일본의 판결에 재심을 신청했는데 일본은 법의 허점을 이용해서 시간을 끌며 버티다가 결국 1심 판결대로 형량을 확정해버린다. 그런데 재심 판결의 날짜가 1945년 8월 13일이었다. 2일 후, 우리나라는 광복을 맞게 되고 조선어학회는 기존의 작업물을 다시 가져와 결국 사전을 편찬해내는데 성공한다. 그렇다면 광복을 맞은 8월 15일, 우리나라는 어땠을까? 엄청난 환호성을 내지르고 사람들의 축제 분위기였다고 생각하는 것과는 달리 매우 조용했다. 그도 그럴 것이 패전국 일본의 방송을 사람들이 알아듣기는 힘들었기 때문이다. 다음 날인 8월 16일이 되어서야, 사람들은 광복을 맞이했다는걸 알게 되고 이제서야 비로소 광복의 날을 맞은 것이다. 조선 내부에서 조선을 달달 볶던 조선총독부는 사태의 심각성을 깨닫고 일본으로의 무사 귀환을 위한 해결책을 찾는다. 그들이 택한건 바로 독립운동가와 손 잡는것 이라는 평범하게는 이해가 가지 않는 방법이었다. 그도 그럴 것이 갑작스러운 해방으로 어지러운 상황이었기 때문이기도 했고 아직 일본으로부터 완전히 벗어난건 아니었기 때문이다. 독립운동가 여운형은 무고하게 투옥된 조선인 석방을 비롯한 여러가지 조건으로 조선총독부의 무사 귀환을 약속했다. 하지만 조선에 있는 일본 헌병은 광복 이후에도 조선을 괴롭혔는데, 조선이 공산주의로 넘어갈 것 같다는 헛소문을 미국에 전달해버렸다. 미국은 이를 듣고 조선이 공산당에 넘어가지 않도록 자신들이 도착할 때 까지 조선을 일본에게 맡겨버린다. 일본은 그렇게 아주 평온하게 자신들의 고국으로 무사 귀환했다. 8월 15일 광복으로부터 일본이 무사 귀환할 때까지도 사람들은 억압받고 고통 속에서 지내야 했다. 총평이 책은 내 기준에서는 흔히 알던 이야기를 하지는 않는다. 그리고 흔히 알려진 사람이 아니고 이름 없이 묻힌 사람들의 이야기도 자주 나온다. 내가 잘 알려진 우리나라의 역사 조차도 잘 아는건 아니지만, 어느 역사든 먼저 배워야하고 먼저 알아야하는게 있는걸까? 이렇게 그냥 흥미롭게 읽는게 사실 맞는거 아닐까 라는 생각이 들어서 이 책이 상당히 재밌게 느껴졌다. 한국사를 처음 입문한다고 했을 때 오히려 이 책을 추천해줘도 괜찮지 않을까? 라는 생각이 들었다. "
    
}, 

{
    "id": 54,
    "url": "https://hoex5h.github.io//%EB%AA%A8%EB%B9%84%EB%94%95_-_%ED%97%88%EB%A8%BC_%EB%A9%9C%EB%B9%8C",
    "title": "모비딕 - 허먼 멜빌",
    "body": "2025/01/14 - 모비딕 - 허먼 멜빌 뭍에 사는 사람들이 물에 대한 호기심을 채울 수 있는 책 줄거리 전반: 모비딕은 주인공 이스마엘의 전지적 작가 시점으로 진행되며 피쿼드 호의 선장, 에이허브가 거대한 고래 모비딕을 좇는 내용으로 진행된다. 에이허브는 이미 수 년전 모비딕과의 전투를 한 번 치른 적이 있었는데, 이 전투에서 이미 대패하면서 다리 한 쪽을 잃는 불상사를 겪었다. 이런 굴욕감, 분노, 비통함 등이 섞여 에이허브는 다른 어떠한 일 보다 모비딕을 좇는 것에 몰두하게 된다. 주인공 이스마엘과 이스마엘의 친구 퀴퀘그는 에이허브가 이끄는 피쿼드 호에 승선하며 에이허브의 복수와 동시에 향유고래 사냥에 나서게 된다. 등장인물: 퀴퀘그: 피쿼드 호에는 능력있는 작살꾼, 항해사 등이 타고 있었다. 그 작살꾼들 중에 주인공 이스마엘의 친구, 퀴퀘그는 항해를 시작하기 전에 만난 사람이었다. 원시 부족의 우두머리의 아들이였던 그는 더 다양한 세계를 경험하고자 자신이 살던 마을에서 뛰쳐나와 배에 올랐다. 이스마엘과 동고동락하며 절친이 된 퀴퀘그는 자신의 돈 마저 이등분하여 이스마엘에게 나눠주고, 이스마엘에게 위험한 일이 생긴다면 목숨을 걸고서라도 지킨다는 약속을 한다. 항해 중간에 열사병을 앓으며 죽기 직전까지 몸이 악화되었으나 살아야겠다는 의지가 생기자마자 몸이 회복되는 초인과 같은 회복력을 보여준다. 이스마엘: 주인공 이스마엘은 책에서 사실 크게 비중이 높지 않다. 마치 전설을 이야기해주는 이야기꾼 느낌이 강하다. 아마 작가 그 자체가 투영되어 있는 인물인 것 같다. 스타벅: 피쿼드 호의 일등항해사 스타벅은 사실 에이허브의 복수를 위해 피쿼드 호에 승선한 것은 아니였다. 그저 향유고래를 사냥하고 기름을 얻는, 지금으로 따지자면 그냥 돈 벌기 위해 출근한 회사원에 가깝다. 다른 선원들이 에이허브의 열정에 동화되어 모비딕을 좇기로 결심할때 조차 스타벅은 차갑도록 냉정했다. 스타벅은 모비딕에게 이미 한 번 패했고 앞으로도 이길 수 없을거라 얘기하며 모비딕을 좇는 것을 포기하라고 에이허브에게 얘기하지만, 에이허브는 항해의 목적 자체가 모비딕이였다. 에이허브는 스타벅의 말을 일관되게 무시하며 모비딕을 향해 나아갔고 스타벅과는 꾸준하게 갈등을 빚는다. 하지만 스타벅은 기본적으로 에이허브를 존중하는 모습을 보였는데 모비딕 사냥에는 반대했지만 에이허브의 안위를 걱정하고 있었다. 물론 에이허브에게 쓴소리 하는 것도 마다하지 않았다. 에이허브: 에이허브는 사실 모비딕의 주인공이다. 모비딕에게 오직 복수하겠다는 일념으로 삶을 버텨오다가 결국 피쿼드 호의 선장으로 임명되며 모비딕의 복수를 시작한다. 그는 모비딕을 좇는 일 외에는 크게 관심이 없다. 심지어 항해 중 마주친 다른 배의 도움조차도 모비딕을 좇는 것 때문에 거절한다. 책의 후반부에서 푸른 바다의 따듯함, 고요한 파도에 인생을 되돌아보며 후회도 하는 인간적인 모습을 보여주지만 그럼에도 모비딕에 대한 복수를 포기하지 않는다. 어떻게 보면 이 대목에서 에이허브 스스로도 모비딕을 잡지 못할 거라는걸 깨닫는 것 같았다. 하지만 결착을 보고 말겠다는 의지 하나로 죽음으로 뛰어들어간다. 모비딕 조우 전: 모비딕을 조우하기 전까지 일반 향유고래 몇 마리를 잡으며 항해가 어떻게 이루어지는지, 향유고래를 잡는 과정, 잡고 난 후 기름을 추출하는 과정 등이 생각보다 상세하게 적혀있다. 바다 생활에 대해 전무한 나로서는 신기했다. 향유고래를 잡을 때도 있고, 놓칠때도 있었지만 피쿼드 호는 무탈하게 앞을 향해 나아가고 있었다. 그러다 어느 순간 번개가 치고 폭풍우가 일며 바다가 일렁이기 시작하는데 마치 게임에서 보스를 만나기 전, 분위기가 어두컴컴해지며 곧 끝이 다가왔다는 것을 알리는 순간이 온다. 모비딕 조우: 모비딕은 말 그대로 최종보스였다. 다른 향유고래와 꽤나 많은 전투를 치른 피쿼드 호 선원들이었으나 모비딕 몸짓 한 방에 우수수 떨어져나가고 보트는 산산조각이 난다. 에이허브조차 처음에 내비치던 용맹한 모습은 사그라들고 공포에 떠는 순간이 있었을 지경이였다. 마음을 다잡고 모비딕과 전투하지만 모비딕은 전투를 하다가 도망가고, 또 나타나서 고래 잡이용 보트를 부숴버리고 선원들을 물에 내동댕이 치고 다시 도망가는 굉장히 영리한 모습을 보였다. 사실 나를 포함한 모든 독자는 모비딕과의 조우가 하이라이트였을거고, 모비딕과의 복수전에서 승리할 수 있을지가 최대 관심사였을 것이다. 그러나 모비딕은 보자마자 이기기는 쉽지 않겠다는 생각이 들 정도로 묘사되었다.  결말 (스포!)모비딕과의 전투에서 피쿼드 호는 결국 대패한다. 주인공 이스마엘을 제외하고 모두 사망하고,이스마엘은 친구 퀴퀘그가 열사병을 앓을 때 만들었던 관을 구명보트 삼아 둥둥 떠다니다가일전에 에이허브가 도움을 무시했던 다른 배에 구조된다. 모비딕만을 좇다가 결국 목표를 성취해내는 그런 결말이 아닌, 생각보다 처참한 결말을 맞게 된다. 후기: 이 책은 모비딕과의 조우 전, 후로 나뉘어진다고 생각한다. 모비딕 조우 전에는 마치 내가 직접 항해를 하는 듯한 느낌이 들 정도로 바다생활을 자세히 표현한다. 그러면서 결국 끝은 모비딕과의 전투임을 간간히 상기시켜주기 때문에 살짝 살짝 긴장감은 유지 시키면서도 항해에 대한 호기심을 충족시켜준다. 향유 고래도 능숙하게 잡아내는 피쿼드 호를 보면 모비딕도 잡을 수 있지 않을까? 라는 생각도 하게 만들고 향유고래에서 기름을 추출하는 장면을 보면 살면서 그런 장면을 눈 앞에서 볼 수 있는 사람이 얼마나 될까 싶은 생각도 든다. 모비딕과의 조우 후에는 어떻게 보면 살짝 뒤틀린 욕망에도 가깝다고 볼 수 있는, 목표에 대한 열망에 대해 생각해보게 된다.  (스포!)그렇게 열망하던 복수가 모비딕을 죽음 근처로 몰아넣지도 못하고 허망하게 끝나버리는데끝까지 읽고 나서는 과연 이 전투가 모비딕의 하이라이트가 맞았나?작가가 쓰고자 한 글은 사실 모비딕 조우 전의 이야기에 가깝지 않나 라는 생각도 들었다. 모비딕은 그저 항해의 이야기를 하기 위한 수단이었을까?중간중간 인간을 벗어난 예언도 나오고, 열사병을 그냥 마음 먹기로 이겨내버리는 모습도 나오지만 읽는데 크게 거부감이 들지는 않았다. 파도가 크게 일렁이는 배 위의 이야기지만 모비딕 조우 전까지는 잔잔하게 읽히는 책이었다. "
    
}, 

{
    "id": 55,
    "url": "https://hoex5h.github.io//%EB%A5%98_-_%ED%9E%88%EA%B0%80%EC%8B%9C%EC%95%BC%EB%A7%88_%EC%95%84%ED%82%A4%EB%9D%BC",
    "title": "류 - 히가시야마 아키라",
    "body": "2025/01/14 - 류 - 히가시야마 아키라일본 3대 문학상을 동시 석권한 전대미문의 걸작  내가 심사를 맡은 이래 단연 최고의 작품이다! «나미야 잡화점의 기적», 히가시노 게이고 처음 이 책을 읽기로 결심한 이유는 단지 내가 재밌게 읽었던 소설을 집필한 히가시노 게이고의 말 한마디 였다. 미스터리 장르로 분류되어있는 책이기도 했고, 할아버지의 죽음을 파헤치는 내용이라는 말에 읽어볼만 하겠다 라는 생각이 들었다. 히가시야마 아키라 라는 일본인의 소설이지만 주인공과 시대 배경은 대만인과 대만이다. 알고보니 글쓴이가 대만 태생이고 5살에 일본으로 건너와 살게 되었다고 한다. 사실 이러한 내용은 내가 이 책을 결정짓는데 크게 작용하지 않았다. 나는 그저 미스테리 / 히가시노 게이고 에 큰 비중을 뒀을 뿐이다. 주인공 예치우성은 예준린의 손자로, 의문의 살해를 당한 할아버지의 죽음을 파헤치고자 한다. 그 과정에서 갑자기 연애도 해보고, 군대도 다녀오고, 영적인 존재들에게 괴롭힘도 당해보고, 깡패들과 부딪혀도 보는 등 다양한 경험을 하게 된다. 연애를 제외하고는 모든 일들의 원인에 할아버지의 죽음이 깔려 있었으나 죽음에 대한 단서에 큰 도움이 되지는 않았다. 나중에 알고 보면 그저 부수적인, 간접적인 근거로 작용하거나 혹은 주인공의 심경 변화 정도만 불러 일으키는 요소로 등장한다. 내가 이 책을 읽기 시작한 이유는 “미스테리 / 히가시노 게이고” 이다. 다시 말해서 할아버지의 죽음에 아주 정교한 트릭이 숨어있길 바랐으며 주인공이, 혹은 그 다른 누군가가 그 트릭을 파헤치며 모든 일이 할아버지의 죽음에 가까워지는 방향으로 굴러갔으면 했다. 그러나 이 책은 내가 상상하던 책과는 조금 달랐다. 사실 주인공이 할아버지의 죽음을 파헤치는 것이 아니라, 할아버지의 죽음이 주인공을 더욱 단단하게 만드는 용도였다. 이 서사의 주된 이야기는 할아버지의 죽음이 아니였고 이 책의 주인공은 예준린이 아닌 예치우성이다. 히가시노 게이고는 우리나라에서 추리소설의 대가로 알려져있다. 많이 읽어보지는 않았지만 대부분 히가시노 게이고의 추리소설이라고 하면 다들 어느정도는 믿고 보는 소설이라고 생각할 것이다. 그렇기에 나는 이 책에 히가시노 게이고의 추천사가 들어가지 않았으면 한다. 이 책은 이 책만의 재미가 있으나 히가시노 게이고의 책과는 다르다. 나처럼 편협한 사고에 갇혀 이 책을 보게 된다면 실망할지도 모르겠다. "
    
}, 

{
    "id": 56,
    "url": "https://hoex5h.github.io//%EA%B0%9C%EB%B0%9C%EC%9E%90_%EC%9B%90%EC%B9%99",
    "title": "개발자 원칙",
    "body": "2025/01/14 - 개발자 원칙오류를 만날 때가 가장 성장하기 좋을 때다. 엔지니어의 실력은 얼마나 많은 오류와 장애를 만나고 이를 해결했는지 여부에 따라 갈린다. 어떻게 더 나은 개발자로 나아갈 수 있는가?    오류가 발생하면 소스 코드 레벨에서 이해하자.   왜 구글에 검색하는 방법이 아니라, 소스 코드 레벨에서 이해하자라고 하는걸까?   검색으로 문제를 해결하고 끝내버리면 실제로 깊은 지식을 얻기가 어렵다.     알아낸 지식을 글로 공개하라.   소스 레벨에서 이해한 내용을 글로 공개하고, 오래 기억할 수 있게하면서 다른 사람의 조언도 들을 기회로 만들어라.   정말인지 소스 코드로 확인하기자바7 -&gt; 자바8로 가면 CPU 사용량이 줄어든다. 그렇게만 알고 자바8로 업그레이드 하지 말고, 이유를 알아내라. 실제로 소스 코드를 뜯어봐야 작동 원리를 이해할 수 있다.    결과물을 내 것으로 만들기타이밍 공격 -&gt; abcdef. startsWith(“b”) 는 당연히 false 이다. 문자열의 첫 알파벳인 a 가 b와 다르기 떄문.   그런데 어느 정도 긴 문자열이고, 반복을 많이 한다면 실패 응답이 돌아오기까지의 시간을 측정해서 암호를 추측할 수 있게 된다.   늘 시간복잡도가 빠르기만해서 좋은 것은 아니다.  개발을 하며 순간마다 떠오르는 이런 의문들에 대해서 가볍게 넘어가지 말자. 하나하나 쌓이다보면 좋은 기술력이 될 것이다. 디자인이랑 무엇인가?아주 작은 단위의 코드에 적용되는 원칙?: KISS: Keep It Simple, Stupid (제발 클래스나 메서드를 간단하게 만들라는 법칙)DRY: Do not Repeat Yourself, (반복되는 기능이나 객체를 하나로 만들어서 관리하라는 법칙)YAGNI: You Ain’t Gonna Need it (코드를 적어나가다 보면 갑자기 머릿속에서 떠오르는 필요한 기능이 있어서 막 적지만 결국 안 쓴다는 법칙) SOLID 원칙?: SRP : 단일 책임 원칙 (각 클래스는 하나의 정보만을 가지게 만들어야 한다)OCP : 개방 폐쇄 원칙 (확장에는 열려있으며 변경에는 닫혀 있어야 한다)LSP : 리스코프 치환 원칙 (인터페이스의 서브타이핑은 인터페이스에 정의된 형태를 최대한 유지해야 한다?)ISP : 인터페이스 분리 원칙 (인터페이스는 최소한으로 유지하라)DIP : 의존 관계 역전 원칙 (상위 레벨의 모듈이나 인터페이스가 서브 클래스나 타이핑에 영향을 받아서는 안된다) 이러한 디자인의 원칙은 있는데, 소프트웨어 디자인 에 대한 정의는 어딨을까? 디자인 은 뭘까? 한글로 번역하면 설계 인데, 사전에 따르면  건축물 설립이나 토지 공사, 기계의 제작 따위에서 그 목적에 따라 실제적인 계획을 세우고 구체적으로 도면을 그려 명시하는 일 이라는 의미로 해석이 가능한데, 설계를 하는 방법에 완벽한 것이 없다. 그리고 설계도면을 보고 똑같은 건축물이 나오는 것과는 달리 소프트웨어의 설계를 보고 같은 결과물이 나오기는 힘들다. 설계는 시스템이 어떤 요구사항을 만족시키는 조건을 정의하는 것이다. 라는 개념에서 바라보자. 가령 드론은 날 수 있어야 한다 라는 요구사항을 예시로 든다면 이런 요구사항을 만족한다는 것을 증명할 수 있는, 아주 세밀하고 상세한 조건을 정의해야 한다. 화성의 대기 상태를 파악하고, 화성을 분석하는 등의 파악이 필요하다. 나의 메이저 버전을 업그레이드 하는 마이너 원칙들개발자는 평생 공부하는 직업이다. 신기술을 탐험하는 숙명을 가진 직업이 개발자다. 나만의 속도로 가라. 방향을 잘 정하고 자신의 속력을 잘 파악해라. : 그렇다면 나만의 속도는 어떻게 인지할 수 있을까? -&gt; ‘낯선 방식으로 해결하기’학습은 익숙한 것을 의식하지 않고 반복하는 게 아니라, 낯선 것을 의도를 갖고 배우는 것 어떤 대상이 물어보더라도 그 사람이 이해하기 쉽게 설명할 수 있어야 진짜로 ‘아는 것’ 이다. : 형식지 -&gt; 암기로 터득한 지식 -&gt; 쉽게 잊을 수 있다. 암묵지 -&gt; 직접 체험해서 체화한 지식 -&gt; 쉽게 잊혀지지 않는다. 결국 둘 다 필요하긴 하다. . 남들에게 자존심을 세우기보다 나의 자존감을 채워라: 다른 사람들이랑 비교하지말고 하루하루 자신이 성장해나가는 것을 느껴라 목표를 세세하게 나누어서 하나씩 해결해나가는 성취감을 느껴라 혼자 프로그래밍하면 스스로 방향을 바꾸는것을 쉽게 인지하지 못한다. -&gt; 짝 프로그래밍: 서로 과정을 기록하고 공유하는 맥락이 제일 중요하다. 의도적으로 실수해라: 의도적으로 실수해서 어떻게 되는지 체화했다. 누구나 실수할 수 있다고 가정하고 최종 릴리즈가 되기 전에 실수를 찾아낼 수 있는 안정적인 흐름을 만들어야 한다. 소프트웨어 개발은 유연하게 해라. 다른 사람의 의견을 듣고 서로 공유해라 실패해라 -&gt; 코드를 완성 못해도 좋다. 자신이 생각한 방향으로 시도해보고 실패하는 것은 의미가 있다. 테스트 조건을 가정하고, 결과를 예측하고, 생각이 맞았는지 -&gt; 그러고 나서 개선해라. -&gt; 정리해서 오픈 소스 형태로 공유해라. 달리는 기차의 바퀴를 갈아 끼우기여러 번의 개발 문화에 대한 질문을 정리하다보니, 어느정도의 행동 강령이 된 세 가지가 있다고 한다. 밥값:  Make it work, then make it better (“일단 동작하게 만들다음, 더 좋게 만들어라”) 일단 동작하게 만들어라: 일단 동작하게 만들어라, 라는 말은 이해하기 어렵지 않다. 어떻게든 요구사항을 만족해서 정상적으로 수행되도록 하라는 것이다. 사실 이렇게만 보면 언젠간 가능해보이지만 현실은 제약사항이 좀 붙는다. 시간, 비용, 인원 등의 자원에 해당하는 것이 대표적인 제약사항일 것이다. 이러한 제약 조건들을 극복하고 제대로 동작하는 코드를 제때 만드는 것을 글쓴이는 밥값 이라고 부른다. 밥값에는 난이도에 제약은 크게 없다. 만약 신입이 일을 너무 어려워한다면 쉬운 일을 줌으로써 밥값을 하도록 할 수도 있다. 지금 당장 요구사항을 만족하면서 잘 돌아가는 코드가 없다면, 미래에 그 조직이 유지될 수 있을까? 아마 불가능할 것이다. 따라서 지금 당장 실행되는 코드가 있어야 미래에 더 좋은 코드를 만들 수 있다. 더 좋게 만들어라: 더 좋게 만들어라 라는 말은 생각해볼 것이 좀 있다. 더 좋은 것의 기준이 뭘까? 더 좋은 코드는 어떤 코드일까? 더 좋은 코드: 대체로 더 좋은 코드라는 말을 쓸 때 가독성, 성능, 유연성 을 제시한다. 가독성은 말 그대로 코드를 읽기 쉬워짐으로써 기능이나 버그 수정에 훨씬 용이하다. 가독성이 좋은 코드는 좋은 관행 을 따른다.  줄 바꿈, 탭, 들여쓰기 등등에서 모두와의 약속을 지켜라. 변수명을 지을때도 습관을 지켜라. 예외를 던지기로 했으면 예외를 던져라, null을 반환하기로 했으면 null을 반환해라.  코멘트가 필요없어도 이해 되는 코드는 매우 훌륭하다. 그러나 흐름을 설명하는 코멘트나 반드시 필요한 코멘트를 적는 것에는 주저하지 말자.  코드가 짧은 것이 읽기 좋다. 함축적인 코드보다는 명시적인 코드가 읽기 좋다.  삭제해야 할 코드를 굳이 주석으로 남겨두지 마라. Git 이 알아서 보관해준다. 성능은 매우 중요한 조건이지만 대부분의 경우에서는 우선순위가 높지 않다.  섣부른 최적화는 모든 악의 근원이다.  최적화의 규칙 1. 하지 마라. 2. 아직 하지 마라 (전문가 전용)성능이 좋은 코드는 대부분 이해하기 어렵다. 정말 성능이 필요한 코드가 있다면 어느 코드가 대부분의 자원을 사용하는지, 프로파일링을 해야한다. 그런 코드들을 격리하고 적절한 시점에 최적화해라. 유연성이 있고 어려운 코드 보다는 유연성이 없더라도 쉬운 코드가 더 좋은 코드이다. 유연성에는 추상화가 필요하다. 그러나 추상화를 숭배하고 남발하지는 마라. if 문을 극단적으로 배제하고, Factory 를 과도하게 사용하고, 클린 코드와 SOLID 를 ‘과도하게’ 숭배하면서 추상화를 챙기지 마라.  잘못된 추상화보다는 중복이 낫다. 일단 Make it work 를 챙겨라. 그리고 가독성을 최우선으로 챙겨라. 성능과 유연성은 그 다음이다. 기술 부채란?: 기술 부채는 당면한 기술적인 문제 해결을 뒤로 미루고, 시급한 비지니스 문제 해결을 우선시하는 일이 반복되면서, 언젠가부터 기술이 비즈니스의 발목을 잡는 상황을 재정적인 부채에 빗단 말이다. 빚을 내서 돈을 쓰면 이자를 댓가로 지불을 해야하는 것 처럼, 기술적인 문제 해결을 뒤로 미루고 비즈니스를 우선시하면 개발 지연과 품질 저하라는 대가를 지불해야 한다. 그렇게 기술 부채가 늘어나면 결국 전면 재개발을 해야하거나 서비스 중단이라는 결과로 이어질지도 모른다. 앞에서 우선 동작하게 하라고 했는데 이러면 기술 부채가 늘어나지는 않을까? 라고 생각할 수 있다. 분명히 동작하게 만든 뒤에는 더 좋은 코드로의 개선이 필요하다. 이럴 때 필요한 것이 보이스카웃 캠핑 규칙이다.  언제나 발견했을 때보다 깨끗하게 해놓고 캠핑장을 떠나라. 수정한 코드는 원래 코드보다는 깨끗해야 한다. 최소한 더 나빠지면 안된다. 물론 현실에서 이렇게 관리하는 건 쉽지 않다. 최소한 더 나빠지지 않게 관리를 하면서 시간을 들여 천천히 더 좋은 개선 방향을 설정하고 실행해야한다. 최근에는 레거시 코드라는 말은 마치 기술 부채와도 비슷한 뉘앙스로 쓰인다. 하나씩 뜯어고쳐야 할 코드라는 의미로 보이는데, 방대한 레거시 부채를 빠른 시일내로 고치려고 한다면 레거시 코드의 양을 보고 좌절할지도 모른다. 천천히 시간을 들여 고쳐라. 부채 청산의 시작은 자동화된 테스트를 확보하는 것이다. 테스트 코드를 작성하면서 부채를 관리할 수 있다면, 그것은 더 이상 부채가 아니다. 현실에서 기술 부채가 없는 개발은 불가능하다. 우리가 지향해야 하는 목표는 기술 부채를 잘 식별해서, 관리 가능한 부채로 만드는 것이다. 삽질: 기존에 잘 굴러가던 코드를 다시 발명하는 짓은 삽질일까?보통은 잘 굴러가는 바퀴를 다시 발명할 필요는 없다. 하지만 개발자는 다르다. 내가 자주 사용하는, 미래에도 자주 사용할 기술이라면 다시 발명해 볼 필요도 있다. 잘 굴러가던 코드를 다시 작성해보면서 작동 원리등의 지식과 경험을 배울 수 있고 운이 좋다면 더 좋은 코드로의 개선도 가능할 것이다. 은탄환: 모든 문제를 해결할 수 있는 은탄환은 개발자한테는 없다. 코드를 많이 읽고, 많이 쓰고, 많이 생각해라. 코드 리뷰를 할때 많이 읽어라. 기왕 읽을거면 좋은 코드를 읽어라. 여러 번 반복해서 읽되, 한 줄 읽는데 시간을 너무 많이 쓰지는 마라. 글쓴이가 이 코드를 왜 썼는지 의도(컨텍스트)도 같이 파악하면서 읽어라. "
    
}, 

{
    "id": 57,
    "url": "https://hoex5h.github.io//11%EB%AC%B8%EC%9E%90_%EC%82%B4%EC%9D%B8%EC%82%AC%EA%B1%B4_-_%ED%9E%88%EA%B0%80%EC%8B%9C%EB%85%B8_%EA%B2%8C%EC%9D%B4%EA%B3%A0",
    "title": "11문자 살인사건 - 히가시노 게이고",
    "body": "2025/01/14 - 11문자 살인사건 - 히가시노 게이고친구 ‘하기오 후유코’ 의 제안으로 만나게 된 ‘가와즈 다카유키’ 와 연인 사이로 발전한 나, 좋은 사이로 발전해가려고 하던 찰나에 그에게 이상한 말을 듣는다. ‘누구에게 노려지고 있는 것 같아. ’ 위협하는 상대가 누구인지도 대충 짐작을 하는 눈치였지만 나에게는 얘기해주지 않았고 결국 이틀 뒤 살해당한다. 조사해보니 수상한 일이 하나 있었는데, 바로 ‘야마모리 다쿠야’ 라는 사람을 포함해 총 9명의 인원과 함께 작년에 요트 여행을 다녀온 것이다. 작년 요트 여행에 참여한 인원은  가와즈 다카유키 (‘나’ 의 연인) 니자토 미유키 (가와즈 다카유키와 전 동업자였던 사진기자) 야마모리 다쿠야 야마모리 마사에 야마모리 유미	(야마모리 일가족) 이시쿠라 유스케 (야마모리 다쿠야의 동생) 가네이 사부로 사카가미 유타카(야마모리 플라자에 근무하는 사람들)  다케모토 유키히로 (요트 여행에서 죽은 사람) 후루사와 야스코	(???)요트 여행에서 거대한 파도를 만나 배가 난파당했고, 이 과정에서 다케모토 유키히로라는 사람이 죽었다고 한다. 나머지는 모두 무인도로 표류되어 구조되었다고 한다. 이 일과 자신의 연인인 가와즈 다카유키의 죽음은 분명히 연관되어있다고 생각한 나는 요트 여행의 참가자들을 만나 당시 있었던 일들에 대해 얘기해달라고 부탁하지만 아무도 얘기해주지 않고 모른다는 답변만 듣게 되었다. 사건을 파헤치는 과정에서 나는 누군가의 습격을 여러 차례 당했는데,  한 밤중에 누군가 창문을 깨고 들어와 나의 컴퓨터를 켜, 메모장에 ‘더 이상 알려고 하지 마라’ 라는 글을 남김 야마모리 플라자에서 가벼운 벤치 프레스를 하던 도중, 누군가가 나의 얼굴에 젖은 수건을 덮고 덤벨을 눌러 목을 조름 가와즈 다카유키가 남긴 유품을 뜯지 않고 집에 잠시 보관해뒀더니, 누군가 그의 유품 중 요트 여행에 단서가 될만한 무언가를 훔쳐감(무엇인지는 알 수 없음, 다만 두 박스 중에 한 박스만 물건이 이리저리 파헤쳐진 정황으로 추측함) 나는 이러한 역경에도 계속 요트 여행 참가자들을 만나 진실을 파헤치려고 하는데, 친구 ‘하기오 후유코’ 의 도움을 받아 ‘니자토 미유키’ 에게 진실을 들으려고 했더니, 만나기로 한 당일에 그는 살해당한다. 마찬가지로 ‘사카가미 유타카’도 만남을 약속했으나 누군가에게 살해당한다. 요트 여행에 분명 켕기는 것이 있다고 생각한 나와 친구 후유코는 야마모리 다쿠야의 딸, 야마모리 유미를 납치(!)한다. 야마모리 유미는 시각 장애를 앓고 있는 어린 아이였는데 1년 전 요트 여행의 전말을 조금은 알고 있는듯 했다. 중간에 기절을 해 기억이 온전치 않지만 유미에게 들은 내용은 다음과 같다. 거대한 파도를 만나 요트가 뒤집혔고, 자신은 아빠인 야마모리 다쿠야가 구조해주었다. 정신을 차려보니 어떤 무인도에 있었고 다른 사람의 생사를 확인하려는 찰나, 어떤 여자가 ‘그 이를 살려주세요!’ 라고 외치는 소리를 들었다고 한다. 그러나 누구도 선뜻 나서지 못했고 이 후의 일은 기절을 해서 알 수 없다. 요트 여행에서 죽은 ‘다케모토 유키히로’ 가 아마 ‘그 이’가 아닐까 싶었고 ‘그 이를 살려주세요!’ 라고 외치는 여자는 정체를 알 수 없던 ‘후루사와 야스코’가 아닐까 하고 ‘나’는 추측해본다. 아무도 도움의 손길을 내밀지 않아 연인인 다케모토 유키히로가 죽었고 깊은 상실감에 빠진 후루사와 야스코의 복수극이 지금의 상황을 만들고 있는게 아닐까? 하는 그런 추측이였다. 요트 여행의 주최자이자 이 사건의 전말을 알고 있을 것이라고 추정되는 ‘야마모리 다쿠야’ 에게 찾아간 나는 여전히 이 일에서 손 떼라는 말을 듣게 되고, 이런 협박에도 절대 발을 빼지 않을 것 같자 1년 전에 있던 요트 여행을 이번에 또 갈 예정인데 같이 가자는 권유를 받는다. 혼자가 무서우면 친구 ‘하기오 후유코’와 같이 와도 좋다고 했고 ‘나’는 친구와 함께 요트 여행에 올라선다. 올해 요트 여행 참가자 목록 : 야마모리 다쿠야 , 야마모리 마사에, 야마모리 유미, 이시쿠라 유스케 (야마모리 일가족) 야마모리 다쿠야의 비서 가네이 사부로 , 하루무라 시즈코(가네이 사부로의 연인) 나, 하기오 후유코 (나의 친구) 요트 여행에서 ‘하기오 후유코’는 ‘나’와 같이 한 방을 썼는데. 후유코는 로비에 있던 게임기에 푹 빠져 ‘나’ 만 먼저 방에 올라오게 된다. 후유코를 기다릴 겸 씻고 나오니 후유코는 방에 돌아와 있었고 왜 벌써 돌아왔냐는 물음에 친구는 ‘잔 돈이 없어서’ 라고 대답한다. 평소 시계를 들고 다니지 않던 나는 원래 호텔 방에 있던 시계를 보니 9시 40분 이었고 내가 직접 보지는 못했지만, 후유코가 창 밖을 바라보며 ‘하루무라 시즈코’ 가 호텔 밖으로 나가고 있다는 이야기를 한다. 10시에 후유코와 함께 잠이 들었으나 악몽을 꿔 한 시간 후인 11시에 일어난 나는, 옆 침대에 있어야 할 후유코가 없다는 사실을 깨닫고 로비로 내려간다. 로비에 있던 모든 이들은 누구도 나가는걸 보지 못했다고 했고 호텔의 바로 앞은 절벽이라 위험하다는 얘기와 함께 다음 날 아침까지 기다린다. 다음 날 아침 절벽에 가서 확인해봤더니, 후유코는 절벽에서 떨어져 죽어있었다. 후유코는 절대 자살이 아니다. 그러할 이유도 없다. 그렇다면 어째서 죽었을까? 호텔에서 있던 일을 정리하면 아래와 같다  방에 들어와 호텔 방의 시계를 보니 ‘9시 40분’ 이었다.  후유코와 호텔 방의 시계가 ‘10시’ 가리킬 때, 잠에 들었다.  ‘11시’ 에 일어나보니 후유코가 없었다 ‘나’의 방에 들어온 사람은 나와 후유코를 제외하고 없다.  9시 40분에 호텔을 나가던 사람은 ‘시즈코’ 가 맞았다.  시즈코를 포함한 모든 사람은 ‘10시부터 11시까지’ 알리바이가 있다.  다른 사람들은 자기가 할 일을 하느라 못봤다고 했지만, 유미가 증언하기로는 후유코가 나간 뒤에 2명의 사람이 더 나갔고, 한 명은 담배를 피지 않고 다른 한 명은 담배를 피는 것 같다. 즉, 호텔 밖을 나간 순서는 후유코 - 담배를 피지 않는 누군가(여자) - 담배를 피는 누군가(남자) 요약하자면 후유코가 살해되었을 거라 추정되는 10시 ~ 11시에는 모두의 알리바이가 있다. 호텔 방의 시계가 10시를 가리킬 때까지 후유코와 나는 같이 있었다. 직접 보지는 못했지만 9시 40분에 시즈코가 나간것은 사실이다. 후유코는 살해당한것이 맞다고 가정한다면, 언제 죽었을까? "
    
}, 

{
    "id": 58,
    "url": "https://hoex5h.github.io//%EC%A0%A0%EB%8D%94_%EB%AA%A8%EC%9E%90%EC%9D%B4%ED%81%AC_-_%EB%8B%A4%ED%94%84%EB%82%98_%EC%A1%B0%EC%97%98,%EB%A3%A8%EB%B0%94_%EB%B9%84%EC%B9%B8%EC%8A%A4%ED%82%A4",
    "title": "젠더 모자이크 - 다프나 조엘,루바 비칸스키",
    "body": "2025/01/14 - 젠더 모자이크 - 다프나 조엘,루바 비칸스키이번 주 주제와 관련해서 책을 선정할 때 많은 고민을 했는데 이러한 책들을 내가 읽으면서 납득할 수 있을까? 라는 생각이 들었고, 내가 가장 납득할 수 있는 책은 과학적으로 입증된 사실들로 구성된 책이겠구나 싶었다. 젠더 모자이크는 남자와 여자의 ‘뇌’ 를 탐구하며 ‘여자 뇌’ 와 ‘남자 뇌’ 라고 불리우는 것들에 반론을 제기한다. 명사 모자이크  한 몸의 여러 부분에서 두 개 이상의 형질을 동시에 지닌 개체. 몸빛이나 털의 유무 따위의 외부 모양에 관계되는 형질이 많은데, 주로 곤충에서 볼 수 있다.      남성의 뇌는 여성의 뇌보다 우월하다?   남성이 여성보다 우월하다는 구 시대적 발상이 만연하던 16세기 까지 과학자들은 남성의 두개골의 크기가 평균적으로 여성의 것보다 크기 때문에 남자의 뇌가 우월하다고 주장했다. 그러나 당연하게도 인간보다 두개골이 큰 동물이 존재하고, 여성보다 우월하기 위해 동물보다 열등한 존재가 될 수 없었기에 학자들은 ‘평균적인 남성 뇌의 크기가 여성보다 크다’라는 다음 주장을 내세운다. 그러나 역시 뇌의 크기와 지능에 직접적인 연관을 찾지는 못한다.  여자가 멀티태스킹을 잘하고 남자가 공간지각에 뛰어나다는건 거짓말일까?이 책을 읽기 전까지 나또한 남성과 여성은 기본적인 생각과 행동에 차이점이 있다고, 당연히 그럴 수 밖에 없다고 알고 있었다. 원시문명에는 남자와 여자의 역할이 명확하게 구분되었고, 그에 맞게 진화했으며 현대에 이르기까지 남성호르몬과 여성호르몬이 각각의 성별에 다르게 작용하기 때문이다. 따라서 나의 주된 생각은 ‘차별이 아니라면 차이는 인정해야 한다. ’ 이었다. 그러나 이 책에서는 조금 다른 견해를 제시하는데, ‘성의 차이는 뇌에 영향을 미치긴 하나, 여자 뇌와 남자 뇌라는 것을 만들지는 않는다. ’ 이다. 이를 입증하려 두뇌에서 어떤 ‘성별적인’ 특징이 스트레스에 의해 반대로 바뀌는 예시를 들었는데 이해를 돕기 위해 예시를 들어보자. 어떠한 신경세포체의 기본적인 스탠스가 남자는 AA , 여자는 AB 라고 하자. (성별의 차이가 아예 없지는 않음. 기본적인 스탠스가 다를 수 있음) 남자와 여자의 어떤 신경세포체에 스트레스를 주면 아래와 같이 변한다. [남자]   전) AA   후) BB [여자]   전) AB   후) AA 여자는 스트레스를 받았을 때 스트레스를 받지 않은 남자처럼 변했고, 남자가 스트레스를 받았을때는 성별에서 볼 수 없는 새로운 스탠스로 변했다. 책에서는 어느날 한 여성이 스트레스를 받아 음부가 음경으로 바뀌는 상상을 하는 매우 극단적인 예시를 농담삼아 제시했다. 책의 주장은 성별은 환경과 같이 뇌를 변화시키는 요인의 일부일 뿐, 절대적인 요인이 아니란 것이다.  성별이 그 사람의 특징을 나타낼 수 있을까?책은 인간의 뇌를 여성과 남성의 뇌로 나누는 것이 아니라, 뇌는 다양한 요소들에 의해 복합적으로 구분되어 있다고 주장한다. 인공지능에게 남자의 뇌를 (혹은 여자의 뇌를) ‘정상’ 이라고 학습시키고 여러 성별의 뇌를 표본으로 넣었을 때 반대 편의 성별이 얼마나 ‘비정상’으로 인식되는지 연구해본 결과, 반대편의 성별과 같은 성별이 비정상으로 인식될 확률은 비슷하다고 한다. 이번에는 어떤 글의 작성자가 여성인지 남성인지를 구분할 수 있는 알고리즘을 개발하여 실험한 결과, 이 알고리즘은 약 80퍼센트의 정확도로 글쓴이의 성별을 구분했다고 한다. 그러나 이 알고리즘이 작성자의 성별을 판별했다고 하더라도, 글이 얼마나 짧고 긴지, 자전적 에세이인지 로맨틱 소설인지, 진지한 이야기인지 가벼운 이야기인지에 대한 정보는 주지 못한다. 즉 성별을 아는 것이, 그 사람의 특징을 나타내지는 않는다는 이야기이다. 2번과 3번을 토대로 결론을 내리면, 사실 남자의 뇌와 여자의 뇌 같이 이분법적인 뇌는 없다. 여성과 남성 사이에 두드러지게 나타나는 차이점인 신체와 달리 뇌는 그렇지 않다는 것이다. 우리는 최근 수 많은 남녀 갈등문제를 마주하고 있다. 전 여자친구를 잊지못하고 그녀의 집에 찾아가 흉악범죄를 저지르거나 애인에게 무고죄를 저지르는 ‘아주 극단적인 뇌’ 를 가진 사람들을 제외하고는 사실 크게 구분지을 필요가 없다는 이야기다. 그렇다면 현재 사회에서 게임과 공대에는 남자가 많고 유아교육과에 여자가 많은 이유는 뭘까? 어렸을때부터 남녀를 다르게 인식해 키웠기 때문일까? "
    
}, 

{
    "id": 59,
    "url": "https://hoex5h.github.io//%EB%8B%AC%EB%9F%AC%EA%B5%AC%ED%8A%B8%EC%9D%98_%EA%BF%88_%EB%B0%B1%ED%99%94%EC%A0%90_-_%EC%9D%B4%EB%AF%B8%EC%98%88",
    "title": "달러구트의 꿈 백화점 - 이미예",
    "body": "2025/01/14 - 달러구트의 꿈 백화점 - 이미예 💡 만약 꿈을 만들어내거나 소비할 수 있다면 어떨까? 달러구트의 꿈 백화점에는 수 십, 수 백가지의 꿈이 판매대에 올라있다. 유명한 제작가의 꿈 부터, 유통기한이 지나버린 꿈까지, 사람들은 원하는 꿈을 백화점에서 구매할 수 있다.  꿈에는 유통기한이 있다. 그 유통기한이 지나면 꿈의 일부분이 끊어지거나 흑백이 되어버린다.  꿈은 후불이다. 꿈을 꾸고 일어날 때 느끼는 ‘감정’ 이 바로 꿈을 구매하는 수단이다. 이 감정의 일부분을 달러구트의 꿈 백화점이 받아감으로써 값을 지불 받는다.  감정은 액체같으며 거래가 가능하다. 그리고 생각보다 이 감정은 단순히 작용하지 않는다. 난로에 분노를 넣으면 불이 활활타오른다고 한다. 사람들은 꿈을 구매하고 잠에 들어 그 꿈을 꿀 수 있다. 백화점에 와서 꿈을 샀다는 그 사실을 잊어버리기에 이 꿈이 자신이 ‘구매한’ 꿈이란 걸 알지 못하지만 말이다. 책에는 다양한 일화의 꿈이 나온다. 짝사랑을 하는 사람이 나오는 꿈이라든지, 소설 작가에게 예지몽을 꾸게 하여 영감을 얻게 한다든지, 한 철 장사하는 산타클로스의 꿈까지 심지어 악몽까지 판매한다. 정확히는 사람의 트라우마에 해당하는 꿈을 판매한다. 트라우마를 극복하기 위해 이 꿈을 산 일부 남자 손님은 3년으로 길어진 군대에 재입대 하는 꿈을 꿔버리는데… 💡 꿈으로 트라우마를 극복할 수 있을까? 트라우마란 기본적으로 과거 겪은 고통이나 정신적인 충격으로 유사한 상황 발생시 불안한 증세를 겪는 현상을 말한다. 일반적으로 트라우마는 기억의 이미지를 동반해 나타나는 경우가 많다. 트라우마는 극복하지 못했기 때문에 트라우마가 아닌가? 극복했다면 트라우마라 부를 수 없다. 과연 트라우마에 해당하는 꿈을 반복적으로 꾼다고 해서 적응하고 익숙해져 극복할 수 있을까? 트라우마 꿈을 산 손님들이 너무 가볍게 극복하는 것 같아서 현실감이 조금 동떨어졌다. 다만 트라우마를 극복하고자 노력하는 행위 자체에 도움이 될 수는 있을 것 같다.  내가 알기론 그런 것보다 무서운 것들이 이미 많던데. 질투심, 열등감, 그런 것들이 요즘에는 쫓아오는 맹수보다 무서운 거라고 💡 사회 갈등에 꿈을 이용해 볼 수 있을까? 질투심이 없어지는 꿈, 열등감이 없어지는 꿈, 그런 꿈들이 있을까? 어떤 꿈을 꾸면 될까?  사람들은 자는 것보다 재밌는 일이 많으니까 잠들지 않는 거에요. 상당히 공감이 가는 내용 중에 하나였다. 벼락치기 같이 일시적으로 잠을 미루는 것이 아니라, 누군가에겐 영화나 넷플릭스가, 또 다른 누군가에겐 게임이 될 수 있겠다. 꿈을 구매한 사람들은 후불로 결제를 하기 때문에 반드시 그 꿈을 꾸고 일어나야 ‘감정’ 으로 백화점에 값을 지불할 수 있다. 즉 , 꿈을 사놓고 ‘노쇼’를 해버리면 백화점이 손해를 봐버리는 것이다. 소설에선 이러한 이해관계가 얽혀있다. 현실도 크게 다르지 않다. 사람들은 자는 것보다 재밌는 일, 중요한일이 많다. 잠을 미루게 되면 다음날에 고스란히 피해를 입을 수 밖에 없다. 재밌는 것이 재미없어지기는 쉽지 않다. 그렇다면, 💡 잠자는 것이 재밌어 질 수 있을까? 보통 불면증 환자들이 잠자는 것을 재밌어 할 리는 없다. 잠자는 것이 재밌어지면 불면증도 없어지지 않을까? "
    
}, 

{
    "id": 60,
    "url": "https://hoex5h.github.io//%EB%8D%B0%EB%AF%B8%EC%95%88_-_%ED%97%A4%EB%A5%B4%EB%A7%8C_%ED%97%A4%EC%84%B8",
    "title": "데미안 - 헤르만 헤세",
    "body": "2025/01/14 - 데미안 - 헤르만 헤세데미안 나는 ‘고전’ 이라는 장르에 대해 크게 관심이 없었다. 주로 어떠한 원리, 개념들이나 자연적인 것에 대한 책들, 혹은 누군가의 일대기를 읽었다. 고전은 둘째치고 나의 내면을 찾아간다든지, 자기 인식에 관한 어떠한 책들도 나는 읽어본 적이 없다. 읽기 싫었던 것은 아니지만 내가 좋아하는 분야가 명확했기 때문이다. 이번에 읽게된 데미안은 예상하듯이 내가 읽던 책들과는 매우 달랐다. 주인공 에밀 싱클레어는 따뜻하고 화목한 부모님과 가족들, ‘선’에 속해 자라면서도 살인,강간,강도와 같은 비참하고 암울한 ‘악’ 의 이야기들을 집안의 하녀에게 들으며 자랐다. 어릴 적 그는 ‘선’ 의 세계에서 벗어날 생각이 크게 없었으며 당연한 운명으로 받아들였다. 그러다 프란츠 크로머라는 아이를 만나게 되는데, 이 아이는 ‘악’ 에 속해있는 아이였고 주인공 싱클레어를 악의 세계로 강하게 끌어 당긴다. 여기서 크로머는 싱클레어에게 ‘악’ 을 계몽시켜주는 역할이 아니라 그저 주인공에게 위해를 가하고 악의 ‘하수인’으로 만들어 부려먹는 악마에 가깝다. 고통에 몸부림치는 싱클레어를 구원해주는 사람이 등장하는데 이 사람이 바로 데미안이다. 또래 아이들보다 어른스러운 외모와 행동, 말투를 지니고 있었으며 중성적인 얼굴을 지녔다. 크로머로부터 싱클레어를 구해준 데미안은 싱클레어와 친해지고 카인과 아벨 이야기를 한다. 성경 구절에 카인은 ‘악’, 아벨은 ‘선’ 으로 표현되고 있었고 부모님과 선생님들도 그렇게 가르쳤다. 당연히 그들에게서 자란 싱클레어도 마찬가지였다. 그러나 데미안은 이에 다른 생각을 가졌고 싱클레어와 자신의 생각을 공유한다. 이는 오직 ‘선’을 좇아 살아가던 싱클레어의 생각에 균열을 일으켰다. 밝디밝은 빛의 세계에 속해 ‘악’을 배척하고 ‘선’을 숭배하던 싱클레어는 데미안과의 교감을 통해 ‘선은 무조건 옳지 않고 악은 무조건 나쁘지 않다. 그 가치를 판단하는 것은 오직 자신의 내면만이 가능하고, 선과 악을 좇는 것이 아니라 자신의 내면을 좇아야한다. ’ 라는 것을 깨닫는다. 여기까지 읽으면서 자신의 내면이라는 말에 굉장히 난해함을 느꼈다. 나는 선과 악은 자신의 내면보다 공동체와 규칙이 정한다는 느낌을 강하게 받고 있었다. 요즘처럼 코로나 바이러스가 창궐하는 것과 같은 어려운 시기에는 더더욱이 공동체가 정한 규칙은 무조건적으로 지켜져야 한다고 생각한다. 그러나 자신의 내면이 강하게 악을 지지한다면 이것은 선이 될 수 있을까. 반대로 강하게 선을 배척한다면 이것은 악이 될 수 있을까. 책에 나오는 대다수는 나와 같은 생각을 하며 살아가는 평범한 사람들이다. 반대로 데미안과 그의 어머니는 남들이 정한 기준이 아닌 자신의 내면을 기준 삼아 살아간다. 주인공인 싱클레어는 바로 나같은 사람들과 데미안 사이에서 갈등하던 존재였고 끝내 자신이 살아온 ‘선’의 세계를 부수고 나가는 사람이 된다. 여기서 데미안의 명언으로 꼽히는 문장이 나오는데, “새는 알에서 나오기 위해 투쟁한다. 알은 세계이다. 태어나려고 하는 자는 누구든 하나의 세계를 파괴하여야 한다. 새는 신을 향해 날아간다. 그 신의 이름은 아브락사스이다” 라는 문장이다. ‘싱클레어를 비롯해 모든 사람들은 자신의 내면이 원하는 바를 이루기 위해서는, 본인들이 그저 안락하게 살아가던 세계를 박차고 나와 그 세계를 파괴해야한다. 자신의 내면이 무엇을 원하는지 알아야하며, 그것을 깨닫고 난 뒤에는 알에서 나오도록 투쟁해야한다’ 라고 나는 느꼈다. 아브락사스라는 신은 ‘선’ 과 ‘악’을 모두 지닌 존재로 표현된다. 싱클레어는 원래 ‘선’의 세계에 속했지만 크로머를 만나 비록 강제적 이었을지라도 ‘악’ 을 경험하게 된다. 그 후 데미안에게 구해져 ‘선’의 세계로 돌아갔지만 데미안과의 교감으로 자신의 내면을 탐구하며 선과 악 사이에서 갈팡질팡한다. 내면이 좇고자 하는 것이 무엇인지 모르던 싱클레어는 술과 못된 무리들과 어울리며 악을 탐구하기도 하고, 공원에서 마주친 어떤 여자에게 사랑에 빠져 악에서 벗어나기도 한다. 훗날 그 여자에게 ‘베아트리체’ 라는 이름을 혼자 짓고 그녀의 그림을 그리는 연습을 하는데, 완성된 그림을 보더니 자신이 그린 사람은 베아트리체가 아니라 데미안이었다는 사실을 깨닫는다. 싱클레어의 내면이 좇던 모습은 데미안이 좇던 모습과 동일했기 때문이었을까. 이 책에서 주고자 하는 교훈은 ‘남들의 잣대에 휘둘리지 말고 자신의 내면이 원하는 꿈을 찾고, 꿈을 위해 살아오던 알을 부셔라. 자신이 원하는것은 선과 악을 모두 지녔고 꿈을 판단하는 주체는 남이 아니라 바로 자신의 내면이다. ’ 라고 느꼈다. 모두들 한 번 쯤은 어떤것들에 대해 남들이 아니라고 말할 때 도전해본 경험들이 있을 것이다. 결과가 좋았든 좋지 않았든 그 도전에 대해 후회는 없었을 것이고 나 또한 그러한 경험이 있다. 그렇기 때문에 이 책의 교훈에 크게 공감했고 모든 사람들이 한 번 쯤은 도움을 받을 수 있는 문장이라고 생각한다. 하지만 요즘들어, 무언가를 선택하기에 있어 자신의 내면이 영향력을 가장 많이 행사하는건 맞지만 오로지 내면만을 보고 선택할 수 있을까? 라는 생각이 든다. 사람은 무리를 지어 살아가고 공동체를 형성해 서로를 보호하고 공감하며 살아가는데 자신의 내면만 생각할 수 있는걸까 라는 생각을 하며 읽게 되었다. 한 편으로는 주인공이 부러웠고 또 다른 한 편으로는 걱정 되었다. 만약 내 자신이 싱클레어였다면 데미안과의 만남은 나를 바꿀 수 있었을까? 대학생이 된 싱클레어는 데미안과 조우하게 되는데, 이는 싱클레어가 강렬하게 데미안과 다시 만나고 싶다고 생각하기 때문이다. 자신의 내면이 강렬하게 원하면 이루어진다는 데미안의 말이 맞아떨어지는 순간이었다. 싱클레어는 데미안과 만나기 전부터 여러 꿈들을 꾸어왔는데 여러 꿈들에는 어떤 사람이 나왔다. 여성이면서 남성이기도 하고 아름다운 외모를 지녔으며 어머니같으면서도 동시에 애인같은 사람이었다고 한다. 이 꿈에 나오던 사람은 바로 데미안의 어머니였고, 작중에서 신적인 존재로 표현된다. 싱클레어는 데미안과 데미안의 어머니, 에바부인과 자주 어울리며 내면의 탐구에 더욱 정진했고 행복한 시간을 보낸다. 행복한 꿈이 지속될 것이라고만 생각하던 싱클레어에게 에바부인은 영원한 꿈은 없다며, 언제까지나 어떤 꿈이 계속되지는 않는다고 얘기한다. 싱클레어는 에바부인에게 친구,어머니,애인 등등 다양한 감정을 느끼고 그녀에게 더욱 빠져들게 된다. 그러자 에바부인은 싱클레어를 불러 이러한 말을 한다. “ 사랑은 간청해서는 안 돼요. … 강요해서도 안 됩니다. 사랑은, 그 자체 안에서 확신에 이르는 힘을 가져야 합니다. 그러면 사랑은 더 이상 끌림을 당하는 것이 아니라 스스로 끕니다. 싱클레어, 당신의 사랑은 나에게 끌리고 있어요. 언젠가 내가 아니라 당신의 사랑이 나를 끌면, 그러면 내가 갈 겁니다. 나는 선물을 주지는 않겠어요. 쟁취되겠습니다. ” 사실 이 챕터부터는 읽으면서도 이해하기 조금 어려웠다. 에바 부인을 최대한 신적인 존재로 이끌어가야 싱클레어의 행동들이 이해되었다. 게다가 마치 예언을 하는듯한 데미안과 에바부인때문에 더더욱 몰입하기가 힘들었다. 이전까지는 그래도 굳센 내면을 가진 사람들이라고 믿었다면 이제는 완전히 신 적인 존재가 되어버린 것 같았다. 데미안과 에바부인은 지속적으로 불길한 꿈을 꾸고, 여러 징조들을 느낀다. 그 징조는 곧 전쟁이라는 현실이 되었다. 데미안과 싱클레어는 전쟁에 참가하게 되고 싱클레어는 이 곳에서 모든 사람이 한 가지의 이상(꿈)을 좇는 광경을 목격한다. 죽음을 목도하면서도 꿈을 위해 운명에 접근해가는 사람들을 보며 사람들이 알을 부수고 있음을 보게 된다. 그러다 불의의 사고로 호송되어 치료받는 도중 데미안을 만나는데, 데미안 역시 치료를 받는 중이었다. 데미안은 이제까지 한 번도 언급한 적 없던 프란츠 크로머의 일을 언급하며 앞으로 내가 필요할 때는 나를 직접 부르는 것이 아닌 자신의 내면에 귀를 기울이라고 한다. 그리고 싱클레어는 자신의 내면에 데미안이 있음을 느끼며 이야기는 끝이난다. 데미안이라는 책은 흥미로우면서도 굉장히 어려웠다. 읽는 내내 한 문장을 여러번 읽어보기도 했고 그럼에도 이해가 안되는 부분이 많았다. 어떠한 교훈을 주려고 하는지는 이해를 했으나 문장 하나하나가 추상적이었고 직관적인 느낌을 받기 어려웠다. 또한 초반부와 달리 후반부에는 신적인 존재와 영적인 요소가 많아 사실감이 존재하던 초반부에 비해 후반부는 이해하기 더욱 어려웠다. 헤르만 헤세는 이 책으로 제1차 세계대전에 의해 좌절감을 느낀 수 많은 청년들에게 희망을 주었다고 한다. 언제까지나 고통에 몸부림칠 것이 아니라, 세계를 깨고 나오라는 이야기를 하고 싶었을거라 생각한다. 그리고 그 말은 당시 사람들뿐 아니라 현대인들에게까지 통용되는 이야기라 아직까지도 인기가 많은 책이 아닐까 싶다. 한 번만 읽고 이해하기는 많이 어려운 책이라는 느낌을 받았다. 누군가가 책에 대해 물어본다면 나도 잘 모른다고 얘기할 것 같다. 꼭 다시 읽고 곱씹어 봐야겠다. 추가로 작은 책이라 그런지 오타가 너무 많다. 다음엔 작은 책을 별로 안사고 싶다. "
    
}, 

{
    "id": 61,
    "url": "https://hoex5h.github.io//%EB%91%90_%EB%B2%88%EC%A7%B8_%EC%A7%80%EA%B5%AC%EB%8A%94_%EC%97%86%EB%8B%A4_-_%ED%83%80%EC%9D%BC%EB%9F%AC_%EB%9D%BC%EC%89%AC",
    "title": "두 번째 지구는 없다 - 타일러 라쉬",
    "body": "2025/01/14 - 두 번째 지구는 없다 - 타일러 라쉬두 번째 지구는 없다. 말 그대로 지금 사는 지구가 사라지면, 대체재는 없다는 소리다. 환경을 아끼고 심각성을 인지해야하며, 그저 개개인이 자원을 아껴쓰는 것만으로는 부족하다는 이야기가 주를 이룬다.  코로나의 근본적인 원인은 기후위기 이다. 사실 이 문장에서 핵심인 단어는 ‘근본적인’ 이라고 생각한다. 코로나 발병에 있어서 가장 높은 영향력을 끼친 원인이 기후위기라고 해석할 수 있는데, 기후 위기로 인해 박쥐의 서식지가 점점 인간쪽에 가까워졌기 때문에 기후위기가 원인이라고 말하는 듯 하다. 이 말에 동의하긴 하지만 ‘근본적인 원인’ 이라고 할 수 있을까? 적합한 조리 과정을 통한 박쥐였다면 (나는 안먹을테지만) 문제가 없었을 것이다. 그저 비위생적인 식용 습관에 의해 발병된 질병이지 않나 생각한다. 박쥐는 동굴같은 비위생적인 환경에 서식하기 때문에 온갖 질병의 매개체가 되기 쉬우면서도 체온은 다른 포유류보다 2~3도 높기 때문에 면역체계가 매우 뛰어나 본인이 질병을 앓지는 않는다고 한다.  78억명의 빚쟁이한 해 동안 지구가 생산할 수 있는 자원의 양이 1 이라고 하면, 우리를 포함한 전 세계인들은 매년 1. 75를 사용하고 있다고 한다. (코로나 때문에 가동을 멈춘 공장들이 늘어나면서 이례적으로 2020년부터는 조금 줄어들었다고 한다. ) 자원이 순환되며 지구가 정상적으로 재생하기 위해서 1보다 같거나 적게 사용해야 하지만 우리는 미래의 0. 75를 계속 빌려다 쓰고 있는 것이다.  경제의 관념5,500원 짜리 바닐라 라떼  가격에 반영된 것 : 바닐라 라떼에 들어가는 재료비, 기타 노동비 …   가격에 반영되지 않았지만 얻은 것 : 카페인 섭취로 인한 업무능력 향상, 휴식, …      경제적 외부성이라는 개념이 있다. 제삼자가 외부의 영향으로 비용이나 이익이 생기지만, 그것을 통제도 제어도 할 수 없어서 공식에 반영되지 않고, 실제 가격이나 값을 계산하는 데에 포함되지 않는다.     가격에 반영되지 않았지만 잃은 것 : 바닐라 라떼가 건강에 미치는 영향, 먼 훗날 늘어날 의료비용, 아메리카노를 시켰다면 아낄 수 있었던 1,000원의 경제적 여력…우리는 계산이 어려운 이런 것들에 전부 ‘경제적 외부성’ 이라는 라벨을 붙여서 무시하는 경제관이 있다. 환경  커피 원두는 열대 지역에서 자라기 떄문에 열대 지역에 커피나무를 심고 키워야 한다. 원래 그 자리에서 자라던 식물이 생태계에서 했던 역할의 값 다른 식물을 제거하고 커피나무를 키웠을 때 해당 지역 공기의 질 커피를 다른 나라에 수출하는 과정에서 생기는 온실가스 대기로 방출된 가스 때문에 대기가 따뜻해지고 강우 패턴이 달라져서 가뭄을 유발하고 해충이 증가 하는 값… 등등 타일러는 경제 활동의 외부 효과를 전혀 고려하지 않는 것, 어떤 일이 유발하는 환경오염과 그것을 회복하는 데 드는 시간과 비용을 염두에 두지 않는 것이 환경 문제의 핵심이라고 한다.    개인이 아껴쓰는 걸로는 부족하다   개인이 종이컵같은 일회용품 좀 덜 쓰고, 물 아껴쓰고, 환경에 관심 좀 갖는 걸로는 부족하다고 얘기한다. 그는 환경을 아끼는 기업을 우대하고, 반대의 기업에는 제재를 내리는 등의 관리가 필요하며 그것이 이루어지기 위해서는 우리가 환경에 관심을 갖는 것을 넘어 기업에게, 정부에게 요구해야한다고 한다. (나는 안했지만) 일본 불매운동 처럼 누군가가 무언가에 투쟁하여 얻어내고자 하는 것처럼 말이다.   나는 이 때 일본 여행을 다녀왔으며 후쿠오카 유후인의 료칸에서 정말 즐거운 시간을 보냈다. 정말 최고다. 또 가고 싶다. 거기서 닌텐도도 샀는데, 닌텐도 라이트여서 TV에 연결이 안되는 바람에 중고로 처분했었던 기억이 있다.     환경을 아껴야한다고 누구나 다 말하지는 않는다.   기온이 바뀌게 되면 불리해지는 곳이 있는 반면, 유리해지는 곳도 있다. 예를 들어 러시아 같은 경우 기후위기가 진행될 수록 북극에 해상로가 열리고 부동항이 생긴다고 한다. 최근 우크라이나를 침공한 이유를 생각해보면 더더욱 이해가 쉬울 듯 하다.     환경을 위한 채식도 존중받아야 한다.   축산업은 온실가스 배출에 큰 비중을 차지한다. 자동차의 매연이나 석유가 기후 위기의 주범이 맞긴 하지만 축산업 또한 온실가스 배출량의 18%를 차지할 정도로 비중이 크다. 혹자는 51%에 라고 주장하기도 했다고 한다. 고로 자신의 건강을 위한 채식이 아니라 환경을 위한 채식도 존중받아야 한다고 한다.   나는 남에게 피해를 끼치지 않는 개인의 신념은 존중받아 마땅하다고 생각한다. 다만 한국만큼 모임자리에서 육류를 많이 섭취하는 나라가 또 있을까 싶기 때문에 ‘사회적인 육식’ 이 있을 수 밖에 없다고 생각한다. 타일러도 마찬가지의 이유로 채식을 포기했다고 한다.     FSC 인증 종이   FSC 인증 마크가 붙은 종이는, 이 종이를 사용하기 위해 벌목한 만큼 다시 심었다는 것이 인증된 마크라고 한다. 학창 시절부터 지금까지 수도 없이 많은 종이를 써왔을 나도 처음듣는 종이였다. 이 책은 FSC 인증 종이로 만들어졌다고 한다.   FSC 인증 종이는 당연히 일반 종이보다 비쌀텐데 FSC 인증 여부만 다른 똑같은 책이 있다면 과연 나는 어떤 선택을 할 것인가?  "
    
}

];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});
function lunr_search(term) {
    document.getElementById('lunrsearchresults').innerHTML = '<ul></ul>';
    if(term) {
        document.getElementById('lunrsearchresults').innerHTML = "<p style='color: black;'>검색 결과: <span style='color: orange;'>" + term + "</span></p>" + document.getElementById('lunrsearchresults').innerHTML;

        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult' style='color: black;'> 검색 결과가 없습니다. </li>";

        }
    }
    return false;
}

function lunr_search(term) {
    $('#lunrsearchresults').show( 400 );
    $( "body" ).addClass( "modal-open" );

    document.getElementById('lunrsearchresults').innerHTML = '<div id="resultsmodal" class="modal fade show d-block"  tabindex="-1" role="dialog" aria-labelledby="resultsmodal"> <div class="modal-dialog shadow-lg" role="document"> <div class="modal-content"> <div class="modal-header" id="modtit"> <button type="button" class="close" id="btnx" data-dismiss="modal" aria-label="Close"> &times; </button> </div> <div class="modal-body"> <ul class="mb-0"> </ul>    </div> <div class="modal-footer"><button id="btnx" type="button" class="btn btn-danger btn-sm" data-dismiss="modal">Close</button></div></div> </div></div>';
    if(term) {
        document.getElementById('modtit').innerHTML = "<h5 class='modal-title' style='color: black;'><span style='color: orange;'>" + term + "</span> 검색 결과입니다!</h5>" + document.getElementById('modtit').innerHTML;

        //put results on the screen.
        console.log(idx);
        var results = idx.search(term + '*');
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><small><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></small></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult' style='color: black;'>검색 결과가 없습니다.</li>";
        }
    }
    return false;
}

$(function() {
    $("#lunrsearchresults").on('click', '#btnx', function () {
        $('#lunrsearchresults').hide( 5 );
        $( "body" ).removeClass( "modal-open" );
    });
});
